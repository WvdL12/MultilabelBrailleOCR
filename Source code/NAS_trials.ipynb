{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring different Neural Architecture Search frameworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from model_utils.augment import num_to_bin, bin_to_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"dark\", font_scale=1.1)\n",
    "\n",
    "DATASETS = os.path.join('..', '..', 'Datasets', 'numpy_datasets')\n",
    "MODEL_PATH = os.path.join('..', 'models')\n",
    "OPT_LOGS = os.path.join('..', 'opt_logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65429, 40, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "angelina = os.path.join(DATASETS, \"angelina.npz\")\n",
    "# dsbi = os.path.join(DATASETS, \"dsbi.npz\")\n",
    "\n",
    "data = np.load(angelina)\n",
    "# ood = np.load(dsbi)\n",
    "print(data['train_x'].shape)\n",
    "# print(ood['test_x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = data[\"train_x\"]\n",
    "train_Y = data[\"train_y\"]\n",
    "\n",
    "N_base, p = train_Y.shape\n",
    "N = int(0.1 * N_base)\n",
    "np.random.seed(110011)\n",
    "subset = np.random.choice(np.arange(0, N_base), N)\n",
    "\n",
    "train_X = train_X[subset]\n",
    "train_Y = train_Y[subset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "np.random.seed(101010)\n",
    "subset = np.random.choice(np.arange(0, N), 10)\n",
    "\n",
    "for i in range(10):\n",
    "    sample = train_X[subset[i]]\n",
    "    plt.subplot(2,10, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    lbl = ''.join(train_Y[subset[i]].astype(str))\n",
    "    num_lbl = bin_to_num(lbl)\n",
    "    plt.imshow(sample)\n",
    "    plt.title(\"Label {}\\n(Dec: {})\".format(lbl, num_lbl))\n",
    "    \n",
    "    gray = cv2.cvtColor(sample, cv2.COLOR_BGR2GRAY)\n",
    "    plt.subplot(2,10, i+11)\n",
    "    plt.axis(\"off\")\n",
    "    plt.gray()\n",
    "    plt.imshow(gray)\n",
    "    \n",
    "    # norm = cv2.normalize(cv2.convertScaleAbs(gray, alpha=1.3, beta=0), None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    # # norm = cv2.convertScaleAbs(gray, alpha=1.5, beta=0)\n",
    "    # plt.subplot(3,10, i+21)\n",
    "    # plt.axis(\"off\")\n",
    "    # plt.imshow(norm)\n",
    "    # print(f\"Min / Max before norm: {np.min(gray)} / {np.max(gray)} and after: {round(np.min(norm), 2)}  / {round(np.max(norm), 2)}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bayes_optim import BO, ParallelBO, RealSpace, IntegerSpace, DiscreteSpace\n",
    "from bayes_optim.surrogate import RandomForest as BO_RF\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model_utils.metrics import bitwise_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myDataLoader(data_x, data_y, batch_sz):\n",
    "    tensor_x = torch.Tensor(data_x)\n",
    "    tensor_y = torch.Tensor(data_y)\n",
    "\n",
    "    return DataLoader(TensorDataset(tensor_x,tensor_y), batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 30, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = 'cuda'\n",
    "torch.set_default_device(DEV)\n",
    "\n",
    "class ConvModel(nn.Module):\n",
    "    def __init__(self, in_size=(40,30,3), filts=32, kerns=3, pad=1,\n",
    "                activ=F.relu, dense_sz=100):\n",
    "        super().__init__()\n",
    "        H, W, C = in_size\n",
    "        self.activ = activ\n",
    "        # Conv out = input_dim - kern + 2 * pad + 1\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=C, out_channels=filts, kernel_size=kerns, padding=pad)\n",
    "        out1 = (H - kerns + 2 * pad + 1, W - kerns + 2 * pad + 1, filts)\n",
    "        self.conv2 = nn.Conv2d(in_channels=filts, out_channels=filts, kernel_size=kerns, padding=pad)\n",
    "        out2 = (out1[0] - kerns + 2 * pad + 1, out1[1] - kerns + 2 * pad + 1, filts)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        out_p = (out2[0] // 2, out2[1] // 2, filts)\n",
    "        self.conv3 = nn.Conv2d(in_channels=filts, out_channels=2*filts, kernel_size=kerns, padding=pad)\n",
    "        out3 = (out_p[0] - kerns + 2 * pad + 1, out_p[1] - kerns + 2 * pad + 1, 2 * filts)\n",
    "        self.conv4 = nn.Conv2d(in_channels=2*filts, out_channels=2*filts, kernel_size=kerns, padding=pad)\n",
    "        out4 = (out3[0] - kerns + 2 * pad + 1, out3[1] - kerns + 2 * pad + 1, 2 * filts)\n",
    "        out_p2 = (out4[0] // 2, out4[1] // 2, 2 * filts)\n",
    "        self.fc1 = nn.Linear(np.product(out_p2), dense_sz)\n",
    "        self.fc2 = nn.Linear(dense_sz, 6)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # From: [batch_size, height, width, channels]\n",
    "        # To: [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = self.activ(self.conv1(x))\n",
    "        x = self.pool(self.activ(self.conv2(x)))\n",
    "        x = self.activ(self.conv2(x))\n",
    "        x = self.pool(self.activ(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activ(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class HyperModel:\n",
    "    def __init__(self, in_size=(40,30,3), filts=32, kerns=3, pad=1, # same = 1, valid = 0\n",
    "                activ=F.relu, dense_sz=100, beta_1=0.9, beta_2=0.999, lr=0.001, l2=0.002):\n",
    "        self.model = ConvModel(in_size=in_size, filts=filts, kerns=kerns, pad=pad,\n",
    "                               activ=activ, dense_sz=dense_sz)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimiser = optim.Adam(self.model.parameters(), lr=lr, betas=(beta_1, beta_2), weight_decay=l2)\n",
    "        \n",
    "        self.trained_epochs = 0\n",
    "        self.conv_epoch = 0\n",
    "        self.min_weights = self.model.state_dict()\n",
    "        self.history = {}\n",
    "        self.l2 = l2\n",
    "        \n",
    "\n",
    "    def train(self, train_loader, val_loader, epochs=10, patience=3, verbose=2):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        min_v_loss = np.inf\n",
    "        stagnant_count = 0\n",
    "        \n",
    "        prev_l, prev_a, prev_vl, prev_va = 0., 0., 0., 0.\n",
    "        for epoch in (pbar := tqdm(range(epochs), total=epochs, disable=verbose!=0)):\n",
    "            pbar.set_description(f\"Training model - l {round(prev_l,4)}, vl {round(prev_vl,4)}; a {round(prev_a,4)}, va {round(prev_va,4)}\")\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            for i, (inputs, labels) in tqdm(enumerate(train_loader), disable=(verbose < 2),\n",
    "                                desc=f\"Epoch {epoch}\", total=len(train_loader)):\n",
    "                inputs, labels = inputs.to(DEV), labels.to(DEV)\n",
    "                \n",
    "                self.optimiser.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                train_acc += bitwise_accuracy((outputs > 0.5).int(), labels)\n",
    "                \n",
    "            val_loss = 0.0\n",
    "            val_acc = 0.0\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                for j, (vinputs, vlabels) in tqdm(enumerate(val_loader), disable=(verbose < 2),\n",
    "                                desc=f\"Evaluating valdiation set\", total=len(val_loader)):\n",
    "                    vinputs, vlabels = vinputs.to(DEV), vlabels.to(DEV)\n",
    "                    \n",
    "                    voutputs = self.model(vinputs)\n",
    "                    vloss = self.criterion(voutputs, vlabels)\n",
    "                    \n",
    "                    val_loss += vloss.item()\n",
    "                    val_acc += bitwise_accuracy((voutputs > 0.5).int(), vlabels)\n",
    "            self.model.train()\n",
    "            # avg_vloss = running_vloss / len(validation_loader)\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            train_acc /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc /= len(val_loader)\n",
    "            \n",
    "            if epoch > 1:\n",
    "                if prev_vl - val_loss > 0:\n",
    "                    stagnant_count = 0\n",
    "                    if val_loss < min_v_loss:\n",
    "                        min_v_loss = val_loss\n",
    "                        self.min_weights = self.model.state_dict()\n",
    "                        self.conv_epoch = epoch\n",
    "                        # if write_dir:\n",
    "                        #     torch.save(self.model.state_dict(), model_path.format(timestamp, epoch_number))\n",
    "                else:\n",
    "                    stagnant_count += 1\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            prev_l, prev_a, prev_vl, prev_va = train_loss, train_acc, val_loss, val_acc\n",
    "            # print(prev_l, prev_a, prev_vl, prev_va)\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print(f'E{epoch}, train loss: {train_loss:.3f}, val loss: {val_loss:.3f} -- train acc: {train_acc:.4f}, val acc: {val_acc:.4f}')\n",
    "            if stagnant_count > patience:\n",
    "                break\n",
    "        \n",
    "        self.model.load_state_dict(self.min_weights)\n",
    "        self.history['train_loss'] = train_losses\n",
    "        self.history['val_loss'] = val_losses\n",
    "        self.history['train_acc'] = train_accs\n",
    "        self.history['val_acc'] = val_accs\n",
    "        self.trained_epochs = epoch+1\n",
    "        return self.history\n",
    "    \n",
    "    \n",
    "    def reevaluate(self, data_loader):\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(data_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = self.model(inputs)\n",
    "                batch_loss = self.criterion(outputs, labels)\n",
    "                loss += batch_loss.item() / len(data_loader)\n",
    "                \n",
    "                outputs = (outputs > 0.5).int()\n",
    "                batch_acc = bitwise_accuracy(outputs, labels)\n",
    "                acc += batch_acc / len(data_loader)\n",
    "        return loss, acc\n",
    "            \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = HyperModel()\n",
    "# train_loader = myDataLoader(train_X, train_Y, batch_sz=16)\n",
    "# val_loader = myDataLoader(train_X, train_Y, batch_sz=16)\n",
    "\n",
    "# model.train(train_loader, val_loader, epochs=10, patience=2)\n",
    "# model.reevaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    0: F.relu,\n",
    "    1: F.tanh\n",
    "}\n",
    "\n",
    "def train_model(model_args, #lr, beta_1, beta_2, filters, kernel_sz, dense_sz, activs, padding,\n",
    "                x_data=train_X, y_data=train_Y, batch_sz=1024, epochs=10, pat=3, verbose=-1, return_all=False):\n",
    "    # lr, beta_1, beta_2, filters, kernel_sz, dense_sz, activs, padding = model_args\n",
    "    lr, beta_1, beta_2, l2, filters, kernel_sz, dense_sz, activs, padding = model_args\n",
    "    print(f\"lr {round(lr, 4)}, beta_1 {round(beta_1, 4)}, beta_2 {round(beta_2, 4)}, l2 {round(l2, 4)}, filters {filters}, kernel_sz {kernel_sz}, \",\n",
    "          f\"dense_sz {dense_sz}, activs {activations[activs].__name__}, padding {padding}\")\n",
    "    \n",
    "    kfold = KFold(10, shuffle=True, random_state=1)\n",
    "    \n",
    "    accs, losses, ratios = [], [], []\n",
    "    start_t = time.time()\n",
    "    for  f, (train_ix, val_ix) in enumerate(kfold.split(x_data)):\n",
    "        f_t = time.time()\n",
    "        model = HyperModel(in_size=x_data.shape[1:], filts=filters, kerns=kernel_sz, pad=padding,\n",
    "                            activ=activations[activs], dense_sz=dense_sz, beta_1=beta_1, beta_2=beta_2, lr=lr, l2=l2)\n",
    "        \n",
    "        trainX, trainY, valX, valY = x_data[train_ix], y_data[train_ix], x_data[val_ix], y_data[val_ix]\n",
    "        train_loader = myDataLoader(trainX, trainY, batch_sz=batch_sz)\n",
    "        val_loader = myDataLoader(valX, valY, batch_sz=batch_sz)\n",
    "                \n",
    "        history = model.train(train_loader, val_loader, epochs=epochs, patience=pat, verbose=verbose)\n",
    "        # t_loss, t_acc = model.reevaluate(train_loader)\n",
    "        # loss, acc = model.reevaluate(val_loader)\n",
    "        ce = model.conv_epoch\n",
    "        t_loss, t_acc = history['train_loss'][ce], history['train_acc'][ce]\n",
    "        loss, acc = history['val_loss'][ce], history['val_acc'][ce]\n",
    "        \n",
    "        print(f\"Fold {f} trained {model.trained_epochs} epochs in {round(time.time() - f_t, 3)}s. \",\n",
    "              f\"Loss: tr - {round(t_loss, 3)}, val - {round(loss, 3)}. \",\n",
    "              f\"Accuracy: tr - {round(t_acc, 3)}, val - {round(acc, 3)}.\")\n",
    "        \n",
    "        accs.append(acc)\n",
    "        losses.append(loss)\n",
    "        ratios.append(t_loss / loss)\n",
    "    print(f\"Cross-validation completed in {round(time.time() - start_t, 3)}s. Mean validation loss {round(np.mean(losses), 3)} and acc {round(np.mean(accs), 3)}\")\n",
    "    print('-'*75)\n",
    "    if return_all:\n",
    "        return losses, accs, ratios\n",
    "    else:\n",
    "        return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAYES_LOGS = os.path.join(OPT_LOGS, 'bayes_opt')\n",
    "if not os.path.exists(BAYES_LOGS):\n",
    "    os.makedirs(BAYES_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/15/2024 07:30:26,918 - [BO (140569664263600).INFO] -- iteration 0 starts...\n",
      "05/15/2024 07:30:26,922 - [BO (140569664263600).INFO] -- asking 5 points (using DoE):\n",
      "05/15/2024 07:30:26,922 - [BO (140569664263600).INFO] -- #1 - [0.0015980446892244953, 0.8748010369762073, 0.9937457121504588, 0.014932905112709364, 43, 3, 43, 0, 0]\n",
      "05/15/2024 07:30:26,922 - [BO (140569664263600).INFO] -- #2 - [0.006693050756686904, 0.8527526656418187, 0.9944438564253367, 0.005823304065378988, 30, 5, 104, 1, 0]\n",
      "05/15/2024 07:30:26,922 - [BO (140569664263600).INFO] -- #3 - [0.0035950632006830213, 0.9287188703972543, 0.990451370583813, 0.0020792759138041177, 36, 4, 106, 0, 1]\n",
      "05/15/2024 07:30:26,923 - [BO (140569664263600).INFO] -- #4 - [0.009132595883531179, 0.9380800407589336, 0.9969872559320065, 0.00999763758891005, 50, 2, 53, 1, 1]\n",
      "05/15/2024 07:30:26,923 - [BO (140569664263600).INFO] -- #5 - [0.005390295993236286, 0.9083616445573646, 0.9984248224366956, 0.019590257752595815, 26, 2, 147, 1, 0]\n",
      "05/15/2024 07:30:26,923 - [BO (140569664263600).INFO] -- ask takes 0.0049s\n",
      "lr 0.0016, beta_1 0.8748, beta_2 0.9937, l2 0.0149, filters 43, kernel_sz 3,  dense_sz 43, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.342s.  Loss: tr - 0.089, val - 0.068.  Accuracy: tr - 0.969, val - 0.978.\n",
      "Fold 1 trained 10 epochs in 4.336s.  Loss: tr - 0.045, val - 0.036.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 2 trained 10 epochs in 4.366s.  Loss: tr - 0.344, val - 0.277.  Accuracy: tr - 0.847, val - 0.883.\n",
      "Fold 3 trained 10 epochs in 4.331s.  Loss: tr - 0.041, val - 0.049.  Accuracy: tr - 0.986, val - 0.985.\n",
      "Fold 4 trained 10 epochs in 4.675s.  Loss: tr - 0.11, val - 0.075.  Accuracy: tr - 0.963, val - 0.975.\n",
      "Fold 5 trained 10 epochs in 4.349s.  Loss: tr - 0.09, val - 0.076.  Accuracy: tr - 0.967, val - 0.973.\n",
      "Fold 6 trained 10 epochs in 4.366s.  Loss: tr - 0.221, val - 0.187.  Accuracy: tr - 0.907, val - 0.928.\n",
      "Fold 7 trained 10 epochs in 4.428s.  Loss: tr - 0.055, val - 0.058.  Accuracy: tr - 0.981, val - 0.98.\n",
      "Fold 8 trained 10 epochs in 4.38s.  Loss: tr - 0.08, val - 0.056.  Accuracy: tr - 0.974, val - 0.98.\n",
      "Fold 9 trained 10 epochs in 4.425s.  Loss: tr - 0.077, val - 0.069.  Accuracy: tr - 0.974, val - 0.976.\n",
      "Cross-validation completed in 44.002s. Mean validation loss 0.095 and acc 0.965\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0067, beta_1 0.8528, beta_2 0.9944, l2 0.0058, filters 30, kernel_sz 5,  dense_sz 104, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.525s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.588, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 4.18s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 4.218s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.591, val - 0.583.\n",
      "Fold 3 trained 10 epochs in 4.207s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.591, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 4.217s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 4.473s.  Loss: tr - 0.659, val - 0.652.  Accuracy: tr - 0.589, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 4.24s.  Loss: tr - 0.658, val - 0.661.  Accuracy: tr - 0.583, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 4.272s.  Loss: tr - 0.66, val - 0.664.  Accuracy: tr - 0.582, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 4.19s.  Loss: tr - 0.657, val - 0.657.  Accuracy: tr - 0.593, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 4.213s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.589, val - 0.592.\n",
      "Cross-validation completed in 42.739s. Mean validation loss 0.656 and acc 0.593\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0036, beta_1 0.9287, beta_2 0.9905, l2 0.0021, filters 36, kernel_sz 4,  dense_sz 106, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.404s.  Loss: tr - 0.124, val - 0.079.  Accuracy: tr - 0.949, val - 0.968.\n",
      "Fold 1 trained 10 epochs in 6.18s.  Loss: tr - 0.153, val - 0.111.  Accuracy: tr - 0.941, val - 0.961.\n",
      "Fold 2 trained 10 epochs in 6.065s.  Loss: tr - 0.651, val - 0.631.  Accuracy: tr - 0.617, val - 0.6.\n",
      "Fold 3 trained 10 epochs in 6.043s.  Loss: tr - 0.091, val - 0.086.  Accuracy: tr - 0.968, val - 0.969.\n",
      "Fold 4 trained 10 epochs in 6.098s.  Loss: tr - 0.078, val - 0.058.  Accuracy: tr - 0.972, val - 0.979.\n",
      "Fold 5 trained 10 epochs in 6.391s.  Loss: tr - 0.189, val - 0.13.  Accuracy: tr - 0.93, val - 0.956.\n",
      "Fold 6 trained 10 epochs in 6.065s.  Loss: tr - 0.3, val - 0.23.  Accuracy: tr - 0.885, val - 0.92.\n",
      "Fold 7 trained 10 epochs in 6.041s.  Loss: tr - 0.467, val - 0.361.  Accuracy: tr - 0.787, val - 0.864.\n",
      "Fold 8 trained 10 epochs in 6.095s.  Loss: tr - 0.072, val - 0.057.  Accuracy: tr - 0.976, val - 0.981.\n",
      "Fold 9 trained 10 epochs in 6.052s.  Loss: tr - 0.042, val - 0.038.  Accuracy: tr - 0.987, val - 0.988.\n",
      "Cross-validation completed in 61.438s. Mean validation loss 0.178 and acc 0.919\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0091, beta_1 0.9381, beta_2 0.997, l2 0.01, filters 50, kernel_sz 2,  dense_sz 53, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 5.853s.  Loss: tr - 0.663, val - 0.66.  Accuracy: tr - 0.586, val - 0.573.\n",
      "Fold 1 trained 10 epochs in 5.626s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.583, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 5.621s.  Loss: tr - 0.662, val - 0.654.  Accuracy: tr - 0.585, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 5.599s.  Loss: tr - 0.66, val - 0.65.  Accuracy: tr - 0.586, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 5.584s.  Loss: tr - 0.667, val - 0.657.  Accuracy: tr - 0.591, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 5.874s.  Loss: tr - 0.659, val - 0.653.  Accuracy: tr - 0.589, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 5.645s.  Loss: tr - 0.656, val - 0.661.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.594s.  Loss: tr - 0.661, val - 0.666.  Accuracy: tr - 0.581, val - 0.587.\n",
      "Fold 8 trained 10 epochs in 5.588s.  Loss: tr - 0.661, val - 0.66.  Accuracy: tr - 0.583, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 5.611s.  Loss: tr - 0.66, val - 0.66.  Accuracy: tr - 0.585, val - 0.587.\n",
      "Cross-validation completed in 56.599s. Mean validation loss 0.658 and acc 0.591\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0054, beta_1 0.9084, beta_2 0.9984, l2 0.0196, filters 26, kernel_sz 2,  dense_sz 147, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 2.328s.  Loss: tr - 0.125, val - 0.105.  Accuracy: tr - 0.966, val - 0.979.\n",
      "Fold 1 trained 10 epochs in 2.588s.  Loss: tr - 0.475, val - 0.411.  Accuracy: tr - 0.831, val - 0.826.\n",
      "Fold 2 trained 10 epochs in 2.4s.  Loss: tr - 0.66, val - 0.397.  Accuracy: tr - 0.69, val - 0.845.\n",
      "Fold 3 trained 10 epochs in 2.348s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 2.37s.  Loss: tr - 0.094, val - 0.089.  Accuracy: tr - 0.98, val - 0.979.\n",
      "Fold 5 trained 10 epochs in 2.398s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.594, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 2.326s.  Loss: tr - 0.657, val - 0.661.  Accuracy: tr - 0.593, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 2.399s.  Loss: tr - 0.553, val - 0.515.  Accuracy: tr - 0.729, val - 0.746.\n",
      "Fold 8 trained 10 epochs in 2.665s.  Loss: tr - 0.225, val - 0.18.  Accuracy: tr - 0.943, val - 0.953.\n",
      "Fold 9 trained 10 epochs in 2.368s.  Loss: tr - 0.627, val - 0.593.  Accuracy: tr - 0.66, val - 0.704.\n",
      "Cross-validation completed in 24.193s. Mean validation loss 0.425 and acc 0.782\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:34:15,910 - [BO (140569664263600).INFO] -- evaluate takes 228.9861s\n",
      "05/15/2024 07:34:15,910 - [BO (140569664263600).INFO] -- observing 5 points:\n",
      "05/15/2024 07:34:15,910 - [BO (140569664263600).INFO] -- #1 - fitness: 0.09503873214125633, solution: [0.0015980446892244953, 0.8748010369762073, 0.9937457121504588, 0.014932905112709364, 43, 3, 43, 0, 0]\n",
      "05/15/2024 07:34:15,911 - [BO (140569664263600).INFO] -- #2 - fitness: 0.656401515007019, solution: [0.006693050756686904, 0.8527526656418187, 0.9944438564253367, 0.005823304065378988, 30, 5, 104, 1, 0]\n",
      "05/15/2024 07:34:15,911 - [BO (140569664263600).INFO] -- #3 - fitness: 0.178049461171031, solution: [0.0035950632006830213, 0.9287188703972543, 0.990451370583813, 0.0020792759138041177, 36, 4, 106, 0, 1]\n",
      "05/15/2024 07:34:15,911 - [BO (140569664263600).INFO] -- #4 - fitness: 0.6575806975364685, solution: [0.009132595883531179, 0.9380800407589336, 0.9969872559320065, 0.00999763758891005, 50, 2, 53, 1, 1]\n",
      "05/15/2024 07:34:15,912 - [BO (140569664263600).INFO] -- #5 - fitness: 0.42536759972572324, solution: [0.005390295993236286, 0.9083616445573646, 0.9984248224366956, 0.019590257752595815, 26, 2, 147, 1, 0]\n",
      "05/15/2024 07:34:15,929 - [BO (140569664263600).INFO] -- model r2: 0.5200198523892801, MAPE: 0.6996223847415111\n",
      "05/15/2024 07:34:15,930 - [BO (140569664263600).INFO] -- fopt: [0.09503873]\n",
      "05/15/2024 07:34:15,930 - [BO (140569664263600).INFO] -- xopt: [0.0015980446892244953, 0.8748010369762073, 0.9937457121504588, 0.014932905112709364, 43, 3, 43, 0, 0]\n",
      "\n",
      "05/15/2024 07:34:15,930 - [BO (140569664263600).INFO] -- tell takes 0.0203s\n",
      "05/15/2024 07:34:15,931 - [BO (140569664263600).INFO] -- iteration 1 starts...\n",
      "05/15/2024 07:34:16,116 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.1850s\n",
      "05/15/2024 07:34:16,117 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:34:16,117 - [BO (140569664263600).INFO] -- #1 - [0.002603140164848708, 0.9439486907012304, 0.9925432618750704, 0.0064384457996822, 51, 3, 79, 0, 1]\n",
      "05/15/2024 07:34:16,117 - [BO (140569664263600).INFO] -- ask takes 0.1863s\n",
      "lr 0.0026, beta_1 0.9439, beta_2 0.9925, l2 0.0064, filters 51, kernel_sz 3,  dense_sz 79, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.109s.  Loss: tr - 0.293, val - 0.254.  Accuracy: tr - 0.869, val - 0.884.\n",
      "Fold 1 trained 10 epochs in 6.102s.  Loss: tr - 0.096, val - 0.089.  Accuracy: tr - 0.964, val - 0.969.\n",
      "Fold 2 trained 10 epochs in 6.113s.  Loss: tr - 0.043, val - 0.047.  Accuracy: tr - 0.984, val - 0.987.\n",
      "Fold 3 trained 10 epochs in 6.381s.  Loss: tr - 0.546, val - 0.474.  Accuracy: tr - 0.744, val - 0.799.\n",
      "Fold 4 trained 10 epochs in 6.121s.  Loss: tr - 0.042, val - 0.034.  Accuracy: tr - 0.985, val - 0.987.\n",
      "Fold 5 trained 10 epochs in 6.11s.  Loss: tr - 0.045, val - 0.045.  Accuracy: tr - 0.985, val - 0.989.\n",
      "Fold 6 trained 10 epochs in 6.109s.  Loss: tr - 0.233, val - 0.178.  Accuracy: tr - 0.901, val - 0.93.\n",
      "Fold 7 trained 10 epochs in 6.16s.  Loss: tr - 0.128, val - 0.102.  Accuracy: tr - 0.95, val - 0.956.\n",
      "Fold 8 trained 10 epochs in 6.134s.  Loss: tr - 0.122, val - 0.093.  Accuracy: tr - 0.956, val - 0.967.\n",
      "Fold 9 trained 10 epochs in 6.169s.  Loss: tr - 0.306, val - 0.267.  Accuracy: tr - 0.865, val - 0.891.\n",
      "Cross-validation completed in 61.512s. Mean validation loss 0.158 and acc 0.936\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:35:17,632 - [BO (140569664263600).INFO] -- evaluate takes 61.5149s\n",
      "05/15/2024 07:35:17,633 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:35:17,633 - [BO (140569664263600).INFO] -- #1 - fitness: 0.15805463530123234, solution: [0.002603140164848708, 0.9439486907012304, 0.9925432618750704, 0.0064384457996822, 51, 3, 79, 0, 1]\n",
      "05/15/2024 07:35:17,650 - [BO (140569664263600).INFO] -- model r2: 0.7687316302411453, MAPE: 0.5704257085268983\n",
      "05/15/2024 07:35:17,651 - [BO (140569664263600).INFO] -- fopt: [0.09503873]\n",
      "05/15/2024 07:35:17,651 - [BO (140569664263600).INFO] -- xopt: [0.0015980446892244953, 0.8748010369762073, 0.9937457121504588, 0.014932905112709364, 43, 3, 43, 0, 0]\n",
      "\n",
      "05/15/2024 07:35:17,651 - [BO (140569664263600).INFO] -- tell takes 0.0187s\n",
      "05/15/2024 07:35:17,652 - [BO (140569664263600).INFO] -- iteration 2 starts...\n",
      "05/15/2024 07:35:18,119 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.4670s\n",
      "05/15/2024 07:35:18,120 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:35:18,120 - [BO (140569664263600).INFO] -- #1 - [0.0038321707144488614, 0.9443795888699515, 0.9938743588062334, 0.0051365756077608495, 49, 4, 110, 0, 1]\n",
      "05/15/2024 07:35:18,120 - [BO (140569664263600).INFO] -- ask takes 0.4684s\n",
      "lr 0.0038, beta_1 0.9444, beta_2 0.9939, l2 0.0051, filters 49, kernel_sz 4,  dense_sz 110, activs relu, padding 1\n",
      "Fold 0 trained 6 epochs in 4.999s.  Loss: tr - 36.851, val - 43.817.  Accuracy: tr - 0.564, val - 0.562.\n",
      "Fold 1 trained 10 epochs in 7.809s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 7.814s.  Loss: tr - 0.652, val - 0.648.  Accuracy: tr - 0.608, val - 0.68.\n",
      "Fold 3 trained 6 epochs in 4.764s.  Loss: tr - 40.796, val - 49.414.  Accuracy: tr - 0.517, val - 0.506.\n",
      "Fold 4 trained 10 epochs in 7.8s.  Loss: tr - 0.647, val - 0.631.  Accuracy: tr - 0.647, val - 0.636.\n",
      "Fold 5 trained 6 epochs in 4.696s.  Loss: tr - 35.08, val - 41.233.  Accuracy: tr - 0.563, val - 0.588.\n",
      "Fold 6 trained 10 epochs in 8.094s.  Loss: tr - 0.894, val - 0.456.  Accuracy: tr - 0.736, val - 0.798.\n",
      "Fold 7 trained 10 epochs in 7.842s.  Loss: tr - 0.655, val - 0.661.  Accuracy: tr - 0.599, val - 0.616.\n",
      "Fold 8 trained 10 epochs in 7.859s.  Loss: tr - 0.6, val - 0.572.  Accuracy: tr - 0.676, val - 0.769.\n",
      "Fold 9 trained 6 epochs in 4.692s.  Loss: tr - 34.478, val - 43.17.  Accuracy: tr - 0.587, val - 0.568.\n",
      "Cross-validation completed in 66.374s. Mean validation loss 18.126 and acc 0.632\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:36:24,498 - [BO (140569664263600).INFO] -- evaluate takes 66.3772s\n",
      "05/15/2024 07:36:24,498 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:36:24,498 - [BO (140569664263600).INFO] -- #1 - fitness: 18.12574348449707, solution: [0.0038321707144488614, 0.9443795888699515, 0.9938743588062334, 0.0051365756077608495, 49, 4, 110, 0, 1]\n",
      "05/15/2024 07:36:24,515 - [BO (140569664263600).INFO] -- model r2: 0.41226476377773336, MAPE: 1.0611364628356725\n",
      "05/15/2024 07:36:24,516 - [BO (140569664263600).INFO] -- fopt: [0.09503873]\n",
      "05/15/2024 07:36:24,516 - [BO (140569664263600).INFO] -- xopt: [0.0015980446892244953, 0.8748010369762073, 0.9937457121504588, 0.014932905112709364, 43, 3, 43, 0, 0]\n",
      "\n",
      "05/15/2024 07:36:24,516 - [BO (140569664263600).INFO] -- tell takes 0.0183s\n",
      "05/15/2024 07:36:24,517 - [BO (140569664263600).INFO] -- iteration 3 starts...\n",
      "05/15/2024 07:36:24,711 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.1947s\n",
      "05/15/2024 07:36:24,712 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:36:24,713 - [BO (140569664263600).INFO] -- #1 - [0.0009313627634460445, 0.8516779740618876, 0.9916449674201719, 0.00307147995958296, 18, 2, 89, 0, 1]\n",
      "05/15/2024 07:36:24,713 - [BO (140569664263600).INFO] -- ask takes 0.1961s\n",
      "lr 0.0009, beta_1 0.8517, beta_2 0.9916, l2 0.0031, filters 18, kernel_sz 2,  dense_sz 89, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 2.106s.  Loss: tr - 0.182, val - 0.146.  Accuracy: tr - 0.949, val - 0.95.\n",
      "Fold 1 trained 10 epochs in 2.091s.  Loss: tr - 0.13, val - 0.107.  Accuracy: tr - 0.964, val - 0.974.\n",
      "Fold 2 trained 10 epochs in 2.162s.  Loss: tr - 0.099, val - 0.075.  Accuracy: tr - 0.969, val - 0.982.\n",
      "Fold 3 trained 10 epochs in 2.404s.  Loss: tr - 0.403, val - 0.355.  Accuracy: tr - 0.854, val - 0.852.\n",
      "Fold 4 trained 10 epochs in 2.183s.  Loss: tr - 0.146, val - 0.119.  Accuracy: tr - 0.954, val - 0.958.\n",
      "Fold 5 trained 10 epochs in 2.104s.  Loss: tr - 0.087, val - 0.073.  Accuracy: tr - 0.977, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 2.133s.  Loss: tr - 0.081, val - 0.069.  Accuracy: tr - 0.98, val - 0.977.\n",
      "Fold 7 trained 10 epochs in 2.124s.  Loss: tr - 0.546, val - 0.509.  Accuracy: tr - 0.747, val - 0.798.\n",
      "Fold 8 trained 10 epochs in 2.376s.  Loss: tr - 0.385, val - 0.344.  Accuracy: tr - 0.864, val - 0.88.\n",
      "Fold 9 trained 10 epochs in 2.151s.  Loss: tr - 0.203, val - 0.172.  Accuracy: tr - 0.927, val - 0.942.\n",
      "Cross-validation completed in 21.838s. Mean validation loss 0.197 and acc 0.929\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:36:46,554 - [BO (140569664263600).INFO] -- evaluate takes 21.8409s\n",
      "05/15/2024 07:36:46,554 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:36:46,555 - [BO (140569664263600).INFO] -- #1 - fitness: 0.19696348384022713, solution: [0.0009313627634460445, 0.8516779740618876, 0.9916449674201719, 0.00307147995958296, 18, 2, 89, 0, 1]\n",
      "05/15/2024 07:36:46,572 - [BO (140569664263600).INFO] -- model r2: 0.38936800895234414, MAPE: 0.7746707475688561\n",
      "05/15/2024 07:36:46,572 - [BO (140569664263600).INFO] -- fopt: [0.09503873]\n",
      "05/15/2024 07:36:46,573 - [BO (140569664263600).INFO] -- xopt: [0.0015980446892244953, 0.8748010369762073, 0.9937457121504588, 0.014932905112709364, 43, 3, 43, 0, 0]\n",
      "\n",
      "05/15/2024 07:36:46,573 - [BO (140569664263600).INFO] -- tell takes 0.0185s\n",
      "05/15/2024 07:36:46,573 - [BO (140569664263600).INFO] -- iteration 4 starts...\n",
      "05/15/2024 07:36:46,926 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.3527s\n",
      "05/15/2024 07:36:46,927 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:36:46,927 - [BO (140569664263600).INFO] -- #1 - [0.00206126947477023, 0.9398887397405752, 0.9918292606254768, 0.014626004430202831, 27, 4, 80, 0, 1]\n",
      "05/15/2024 07:36:46,927 - [BO (140569664263600).INFO] -- ask takes 0.3541s\n",
      "lr 0.0021, beta_1 0.9399, beta_2 0.9918, l2 0.0146, filters 27, kernel_sz 4,  dense_sz 80, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.246s.  Loss: tr - 0.049, val - 0.04.  Accuracy: tr - 0.982, val - 0.986.\n",
      "Fold 1 trained 10 epochs in 5.271s.  Loss: tr - 0.032, val - 0.026.  Accuracy: tr - 0.991, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 5.195s.  Loss: tr - 0.232, val - 0.15.  Accuracy: tr - 0.92, val - 0.946.\n",
      "Fold 3 trained 10 epochs in 5.488s.  Loss: tr - 0.118, val - 0.103.  Accuracy: tr - 0.954, val - 0.963.\n",
      "Fold 4 trained 10 epochs in 5.208s.  Loss: tr - 0.047, val - 0.038.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 5 trained 10 epochs in 5.231s.  Loss: tr - 0.108, val - 0.095.  Accuracy: tr - 0.964, val - 0.968.\n",
      "Fold 6 trained 10 epochs in 5.199s.  Loss: tr - 0.085, val - 0.075.  Accuracy: tr - 0.969, val - 0.974.\n",
      "Fold 7 trained 10 epochs in 5.218s.  Loss: tr - 0.064, val - 0.058.  Accuracy: tr - 0.976, val - 0.98.\n",
      "Fold 8 trained 10 epochs in 5.243s.  Loss: tr - 0.045, val - 0.039.  Accuracy: tr - 0.988, val - 0.991.\n",
      "Fold 9 trained 10 epochs in 5.204s.  Loss: tr - 0.055, val - 0.045.  Accuracy: tr - 0.981, val - 0.985.\n",
      "Cross-validation completed in 52.508s. Mean validation loss 0.067 and acc 0.977\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:37:39,439 - [BO (140569664263600).INFO] -- evaluate takes 52.5111s\n",
      "05/15/2024 07:37:39,439 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:37:39,440 - [BO (140569664263600).INFO] -- #1 - fitness: 0.06691812127828597, solution: [0.00206126947477023, 0.9398887397405752, 0.9918292606254768, 0.014626004430202831, 27, 4, 80, 0, 1]\n",
      "05/15/2024 07:37:39,457 - [BO (140569664263600).INFO] -- model r2: 0.3439567294303971, MAPE: 0.7935629501922182\n",
      "05/15/2024 07:37:39,457 - [BO (140569664263600).INFO] -- fopt: [0.06691812]\n",
      "05/15/2024 07:37:39,458 - [BO (140569664263600).INFO] -- xopt: [0.00206126947477023, 0.9398887397405752, 0.9918292606254768, 0.014626004430202831, 27, 4, 80, 0, 1]\n",
      "\n",
      "05/15/2024 07:37:39,458 - [BO (140569664263600).INFO] -- tell takes 0.0184s\n",
      "05/15/2024 07:37:39,458 - [BO (140569664263600).INFO] -- iteration 5 starts...\n",
      "05/15/2024 07:37:40,970 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5115s\n",
      "05/15/2024 07:37:40,971 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:37:40,971 - [BO (140569664263600).INFO] -- #1 - [0.0037936908396565406, 0.9250717626221148, 0.9921496514162955, 0.0108549933607532, 63, 2, 50, 0, 1]\n",
      "05/15/2024 07:37:40,971 - [BO (140569664263600).INFO] -- ask takes 1.5130s\n",
      "lr 0.0038, beta_1 0.9251, beta_2 0.9921, l2 0.0109, filters 63, kernel_sz 2,  dense_sz 50, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.335s.  Loss: tr - 0.619, val - 0.598.  Accuracy: tr - 0.667, val - 0.614.\n",
      "Fold 1 trained 10 epochs in 8.038s.  Loss: tr - 0.123, val - 0.104.  Accuracy: tr - 0.952, val - 0.955.\n",
      "Fold 2 trained 10 epochs in 8.122s.  Loss: tr - 0.168, val - 0.114.  Accuracy: tr - 0.935, val - 0.96.\n",
      "Fold 3 trained 10 epochs in 8.086s.  Loss: tr - 0.629, val - 0.608.  Accuracy: tr - 0.635, val - 0.709.\n",
      "Fold 4 trained 10 epochs in 8.136s.  Loss: tr - 0.619, val - 0.577.  Accuracy: tr - 0.663, val - 0.698.\n",
      "Fold 5 trained 10 epochs in 8.396s.  Loss: tr - 0.399, val - 0.277.  Accuracy: tr - 0.831, val - 0.906.\n",
      "Fold 6 trained 10 epochs in 8.144s.  Loss: tr - 0.557, val - 0.534.  Accuracy: tr - 0.753, val - 0.685.\n",
      "Fold 7 trained 10 epochs in 8.158s.  Loss: tr - 0.635, val - 0.633.  Accuracy: tr - 0.637, val - 0.589.\n",
      "Fold 8 trained 10 epochs in 8.073s.  Loss: tr - 0.062, val - 0.055.  Accuracy: tr - 0.979, val - 0.98.\n",
      "Fold 9 trained 10 epochs in 8.108s.  Loss: tr - 0.406, val - 0.362.  Accuracy: tr - 0.845, val - 0.901.\n",
      "Cross-validation completed in 81.602s. Mean validation loss 0.386 and acc 0.8\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:39:02,577 - [BO (140569664263600).INFO] -- evaluate takes 81.6054s\n",
      "05/15/2024 07:39:02,578 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:39:02,578 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3860743440687656, solution: [0.0037936908396565406, 0.9250717626221148, 0.9921496514162955, 0.0108549933607532, 63, 2, 50, 0, 1]\n",
      "05/15/2024 07:39:02,595 - [BO (140569664263600).INFO] -- model r2: 0.37015901197570455, MAPE: 0.6920814088640174\n",
      "05/15/2024 07:39:02,596 - [BO (140569664263600).INFO] -- fopt: [0.06691812]\n",
      "05/15/2024 07:39:02,596 - [BO (140569664263600).INFO] -- xopt: [0.00206126947477023, 0.9398887397405752, 0.9918292606254768, 0.014626004430202831, 27, 4, 80, 0, 1]\n",
      "\n",
      "05/15/2024 07:39:02,596 - [BO (140569664263600).INFO] -- tell takes 0.0185s\n",
      "05/15/2024 07:39:02,596 - [BO (140569664263600).INFO] -- iteration 6 starts...\n",
      "05/15/2024 07:39:03,277 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.6807s\n",
      "05/15/2024 07:39:03,278 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:39:03,278 - [BO (140569664263600).INFO] -- #1 - [0.0024999081054341153, 0.948310288939078, 0.9939105595947907, 0.003499121958815187, 50, 4, 44, 0, 1]\n",
      "05/15/2024 07:39:03,278 - [BO (140569664263600).INFO] -- ask takes 0.6819s\n",
      "lr 0.0025, beta_1 0.9483, beta_2 0.9939, l2 0.0035, filters 50, kernel_sz 4,  dense_sz 44, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.788s.  Loss: tr - 0.514, val - 0.483.  Accuracy: tr - 0.731, val - 0.764.\n",
      "Fold 1 trained 10 epochs in 8.078s.  Loss: tr - 0.653, val - 0.646.  Accuracy: tr - 0.615, val - 0.64.\n",
      "Fold 2 trained 10 epochs in 7.805s.  Loss: tr - 0.491, val - 0.405.  Accuracy: tr - 0.782, val - 0.82.\n",
      "Fold 3 trained 10 epochs in 7.813s.  Loss: tr - 0.387, val - 0.357.  Accuracy: tr - 0.832, val - 0.843.\n",
      "Fold 4 trained 10 epochs in 7.807s.  Loss: tr - 0.599, val - 0.561.  Accuracy: tr - 0.676, val - 0.726.\n",
      "Fold 5 trained 10 epochs in 7.819s.  Loss: tr - 0.088, val - 0.07.  Accuracy: tr - 0.968, val - 0.978.\n",
      "Fold 6 trained 10 epochs in 8.059s.  Loss: tr - 0.616, val - 0.576.  Accuracy: tr - 0.691, val - 0.68.\n",
      "Fold 7 trained 10 epochs in 7.773s.  Loss: tr - 0.2, val - 0.144.  Accuracy: tr - 0.917, val - 0.944.\n",
      "Fold 8 trained 10 epochs in 7.754s.  Loss: tr - 0.264, val - 0.186.  Accuracy: tr - 0.901, val - 0.93.\n",
      "Fold 9 trained 10 epochs in 7.817s.  Loss: tr - 0.653, val - 0.657.  Accuracy: tr - 0.604, val - 0.592.\n",
      "Cross-validation completed in 78.517s. Mean validation loss 0.408 and acc 0.792\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:40:21,798 - [BO (140569664263600).INFO] -- evaluate takes 78.5198s\n",
      "05/15/2024 07:40:21,799 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:40:21,799 - [BO (140569664263600).INFO] -- #1 - fitness: 0.40829405263066293, solution: [0.0024999081054341153, 0.948310288939078, 0.9939105595947907, 0.003499121958815187, 50, 4, 44, 0, 1]\n",
      "05/15/2024 07:40:21,816 - [BO (140569664263600).INFO] -- model r2: 0.36052719363522867, MAPE: 0.7126004870934529\n",
      "05/15/2024 07:40:21,817 - [BO (140569664263600).INFO] -- fopt: [0.06691812]\n",
      "05/15/2024 07:40:21,817 - [BO (140569664263600).INFO] -- xopt: [0.00206126947477023, 0.9398887397405752, 0.9918292606254768, 0.014626004430202831, 27, 4, 80, 0, 1]\n",
      "\n",
      "05/15/2024 07:40:21,817 - [BO (140569664263600).INFO] -- tell takes 0.0183s\n",
      "05/15/2024 07:40:21,817 - [BO (140569664263600).INFO] -- iteration 7 starts...\n",
      "05/15/2024 07:40:23,127 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3089s\n",
      "05/15/2024 07:40:23,127 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:40:23,128 - [BO (140569664263600).INFO] -- #1 - [0.00011032802398996543, 0.9162824232314734, 0.9922322146730488, 0.000799304207940413, 27, 3, 149, 0, 0]\n",
      "05/15/2024 07:40:23,128 - [BO (140569664263600).INFO] -- ask takes 1.3103s\n",
      "lr 0.0001, beta_1 0.9163, beta_2 0.9922, l2 0.0008, filters 27, kernel_sz 3,  dense_sz 149, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.055s.  Loss: tr - 0.168, val - 0.14.  Accuracy: tr - 0.953, val - 0.967.\n",
      "Fold 1 trained 10 epochs in 4.325s.  Loss: tr - 0.3, val - 0.254.  Accuracy: tr - 0.933, val - 0.943.\n",
      "Fold 2 trained 10 epochs in 4.075s.  Loss: tr - 0.26, val - 0.217.  Accuracy: tr - 0.936, val - 0.943.\n",
      "Fold 3 trained 10 epochs in 4.026s.  Loss: tr - 0.303, val - 0.283.  Accuracy: tr - 0.919, val - 0.913.\n",
      "Fold 4 trained 10 epochs in 4.088s.  Loss: tr - 0.395, val - 0.353.  Accuracy: tr - 0.89, val - 0.897.\n",
      "Fold 5 trained 10 epochs in 4.101s.  Loss: tr - 0.392, val - 0.356.  Accuracy: tr - 0.895, val - 0.903.\n",
      "Fold 6 trained 10 epochs in 4.359s.  Loss: tr - 0.249, val - 0.219.  Accuracy: tr - 0.933, val - 0.935.\n",
      "Fold 7 trained 10 epochs in 4.035s.  Loss: tr - 0.396, val - 0.356.  Accuracy: tr - 0.91, val - 0.922.\n",
      "Fold 8 trained 10 epochs in 4.107s.  Loss: tr - 0.348, val - 0.301.  Accuracy: tr - 0.915, val - 0.923.\n",
      "Fold 9 trained 10 epochs in 4.084s.  Loss: tr - 0.509, val - 0.495.  Accuracy: tr - 0.821, val - 0.827.\n",
      "Cross-validation completed in 41.259s. Mean validation loss 0.297 and acc 0.917\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:41:04,390 - [BO (140569664263600).INFO] -- evaluate takes 41.2623s\n",
      "05/15/2024 07:41:04,391 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:41:04,391 - [BO (140569664263600).INFO] -- #1 - fitness: 0.29733605682849884, solution: [0.00011032802398996543, 0.9162824232314734, 0.9922322146730488, 0.000799304207940413, 27, 3, 149, 0, 0]\n",
      "05/15/2024 07:41:04,408 - [BO (140569664263600).INFO] -- model r2: 0.3789234851121409, MAPE: 1.4679747337364688\n",
      "05/15/2024 07:41:04,409 - [BO (140569664263600).INFO] -- fopt: [0.06691812]\n",
      "05/15/2024 07:41:04,409 - [BO (140569664263600).INFO] -- xopt: [0.00206126947477023, 0.9398887397405752, 0.9918292606254768, 0.014626004430202831, 27, 4, 80, 0, 1]\n",
      "\n",
      "05/15/2024 07:41:04,409 - [BO (140569664263600).INFO] -- tell takes 0.0186s\n",
      "05/15/2024 07:41:04,410 - [BO (140569664263600).INFO] -- iteration 8 starts...\n",
      "05/15/2024 07:41:04,999 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.5892s\n",
      "05/15/2024 07:41:05,00 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:41:05,00 - [BO (140569664263600).INFO] -- #1 - [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "05/15/2024 07:41:05,01 - [BO (140569664263600).INFO] -- ask takes 0.5906s\n",
      "lr 0.0004, beta_1 0.8526, beta_2 0.9906, l2 0.0137, filters 54, kernel_sz 5,  dense_sz 128, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.812s.  Loss: tr - 0.042, val - 0.033.  Accuracy: tr - 0.986, val - 0.988.\n",
      "Fold 1 trained 10 epochs in 5.818s.  Loss: tr - 0.041, val - 0.033.  Accuracy: tr - 0.988, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 6.099s.  Loss: tr - 0.052, val - 0.047.  Accuracy: tr - 0.982, val - 0.985.\n",
      "Fold 3 trained 10 epochs in 5.837s.  Loss: tr - 0.036, val - 0.038.  Accuracy: tr - 0.989, val - 0.987.\n",
      "Fold 4 trained 10 epochs in 5.84s.  Loss: tr - 0.04, val - 0.036.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 5 trained 10 epochs in 5.824s.  Loss: tr - 0.031, val - 0.033.  Accuracy: tr - 0.991, val - 0.992.\n",
      "Fold 6 trained 10 epochs in 5.825s.  Loss: tr - 0.042, val - 0.043.  Accuracy: tr - 0.987, val - 0.986.\n",
      "Fold 7 trained 10 epochs in 6.067s.  Loss: tr - 0.042, val - 0.046.  Accuracy: tr - 0.987, val - 0.984.\n",
      "Fold 8 trained 10 epochs in 5.792s.  Loss: tr - 0.092, val - 0.074.  Accuracy: tr - 0.965, val - 0.973.\n",
      "Fold 9 trained 10 epochs in 5.847s.  Loss: tr - 0.039, val - 0.037.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Cross-validation completed in 58.765s. Mean validation loss 0.042 and acc 0.986\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:42:03,769 - [BO (140569664263600).INFO] -- evaluate takes 58.7682s\n",
      "05/15/2024 07:42:03,770 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:42:03,770 - [BO (140569664263600).INFO] -- #1 - fitness: 0.04181860350072384, solution: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "05/15/2024 07:42:03,787 - [BO (140569664263600).INFO] -- model r2: 0.4202499222277416, MAPE: 1.0305380012766467\n",
      "05/15/2024 07:42:03,788 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:42:03,788 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:42:03,788 - [BO (140569664263600).INFO] -- tell takes 0.0185s\n",
      "05/15/2024 07:42:03,788 - [BO (140569664263600).INFO] -- iteration 9 starts...\n",
      "05/15/2024 07:42:04,297 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.5082s\n",
      "05/15/2024 07:42:04,297 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:42:04,298 - [BO (140569664263600).INFO] -- #1 - [0.0018949167804396625, 0.9223504274039607, 0.9913741537465071, 0.00454075789717495, 51, 4, 104, 0, 0]\n",
      "05/15/2024 07:42:04,298 - [BO (140569664263600).INFO] -- ask takes 0.5097s\n",
      "lr 0.0019, beta_1 0.9224, beta_2 0.9914, l2 0.0045, filters 51, kernel_sz 4,  dense_sz 104, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 6.926s.  Loss: tr - 0.207, val - 0.159.  Accuracy: tr - 0.917, val - 0.939.\n",
      "Fold 1 trained 10 epochs in 6.939s.  Loss: tr - 0.127, val - 0.11.  Accuracy: tr - 0.954, val - 0.964.\n",
      "Fold 2 trained 10 epochs in 6.951s.  Loss: tr - 0.335, val - 0.275.  Accuracy: tr - 0.854, val - 0.877.\n",
      "Fold 3 trained 10 epochs in 7.18s.  Loss: tr - 0.146, val - 0.118.  Accuracy: tr - 0.95, val - 0.957.\n",
      "Fold 4 trained 10 epochs in 6.931s.  Loss: tr - 0.354, val - 0.284.  Accuracy: tr - 0.839, val - 0.892.\n",
      "Fold 5 trained 10 epochs in 6.927s.  Loss: tr - 0.192, val - 0.148.  Accuracy: tr - 0.923, val - 0.947.\n",
      "Fold 6 trained 10 epochs in 6.947s.  Loss: tr - 0.091, val - 0.09.  Accuracy: tr - 0.97, val - 0.977.\n",
      "Fold 7 trained 10 epochs in 6.908s.  Loss: tr - 0.171, val - 0.141.  Accuracy: tr - 0.937, val - 0.945.\n",
      "Fold 8 trained 10 epochs in 6.912s.  Loss: tr - 0.176, val - 0.143.  Accuracy: tr - 0.933, val - 0.948.\n",
      "Fold 9 trained 10 epochs in 7.181s.  Loss: tr - 0.199, val - 0.16.  Accuracy: tr - 0.921, val - 0.941.\n",
      "Cross-validation completed in 69.806s. Mean validation loss 0.163 and acc 0.939\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:43:14,108 - [BO (140569664263600).INFO] -- evaluate takes 69.8095s\n",
      "05/15/2024 07:43:14,108 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:43:14,109 - [BO (140569664263600).INFO] -- #1 - fitness: 0.16274213716387748, solution: [0.0018949167804396625, 0.9223504274039607, 0.9913741537465071, 0.00454075789717495, 51, 4, 104, 0, 0]\n",
      "05/15/2024 07:43:14,126 - [BO (140569664263600).INFO] -- model r2: 0.3861881275996265, MAPE: 0.8470135468216785\n",
      "05/15/2024 07:43:14,126 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:43:14,127 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:43:14,127 - [BO (140569664263600).INFO] -- tell takes 0.0188s\n",
      "05/15/2024 07:43:14,127 - [BO (140569664263600).INFO] -- iteration 10 starts...\n",
      "05/15/2024 07:43:14,369 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.2412s\n",
      "05/15/2024 07:43:14,370 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:43:14,371 - [BO (140569664263600).INFO] -- #1 - [0.0028396501274692183, 0.8665216255659038, 0.998604932612547, 0.012665541636798764, 51, 4, 149, 0, 1]\n",
      "05/15/2024 07:43:14,371 - [BO (140569664263600).INFO] -- ask takes 0.2440s\n",
      "lr 0.0028, beta_1 0.8665, beta_2 0.9986, l2 0.0127, filters 51, kernel_sz 4,  dense_sz 149, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.096s.  Loss: tr - 0.637, val - 0.58.  Accuracy: tr - 0.678, val - 0.77.\n",
      "Fold 1 trained 10 epochs in 8.134s.  Loss: tr - 0.652, val - 0.647.  Accuracy: tr - 0.613, val - 0.653.\n",
      "Fold 2 trained 10 epochs in 8.116s.  Loss: tr - 0.043, val - 0.04.  Accuracy: tr - 0.987, val - 0.989.\n",
      "Fold 3 trained 10 epochs in 8.086s.  Loss: tr - 0.648, val - 0.636.  Accuracy: tr - 0.606, val - 0.632.\n",
      "Fold 4 trained 10 epochs in 8.336s.  Loss: tr - 0.576, val - 0.478.  Accuracy: tr - 0.726, val - 0.792.\n",
      "Fold 5 trained 10 epochs in 8.096s.  Loss: tr - 0.069, val - 0.064.  Accuracy: tr - 0.977, val - 0.978.\n",
      "Fold 6 trained 10 epochs in 8.127s.  Loss: tr - 0.12, val - 0.061.  Accuracy: tr - 0.954, val - 0.98.\n",
      "Fold 7 trained 10 epochs in 8.08s.  Loss: tr - 0.543, val - 0.463.  Accuracy: tr - 0.746, val - 0.773.\n",
      "Fold 8 trained 10 epochs in 8.112s.  Loss: tr - 0.048, val - 0.039.  Accuracy: tr - 0.984, val - 0.988.\n",
      "Fold 9 trained 10 epochs in 8.114s.  Loss: tr - 0.643, val - 0.629.  Accuracy: tr - 0.648, val - 0.687.\n",
      "Cross-validation completed in 81.302s. Mean validation loss 0.364 and acc 0.824\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:44:35,677 - [BO (140569664263600).INFO] -- evaluate takes 81.3049s\n",
      "05/15/2024 07:44:35,677 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:44:35,677 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3636732306331396, solution: [0.0028396501274692183, 0.8665216255659038, 0.998604932612547, 0.012665541636798764, 51, 4, 149, 0, 1]\n",
      "05/15/2024 07:44:35,694 - [BO (140569664263600).INFO] -- model r2: 0.3778306304518053, MAPE: 1.04358453795455\n",
      "05/15/2024 07:44:35,695 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:44:35,695 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:44:35,696 - [BO (140569664263600).INFO] -- tell takes 0.0185s\n",
      "05/15/2024 07:44:35,696 - [BO (140569664263600).INFO] -- iteration 11 starts...\n",
      "05/15/2024 07:44:36,447 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.7507s\n",
      "05/15/2024 07:44:36,448 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:44:36,448 - [BO (140569664263600).INFO] -- #1 - [0.005120206379630261, 0.9321616859124107, 0.9917102294377252, 0.01553501503075528, 43, 5, 58, 0, 0]\n",
      "05/15/2024 07:44:36,448 - [BO (140569664263600).INFO] -- ask takes 0.7521s\n",
      "lr 0.0051, beta_1 0.9322, beta_2 0.9917, l2 0.0155, filters 43, kernel_sz 5,  dense_sz 58, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 3.7s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.597.\n",
      "Fold 1 trained 10 epochs in 4.001s.  Loss: tr - 0.676, val - 0.663.  Accuracy: tr - 0.577, val - 0.578.\n",
      "Fold 2 trained 10 epochs in 3.722s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 3.712s.  Loss: tr - 0.658, val - 0.652.  Accuracy: tr - 0.594, val - 0.612.\n",
      "Fold 4 trained 10 epochs in 3.733s.  Loss: tr - 0.748, val - 0.668.  Accuracy: tr - 0.546, val - 0.573.\n",
      "Fold 5 trained 10 epochs in 3.735s.  Loss: tr - 0.675, val - 0.668.  Accuracy: tr - 0.584, val - 0.597.\n",
      "Fold 6 trained 6 epochs in 2.24s.  Loss: tr - 38.047, val - 49.694.  Accuracy: tr - 0.516, val - 0.503.\n",
      "Fold 7 trained 10 epochs in 4.036s.  Loss: tr - 0.659, val - 0.664.  Accuracy: tr - 0.589, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 3.709s.  Loss: tr - 0.662, val - 0.661.  Accuracy: tr - 0.584, val - 0.595.\n",
      "Fold 9 trained 10 epochs in 3.715s.  Loss: tr - 0.658, val - 0.662.  Accuracy: tr - 0.592, val - 0.573.\n",
      "Cross-validation completed in 36.308s. Mean validation loss 5.564 and acc 0.581\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:45:12,760 - [BO (140569664263600).INFO] -- evaluate takes 36.3112s\n",
      "05/15/2024 07:45:12,760 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:45:12,760 - [BO (140569664263600).INFO] -- #1 - fitness: 5.5641805768013, solution: [0.005120206379630261, 0.9321616859124107, 0.9917102294377252, 0.01553501503075528, 43, 5, 58, 0, 0]\n",
      "05/15/2024 07:45:12,777 - [BO (140569664263600).INFO] -- model r2: 0.4205458791651664, MAPE: 0.5755661452402101\n",
      "05/15/2024 07:45:12,778 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:45:12,778 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:45:12,778 - [BO (140569664263600).INFO] -- tell takes 0.0182s\n",
      "05/15/2024 07:45:12,778 - [BO (140569664263600).INFO] -- iteration 12 starts...\n",
      "05/15/2024 07:45:13,801 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.0220s\n",
      "05/15/2024 07:45:13,802 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:45:13,802 - [BO (140569664263600).INFO] -- #1 - [0.004050465469973394, 0.9417473824766802, 0.9924616830436404, 0.010912916105509264, 49, 3, 100, 0, 1]\n",
      "05/15/2024 07:45:13,802 - [BO (140569664263600).INFO] -- ask takes 1.0234s\n",
      "lr 0.0041, beta_1 0.9417, beta_2 0.9925, l2 0.0109, filters 49, kernel_sz 3,  dense_sz 100, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.99s.  Loss: tr - 0.141, val - 0.089.  Accuracy: tr - 0.951, val - 0.969.\n",
      "Fold 1 trained 10 epochs in 5.961s.  Loss: tr - 0.432, val - 0.297.  Accuracy: tr - 0.799, val - 0.89.\n",
      "Fold 2 trained 10 epochs in 5.941s.  Loss: tr - 0.653, val - 0.648.  Accuracy: tr - 0.598, val - 0.63.\n",
      "Fold 3 trained 10 epochs in 6.002s.  Loss: tr - 0.47, val - 0.416.  Accuracy: tr - 0.782, val - 0.818.\n",
      "Fold 4 trained 10 epochs in 6.276s.  Loss: tr - 0.375, val - 0.289.  Accuracy: tr - 0.84, val - 0.882.\n",
      "Fold 5 trained 6 epochs in 3.648s.  Loss: tr - 32.531, val - 49.046.  Accuracy: tr - 0.558, val - 0.496.\n",
      "Fold 6 trained 10 epochs in 5.969s.  Loss: tr - 0.043, val - 0.043.  Accuracy: tr - 0.986, val - 0.989.\n",
      "Fold 7 trained 10 epochs in 5.963s.  Loss: tr - 0.088, val - 0.074.  Accuracy: tr - 0.966, val - 0.971.\n",
      "Fold 8 trained 10 epochs in 5.971s.  Loss: tr - 0.656, val - 0.657.  Accuracy: tr - 0.591, val - 0.6.\n",
      "Fold 9 trained 6 epochs in 3.595s.  Loss: tr - 32.614, val - 43.461.  Accuracy: tr - 0.548, val - 0.565.\n",
      "Cross-validation completed in 55.322s. Mean validation loss 9.502 and acc 0.781\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:46:09,127 - [BO (140569664263600).INFO] -- evaluate takes 55.3251s\n",
      "05/15/2024 07:46:09,128 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:46:09,128 - [BO (140569664263600).INFO] -- #1 - fitness: 9.502235578373075, solution: [0.004050465469973394, 0.9417473824766802, 0.9924616830436404, 0.010912916105509264, 49, 3, 100, 0, 1]\n",
      "05/15/2024 07:46:09,146 - [BO (140569664263600).INFO] -- model r2: 0.5912977758304914, MAPE: 0.653434759999443\n",
      "05/15/2024 07:46:09,147 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:46:09,147 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:46:09,147 - [BO (140569664263600).INFO] -- tell takes 0.0195s\n",
      "05/15/2024 07:46:09,147 - [BO (140569664263600).INFO] -- iteration 13 starts...\n",
      "05/15/2024 07:46:10,499 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3515s\n",
      "05/15/2024 07:46:10,500 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:46:10,501 - [BO (140569664263600).INFO] -- #1 - [0.0037436103661648993, 0.9207081704314908, 0.9901714239300656, 0.011422978437589016, 30, 4, 137, 0, 1]\n",
      "05/15/2024 07:46:10,501 - [BO (140569664263600).INFO] -- ask takes 1.3530s\n",
      "lr 0.0037, beta_1 0.9207, beta_2 0.9902, l2 0.0114, filters 30, kernel_sz 4,  dense_sz 137, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.936s.  Loss: tr - 0.617, val - 0.535.  Accuracy: tr - 0.645, val - 0.736.\n",
      "Fold 1 trained 10 epochs in 5.577s.  Loss: tr - 0.034, val - 0.043.  Accuracy: tr - 0.991, val - 0.994.\n",
      "Fold 2 trained 10 epochs in 5.591s.  Loss: tr - 0.278, val - 0.185.  Accuracy: tr - 0.885, val - 0.927.\n",
      "Fold 3 trained 10 epochs in 5.604s.  Loss: tr - 0.593, val - 0.513.  Accuracy: tr - 0.7, val - 0.903.\n",
      "Fold 4 trained 10 epochs in 5.571s.  Loss: tr - 0.055, val - 0.052.  Accuracy: tr - 0.982, val - 0.983.\n",
      "Fold 5 trained 10 epochs in 5.958s.  Loss: tr - 0.638, val - 0.604.  Accuracy: tr - 0.642, val - 0.682.\n",
      "Fold 6 trained 10 epochs in 5.561s.  Loss: tr - 0.275, val - 0.21.  Accuracy: tr - 0.882, val - 0.925.\n",
      "Fold 7 trained 10 epochs in 5.622s.  Loss: tr - 0.509, val - 0.347.  Accuracy: tr - 0.764, val - 0.859.\n",
      "Fold 8 trained 10 epochs in 5.582s.  Loss: tr - 0.641, val - 0.626.  Accuracy: tr - 0.642, val - 0.654.\n",
      "Fold 9 trained 10 epochs in 5.581s.  Loss: tr - 0.505, val - 0.571.  Accuracy: tr - 0.769, val - 0.755.\n",
      "Cross-validation completed in 56.587s. Mean validation loss 0.369 and acc 0.842\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:47:07,91 - [BO (140569664263600).INFO] -- evaluate takes 56.5905s\n",
      "05/15/2024 07:47:07,92 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:47:07,92 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3685107730329037, solution: [0.0037436103661648993, 0.9207081704314908, 0.9901714239300656, 0.011422978437589016, 30, 4, 137, 0, 1]\n",
      "05/15/2024 07:47:07,109 - [BO (140569664263600).INFO] -- model r2: 0.5908141784444316, MAPE: 0.6679149526608149\n",
      "05/15/2024 07:47:07,110 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:47:07,110 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:47:07,110 - [BO (140569664263600).INFO] -- tell takes 0.0186s\n",
      "05/15/2024 07:47:07,111 - [BO (140569664263600).INFO] -- iteration 14 starts...\n",
      "05/15/2024 07:47:08,325 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.2143s\n",
      "05/15/2024 07:47:08,326 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:47:08,327 - [BO (140569664263600).INFO] -- #1 - [0.003813847993613822, 0.9299077910685649, 0.9909585205296056, 0.0122091038390156, 35, 3, 101, 0, 1]\n",
      "05/15/2024 07:47:08,327 - [BO (140569664263600).INFO] -- ask takes 1.2159s\n",
      "lr 0.0038, beta_1 0.9299, beta_2 0.991, l2 0.0122, filters 35, kernel_sz 3,  dense_sz 101, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.319s.  Loss: tr - 0.167, val - 0.1.  Accuracy: tr - 0.932, val - 0.963.\n",
      "Fold 1 trained 10 epochs in 6.007s.  Loss: tr - 0.059, val - 0.049.  Accuracy: tr - 0.98, val - 0.985.\n",
      "Fold 2 trained 10 epochs in 6.015s.  Loss: tr - 0.149, val - 0.102.  Accuracy: tr - 0.944, val - 0.96.\n",
      "Fold 3 trained 10 epochs in 6.089s.  Loss: tr - 0.656, val - 0.65.  Accuracy: tr - 0.593, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 6.051s.  Loss: tr - 0.096, val - 0.063.  Accuracy: tr - 0.964, val - 0.976.\n",
      "Fold 5 trained 10 epochs in 6.324s.  Loss: tr - 0.113, val - 0.09.  Accuracy: tr - 0.958, val - 0.964.\n",
      "Fold 6 trained 10 epochs in 5.994s.  Loss: tr - 0.448, val - 0.384.  Accuracy: tr - 0.802, val - 0.832.\n",
      "Fold 7 trained 10 epochs in 6.003s.  Loss: tr - 0.133, val - 0.127.  Accuracy: tr - 0.948, val - 0.956.\n",
      "Fold 8 trained 10 epochs in 5.995s.  Loss: tr - 0.58, val - 0.457.  Accuracy: tr - 0.716, val - 0.842.\n",
      "Fold 9 trained 10 epochs in 6.002s.  Loss: tr - 0.504, val - 0.429.  Accuracy: tr - 0.76, val - 0.798.\n",
      "Cross-validation completed in 60.803s. Mean validation loss 0.245 and acc 0.887\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:48:09,133 - [BO (140569664263600).INFO] -- evaluate takes 60.8058s\n",
      "05/15/2024 07:48:09,134 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:48:09,134 - [BO (140569664263600).INFO] -- #1 - fitness: 0.24513246566057206, solution: [0.003813847993613822, 0.9299077910685649, 0.9909585205296056, 0.0122091038390156, 35, 3, 101, 0, 1]\n",
      "05/15/2024 07:48:09,153 - [BO (140569664263600).INFO] -- model r2: 0.5085386587712615, MAPE: 0.46971208961924904\n",
      "05/15/2024 07:48:09,153 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:48:09,154 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:48:09,154 - [BO (140569664263600).INFO] -- tell takes 0.0205s\n",
      "05/15/2024 07:48:09,154 - [BO (140569664263600).INFO] -- iteration 15 starts...\n",
      "05/15/2024 07:48:10,809 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.6547s\n",
      "05/15/2024 07:48:10,810 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:48:10,810 - [BO (140569664263600).INFO] -- #1 - [0.0010410993909655774, 0.9409932730354893, 0.9928396543166688, 0.005898354753034813, 53, 4, 105, 0, 1]\n",
      "05/15/2024 07:48:10,811 - [BO (140569664263600).INFO] -- ask takes 1.6561s\n",
      "lr 0.001, beta_1 0.941, beta_2 0.9928, l2 0.0059, filters 53, kernel_sz 4,  dense_sz 105, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.321s.  Loss: tr - 0.031, val - 0.029.  Accuracy: tr - 0.992, val - 0.994.\n",
      "Fold 1 trained 10 epochs in 8.617s.  Loss: tr - 0.038, val - 0.026.  Accuracy: tr - 0.987, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 8.307s.  Loss: tr - 0.052, val - 0.051.  Accuracy: tr - 0.983, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 8.29s.  Loss: tr - 0.051, val - 0.047.  Accuracy: tr - 0.983, val - 0.981.\n",
      "Fold 4 trained 10 epochs in 8.3s.  Loss: tr - 0.047, val - 0.036.  Accuracy: tr - 0.985, val - 0.989.\n",
      "Fold 5 trained 10 epochs in 8.298s.  Loss: tr - 0.06, val - 0.055.  Accuracy: tr - 0.978, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 8.568s.  Loss: tr - 0.045, val - 0.045.  Accuracy: tr - 0.986, val - 0.986.\n",
      "Fold 7 trained 10 epochs in 8.282s.  Loss: tr - 0.099, val - 0.076.  Accuracy: tr - 0.963, val - 0.973.\n",
      "Fold 8 trained 10 epochs in 8.276s.  Loss: tr - 0.036, val - 0.031.  Accuracy: tr - 0.988, val - 0.99.\n",
      "Fold 9 trained 10 epochs in 8.282s.  Loss: tr - 0.036, val - 0.027.  Accuracy: tr - 0.989, val - 0.992.\n",
      "Cross-validation completed in 83.545s. Mean validation loss 0.042 and acc 0.986\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:49:34,359 - [BO (140569664263600).INFO] -- evaluate takes 83.5485s\n",
      "05/15/2024 07:49:34,360 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:49:34,360 - [BO (140569664263600).INFO] -- #1 - fitness: 0.042281216382980345, solution: [0.0010410993909655774, 0.9409932730354893, 0.9928396543166688, 0.005898354753034813, 53, 4, 105, 0, 1]\n",
      "05/15/2024 07:49:34,379 - [BO (140569664263600).INFO] -- model r2: 0.6126732653534088, MAPE: 0.5995189543497345\n",
      "05/15/2024 07:49:34,379 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:49:34,380 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:49:34,380 - [BO (140569664263600).INFO] -- tell takes 0.0202s\n",
      "05/15/2024 07:49:34,380 - [BO (140569664263600).INFO] -- iteration 16 starts...\n",
      "05/15/2024 07:49:35,100 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.7194s\n",
      "05/15/2024 07:49:35,101 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:49:35,101 - [BO (140569664263600).INFO] -- #1 - [0.003905914145311972, 0.8801634216529208, 0.9927354640400257, 0.013698097476172677, 50, 3, 52, 0, 1]\n",
      "05/15/2024 07:49:35,101 - [BO (140569664263600).INFO] -- ask takes 0.7208s\n",
      "lr 0.0039, beta_1 0.8802, beta_2 0.9927, l2 0.0137, filters 50, kernel_sz 3,  dense_sz 52, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.093s.  Loss: tr - 0.615, val - 0.587.  Accuracy: tr - 0.665, val - 0.719.\n",
      "Fold 1 trained 10 epochs in 6.08s.  Loss: tr - 0.641, val - 0.628.  Accuracy: tr - 0.642, val - 0.613.\n",
      "Fold 2 trained 10 epochs in 6.345s.  Loss: tr - 0.416, val - 0.361.  Accuracy: tr - 0.804, val - 0.833.\n",
      "Fold 3 trained 10 epochs in 6.035s.  Loss: tr - 0.647, val - 0.638.  Accuracy: tr - 0.612, val - 0.598.\n",
      "Fold 4 trained 10 epochs in 6.058s.  Loss: tr - 0.642, val - 0.623.  Accuracy: tr - 0.637, val - 0.679.\n",
      "Fold 5 trained 10 epochs in 6.076s.  Loss: tr - 0.248, val - 0.184.  Accuracy: tr - 0.897, val - 0.926.\n",
      "Fold 6 trained 10 epochs in 6.137s.  Loss: tr - 0.251, val - 0.168.  Accuracy: tr - 0.901, val - 0.93.\n",
      "Fold 7 trained 10 epochs in 6.307s.  Loss: tr - 0.53, val - 0.382.  Accuracy: tr - 0.744, val - 0.856.\n",
      "Fold 8 trained 10 epochs in 6.056s.  Loss: tr - 38.488, val - 0.797.  Accuracy: tr - 0.585, val - 0.456.\n",
      "Fold 9 trained 10 epochs in 6.092s.  Loss: tr - 0.638, val - 0.626.  Accuracy: tr - 0.631, val - 0.67.\n",
      "Cross-validation completed in 61.285s. Mean validation loss 0.499 and acc 0.728\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:50:36,389 - [BO (140569664263600).INFO] -- evaluate takes 61.2877s\n",
      "05/15/2024 07:50:36,390 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:50:36,390 - [BO (140569664263600).INFO] -- #1 - fitness: 0.4994234338402748, solution: [0.003905914145311972, 0.8801634216529208, 0.9927354640400257, 0.013698097476172677, 50, 3, 52, 0, 1]\n",
      "05/15/2024 07:50:36,407 - [BO (140569664263600).INFO] -- model r2: 0.5981054525102762, MAPE: 0.648997899413359\n",
      "05/15/2024 07:50:36,408 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:50:36,408 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:50:36,408 - [BO (140569664263600).INFO] -- tell takes 0.0187s\n",
      "05/15/2024 07:50:36,409 - [BO (140569664263600).INFO] -- iteration 17 starts...\n",
      "05/15/2024 07:50:37,592 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.1831s\n",
      "05/15/2024 07:50:37,593 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:50:37,593 - [BO (140569664263600).INFO] -- #1 - [0.003825146870144249, 0.8690964997915707, 0.9980727013898886, 0.013369562111210624, 54, 5, 106, 0, 0]\n",
      "05/15/2024 07:50:37,593 - [BO (140569664263600).INFO] -- ask takes 1.1845s\n",
      "lr 0.0038, beta_1 0.8691, beta_2 0.9981, l2 0.0134, filters 54, kernel_sz 5,  dense_sz 106, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.349s.  Loss: tr - 0.674, val - 0.662.  Accuracy: tr - 0.584, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 4.307s.  Loss: tr - 0.662, val - 0.654.  Accuracy: tr - 0.593, val - 0.599.\n",
      "Fold 2 trained 10 epochs in 4.592s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.587, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 4.289s.  Loss: tr - 0.657, val - 0.652.  Accuracy: tr - 0.594, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 4.285s.  Loss: tr - 0.664, val - 0.656.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 5 trained 6 epochs in 2.657s.  Loss: tr - 34.734, val - 41.845.  Accuracy: tr - 0.575, val - 0.582.\n",
      "Fold 6 trained 10 epochs in 4.33s.  Loss: tr - 51.304, val - 51.523.  Accuracy: tr - 0.479, val - 0.473.\n",
      "Fold 7 trained 10 epochs in 4.324s.  Loss: tr - 0.775, val - 0.681.  Accuracy: tr - 0.558, val - 0.509.\n",
      "Fold 8 trained 6 epochs in 2.616s.  Loss: tr - 33.367, val - 43.909.  Accuracy: tr - 0.543, val - 0.561.\n",
      "Fold 9 trained 10 epochs in 4.323s.  Loss: tr - 0.66, val - 0.662.  Accuracy: tr - 0.593, val - 0.589.\n",
      "Cross-validation completed in 40.076s. Mean validation loss 14.19 and acc 0.569\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:51:17,673 - [BO (140569664263600).INFO] -- evaluate takes 40.0793s\n",
      "05/15/2024 07:51:17,673 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:51:17,673 - [BO (140569664263600).INFO] -- #1 - fitness: 14.189904457330703, solution: [0.003825146870144249, 0.8690964997915707, 0.9980727013898886, 0.013369562111210624, 54, 5, 106, 0, 0]\n",
      "05/15/2024 07:51:17,691 - [BO (140569664263600).INFO] -- model r2: 0.6334305289891511, MAPE: 0.6512459269620068\n",
      "05/15/2024 07:51:17,691 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:51:17,692 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:51:17,692 - [BO (140569664263600).INFO] -- tell takes 0.0189s\n",
      "05/15/2024 07:51:17,692 - [BO (140569664263600).INFO] -- iteration 18 starts...\n",
      "05/15/2024 07:51:18,336 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.6437s\n",
      "05/15/2024 07:51:18,337 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:51:18,337 - [BO (140569664263600).INFO] -- #1 - [0.003846349569529683, 0.9113938993541246, 0.9910070071425261, 0.003853869883962663, 49, 2, 38, 1, 0]\n",
      "05/15/2024 07:51:18,338 - [BO (140569664263600).INFO] -- ask takes 0.6451s\n",
      "lr 0.0038, beta_1 0.9114, beta_2 0.991, l2 0.0039, filters 49, kernel_sz 2,  dense_sz 38, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.797s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 4.502s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 4.507s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.593, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 4.498s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.59, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 4.525s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 4.531s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.594, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 4.86s.  Loss: tr - 0.656, val - 0.661.  Accuracy: tr - 0.592, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 4.516s.  Loss: tr - 0.658, val - 0.665.  Accuracy: tr - 0.594, val - 0.574.\n",
      "Fold 8 trained 10 epochs in 4.483s.  Loss: tr - 0.663, val - 0.657.  Accuracy: tr - 0.579, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 4.484s.  Loss: tr - 0.657, val - 0.659.  Accuracy: tr - 0.59, val - 0.592.\n",
      "Cross-validation completed in 45.706s. Mean validation loss 0.656 and acc 0.593\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:52:04,47 - [BO (140569664263600).INFO] -- evaluate takes 45.7092s\n",
      "05/15/2024 07:52:04,47 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:52:04,48 - [BO (140569664263600).INFO] -- #1 - fitness: 0.656118881702423, solution: [0.003846349569529683, 0.9113938993541246, 0.9910070071425261, 0.003853869883962663, 49, 2, 38, 1, 0]\n",
      "05/15/2024 07:52:04,65 - [BO (140569664263600).INFO] -- model r2: 0.63257026243785, MAPE: 0.5141671390320708\n",
      "05/15/2024 07:52:04,65 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:52:04,66 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:52:04,66 - [BO (140569664263600).INFO] -- tell takes 0.0185s\n",
      "05/15/2024 07:52:04,66 - [BO (140569664263600).INFO] -- iteration 19 starts...\n",
      "05/15/2024 07:52:04,965 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.8984s\n",
      "05/15/2024 07:52:04,966 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:52:04,966 - [BO (140569664263600).INFO] -- #1 - [0.001318860599492404, 0.8519341985352165, 0.9956386538873648, 0.014496166648027435, 25, 2, 110, 0, 1]\n",
      "05/15/2024 07:52:04,966 - [BO (140569664263600).INFO] -- ask takes 0.8997s\n",
      "lr 0.0013, beta_1 0.8519, beta_2 0.9956, l2 0.0145, filters 25, kernel_sz 2,  dense_sz 110, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 2.51s.  Loss: tr - 0.055, val - 0.046.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 1 trained 10 epochs in 2.539s.  Loss: tr - 0.06, val - 0.053.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 2 trained 10 epochs in 2.814s.  Loss: tr - 0.062, val - 0.056.  Accuracy: tr - 0.982, val - 0.985.\n",
      "Fold 3 trained 10 epochs in 2.597s.  Loss: tr - 0.053, val - 0.051.  Accuracy: tr - 0.986, val - 0.984.\n",
      "Fold 4 trained 10 epochs in 2.566s.  Loss: tr - 0.1, val - 0.074.  Accuracy: tr - 0.972, val - 0.981.\n",
      "Fold 5 trained 10 epochs in 2.556s.  Loss: tr - 0.073, val - 0.072.  Accuracy: tr - 0.979, val - 0.978.\n",
      "Fold 6 trained 10 epochs in 2.538s.  Loss: tr - 0.21, val - 0.168.  Accuracy: tr - 0.933, val - 0.944.\n",
      "Fold 7 trained 10 epochs in 2.554s.  Loss: tr - 0.05, val - 0.043.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 8 trained 10 epochs in 2.779s.  Loss: tr - 0.054, val - 0.042.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 9 trained 10 epochs in 2.54s.  Loss: tr - 0.054, val - 0.041.  Accuracy: tr - 0.982, val - 0.989.\n",
      "Cross-validation completed in 25.997s. Mean validation loss 0.064 and acc 0.982\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:52:30,966 - [BO (140569664263600).INFO] -- evaluate takes 26.0002s\n",
      "05/15/2024 07:52:30,967 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:52:30,967 - [BO (140569664263600).INFO] -- #1 - fitness: 0.06443522796034813, solution: [0.001318860599492404, 0.8519341985352165, 0.9956386538873648, 0.014496166648027435, 25, 2, 110, 0, 1]\n",
      "05/15/2024 07:52:30,985 - [BO (140569664263600).INFO] -- model r2: 0.6631271942608907, MAPE: 0.5835460464435938\n",
      "05/15/2024 07:52:30,986 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:52:30,986 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:52:30,987 - [BO (140569664263600).INFO] -- tell takes 0.0198s\n",
      "05/15/2024 07:52:30,987 - [BO (140569664263600).INFO] -- iteration 20 starts...\n",
      "05/15/2024 07:52:32,00 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.0128s\n",
      "05/15/2024 07:52:32,01 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:52:32,01 - [BO (140569664263600).INFO] -- #1 - [0.007539155830740497, 0.8771884076077852, 0.9900635978418246, 0.006373490375381275, 63, 2, 120, 1, 1]\n",
      "05/15/2024 07:52:32,01 - [BO (140569664263600).INFO] -- ask takes 1.0142s\n",
      "lr 0.0075, beta_1 0.8772, beta_2 0.9901, l2 0.0064, filters 63, kernel_sz 2,  dense_sz 120, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 8.205s.  Loss: tr - 0.663, val - 0.657.  Accuracy: tr - 0.589, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 8.124s.  Loss: tr - 0.662, val - 0.657.  Accuracy: tr - 0.592, val - 0.587.\n",
      "Fold 2 trained 10 epochs in 8.179s.  Loss: tr - 0.669, val - 0.656.  Accuracy: tr - 0.571, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 8.118s.  Loss: tr - 0.664, val - 0.652.  Accuracy: tr - 0.581, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 8.5s.  Loss: tr - 0.662, val - 0.656.  Accuracy: tr - 0.587, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 8.23s.  Loss: tr - 0.674, val - 0.652.  Accuracy: tr - 0.579, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 8.21s.  Loss: tr - 0.667, val - 0.664.  Accuracy: tr - 0.575, val - 0.569.\n",
      "Fold 7 trained 10 epochs in 8.245s.  Loss: tr - 0.663, val - 0.665.  Accuracy: tr - 0.584, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 8.224s.  Loss: tr - 0.666, val - 0.66.  Accuracy: tr - 0.58, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 8.468s.  Loss: tr - 0.662, val - 0.661.  Accuracy: tr - 0.587, val - 0.592.\n",
      "Cross-validation completed in 82.506s. Mean validation loss 0.658 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:53:54,511 - [BO (140569664263600).INFO] -- evaluate takes 82.5092s\n",
      "05/15/2024 07:53:54,511 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:53:54,512 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6578897655010223, solution: [0.007539155830740497, 0.8771884076077852, 0.9900635978418246, 0.006373490375381275, 63, 2, 120, 1, 1]\n",
      "05/15/2024 07:53:54,529 - [BO (140569664263600).INFO] -- model r2: 0.6293312993900002, MAPE: 0.6446285799364907\n",
      "05/15/2024 07:53:54,530 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:53:54,530 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:53:54,530 - [BO (140569664263600).INFO] -- tell takes 0.0191s\n",
      "05/15/2024 07:53:54,531 - [BO (140569664263600).INFO] -- iteration 21 starts...\n",
      "05/15/2024 07:53:55,761 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.2297s\n",
      "05/15/2024 07:53:55,762 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:53:55,762 - [BO (140569664263600).INFO] -- #1 - [0.0037443656399989266, 0.8894600702494252, 0.9963507734211146, 0.005339879349968641, 34, 2, 46, 0, 0]\n",
      "05/15/2024 07:53:55,762 - [BO (140569664263600).INFO] -- ask takes 1.2312s\n",
      "lr 0.0037, beta_1 0.8895, beta_2 0.9964, l2 0.0053, filters 34, kernel_sz 2,  dense_sz 46, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 3.241s.  Loss: tr - 0.044, val - 0.036.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 1 trained 10 epochs in 3.253s.  Loss: tr - 0.163, val - 0.129.  Accuracy: tr - 0.935, val - 0.948.\n",
      "Fold 2 trained 10 epochs in 3.253s.  Loss: tr - 0.385, val - 0.348.  Accuracy: tr - 0.829, val - 0.844.\n",
      "Fold 3 trained 10 epochs in 3.269s.  Loss: tr - 0.134, val - 0.095.  Accuracy: tr - 0.951, val - 0.971.\n",
      "Fold 4 trained 10 epochs in 3.594s.  Loss: tr - 0.03, val - 0.028.  Accuracy: tr - 0.991, val - 0.991.\n",
      "Fold 5 trained 10 epochs in 3.299s.  Loss: tr - 0.075, val - 0.061.  Accuracy: tr - 0.976, val - 0.983.\n",
      "Fold 6 trained 10 epochs in 3.305s.  Loss: tr - 0.044, val - 0.047.  Accuracy: tr - 0.985, val - 0.986.\n",
      "Fold 7 trained 10 epochs in 3.315s.  Loss: tr - 0.043, val - 0.046.  Accuracy: tr - 0.986, val - 0.986.\n",
      "Fold 8 trained 10 epochs in 3.263s.  Loss: tr - 0.061, val - 0.045.  Accuracy: tr - 0.981, val - 0.986.\n",
      "Fold 9 trained 10 epochs in 3.516s.  Loss: tr - 0.07, val - 0.054.  Accuracy: tr - 0.975, val - 0.979.\n",
      "Cross-validation completed in 33.312s. Mean validation loss 0.089 and acc 0.966\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:54:29,78 - [BO (140569664263600).INFO] -- evaluate takes 33.3151s\n",
      "05/15/2024 07:54:29,78 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:54:29,78 - [BO (140569664263600).INFO] -- #1 - fitness: 0.08897497449070216, solution: [0.0037443656399989266, 0.8894600702494252, 0.9963507734211146, 0.005339879349968641, 34, 2, 46, 0, 0]\n",
      "05/15/2024 07:54:29,96 - [BO (140569664263600).INFO] -- model r2: 0.6363444445137458, MAPE: 0.7163918718828922\n",
      "05/15/2024 07:54:29,97 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:54:29,97 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:54:29,97 - [BO (140569664263600).INFO] -- tell takes 0.0194s\n",
      "05/15/2024 07:54:29,98 - [BO (140569664263600).INFO] -- iteration 22 starts...\n",
      "05/15/2024 07:54:31,299 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.2012s\n",
      "05/15/2024 07:54:31,300 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:54:31,300 - [BO (140569664263600).INFO] -- #1 - [0.0038193118253889084, 0.8931935707405777, 0.992936264936367, 0.004005582527980654, 45, 3, 84, 0, 1]\n",
      "05/15/2024 07:54:31,301 - [BO (140569664263600).INFO] -- ask takes 2.2027s\n",
      "lr 0.0038, beta_1 0.8932, beta_2 0.9929, l2 0.004, filters 45, kernel_sz 3,  dense_sz 84, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.528s.  Loss: tr - 0.079, val - 0.056.  Accuracy: tr - 0.974, val - 0.984.\n",
      "Fold 1 trained 10 epochs in 5.46s.  Loss: tr - 0.136, val - 0.087.  Accuracy: tr - 0.947, val - 0.969.\n",
      "Fold 2 trained 10 epochs in 5.484s.  Loss: tr - 0.056, val - 0.049.  Accuracy: tr - 0.98, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 5.486s.  Loss: tr - 0.038, val - 0.04.  Accuracy: tr - 0.987, val - 0.987.\n",
      "Fold 4 trained 10 epochs in 5.731s.  Loss: tr - 0.087, val - 0.054.  Accuracy: tr - 0.971, val - 0.984.\n",
      "Fold 5 trained 10 epochs in 5.474s.  Loss: tr - 0.597, val - 0.482.  Accuracy: tr - 0.707, val - 0.736.\n",
      "Fold 6 trained 10 epochs in 5.421s.  Loss: tr - 0.18, val - 0.141.  Accuracy: tr - 0.931, val - 0.946.\n",
      "Fold 7 trained 10 epochs in 5.522s.  Loss: tr - 0.051, val - 0.047.  Accuracy: tr - 0.984, val - 0.984.\n",
      "Fold 8 trained 6 epochs in 3.289s.  Loss: tr - 40.053, val - 51.198.  Accuracy: tr - 0.513, val - 0.488.\n",
      "Fold 9 trained 10 epochs in 5.47s.  Loss: tr - 0.029, val - 0.027.  Accuracy: tr - 0.99, val - 0.989.\n",
      "Cross-validation completed in 52.87s. Mean validation loss 5.218 and acc 0.905\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:55:24,174 - [BO (140569664263600).INFO] -- evaluate takes 52.8730s\n",
      "05/15/2024 07:55:24,175 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:55:24,175 - [BO (140569664263600).INFO] -- #1 - fitness: 5.217981811240316, solution: [0.0038193118253889084, 0.8931935707405777, 0.992936264936367, 0.004005582527980654, 45, 3, 84, 0, 1]\n",
      "05/15/2024 07:55:24,192 - [BO (140569664263600).INFO] -- model r2: 0.6798629010108139, MAPE: 0.6493370400454889\n",
      "05/15/2024 07:55:24,193 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:55:24,193 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:55:24,193 - [BO (140569664263600).INFO] -- tell takes 0.0188s\n",
      "05/15/2024 07:55:24,194 - [BO (140569664263600).INFO] -- iteration 23 starts...\n",
      "05/15/2024 07:55:25,78 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.8840s\n",
      "05/15/2024 07:55:25,79 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:55:25,79 - [BO (140569664263600).INFO] -- #1 - [0.007148473686743623, 0.8504210994857375, 0.9912121677667663, 0.009349878533765508, 51, 4, 124, 1, 0]\n",
      "05/15/2024 07:55:25,79 - [BO (140569664263600).INFO] -- ask takes 0.8853s\n",
      "lr 0.0071, beta_1 0.8504, beta_2 0.9912, l2 0.0093, filters 51, kernel_sz 4,  dense_sz 124, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 6.927s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.583, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 7.194s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.591, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 6.928s.  Loss: tr - 0.663, val - 0.654.  Accuracy: tr - 0.589, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 6.9s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.588, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 6.933s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 6.976s.  Loss: tr - 0.661, val - 0.653.  Accuracy: tr - 0.58, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 7.212s.  Loss: tr - 0.659, val - 0.662.  Accuracy: tr - 0.585, val - 0.577.\n",
      "Fold 7 trained 10 epochs in 6.963s.  Loss: tr - 0.657, val - 0.666.  Accuracy: tr - 0.591, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 6.896s.  Loss: tr - 0.669, val - 0.657.  Accuracy: tr - 0.569, val - 0.589.\n",
      "Fold 9 trained 10 epochs in 6.909s.  Loss: tr - 0.677, val - 0.66.  Accuracy: tr - 0.575, val - 0.589.\n",
      "Cross-validation completed in 69.842s. Mean validation loss 0.657 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:56:34,924 - [BO (140569664263600).INFO] -- evaluate takes 69.8451s\n",
      "05/15/2024 07:56:34,925 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:56:34,925 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6567233562469482, solution: [0.007148473686743623, 0.8504210994857375, 0.9912121677667663, 0.009349878533765508, 51, 4, 124, 1, 0]\n",
      "05/15/2024 07:56:34,943 - [BO (140569664263600).INFO] -- model r2: 0.6611479456501198, MAPE: 0.6803455663874068\n",
      "05/15/2024 07:56:34,943 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:56:34,943 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:56:34,944 - [BO (140569664263600).INFO] -- tell takes 0.0188s\n",
      "05/15/2024 07:56:34,944 - [BO (140569664263600).INFO] -- iteration 24 starts...\n",
      "05/15/2024 07:56:36,771 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.8264s\n",
      "05/15/2024 07:56:36,771 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:56:36,772 - [BO (140569664263600).INFO] -- #1 - [0.000120239509280517, 0.8780960337990369, 0.9973896269096522, 0.014279899393492276, 53, 3, 39, 0, 1]\n",
      "05/15/2024 07:56:36,772 - [BO (140569664263600).INFO] -- ask takes 1.8278s\n",
      "lr 0.0001, beta_1 0.8781, beta_2 0.9974, l2 0.0143, filters 53, kernel_sz 3,  dense_sz 39, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.363s.  Loss: tr - 0.401, val - 0.373.  Accuracy: tr - 0.845, val - 0.85.\n",
      "Fold 1 trained 10 epochs in 6.55s.  Loss: tr - 0.265, val - 0.233.  Accuracy: tr - 0.906, val - 0.924.\n",
      "Fold 2 trained 10 epochs in 6.315s.  Loss: tr - 0.319, val - 0.275.  Accuracy: tr - 0.884, val - 0.901.\n",
      "Fold 3 trained 10 epochs in 6.295s.  Loss: tr - 0.329, val - 0.3.  Accuracy: tr - 0.893, val - 0.899.\n",
      "Fold 4 trained 10 epochs in 6.275s.  Loss: tr - 0.549, val - 0.525.  Accuracy: tr - 0.768, val - 0.774.\n",
      "Fold 5 trained 10 epochs in 6.363s.  Loss: tr - 0.448, val - 0.413.  Accuracy: tr - 0.818, val - 0.818.\n",
      "Fold 6 trained 10 epochs in 6.544s.  Loss: tr - 0.307, val - 0.281.  Accuracy: tr - 0.885, val - 0.899.\n",
      "Fold 7 trained 10 epochs in 6.303s.  Loss: tr - 0.466, val - 0.432.  Accuracy: tr - 0.82, val - 0.846.\n",
      "Fold 8 trained 10 epochs in 6.344s.  Loss: tr - 0.327, val - 0.277.  Accuracy: tr - 0.88, val - 0.905.\n",
      "Fold 9 trained 10 epochs in 6.271s.  Loss: tr - 0.457, val - 0.426.  Accuracy: tr - 0.843, val - 0.843.\n",
      "Cross-validation completed in 63.627s. Mean validation loss 0.354 and acc 0.866\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:57:40,402 - [BO (140569664263600).INFO] -- evaluate takes 63.6297s\n",
      "05/15/2024 07:57:40,402 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:57:40,403 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3535624802112579, solution: [0.000120239509280517, 0.8780960337990369, 0.9973896269096522, 0.014279899393492276, 53, 3, 39, 0, 1]\n",
      "05/15/2024 07:57:40,421 - [BO (140569664263600).INFO] -- model r2: 0.5637807358297622, MAPE: 0.566831478979534\n",
      "05/15/2024 07:57:40,421 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:57:40,422 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:57:40,422 - [BO (140569664263600).INFO] -- tell takes 0.0198s\n",
      "05/15/2024 07:57:40,422 - [BO (140569664263600).INFO] -- iteration 25 starts...\n",
      "05/15/2024 07:57:43,499 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 3.0768s\n",
      "05/15/2024 07:57:43,500 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:57:43,500 - [BO (140569664263600).INFO] -- #1 - [0.0013878106613925813, 0.8508670804101761, 0.9921410749074717, 0.01477427393104637, 47, 4, 102, 0, 1]\n",
      "05/15/2024 07:57:43,501 - [BO (140569664263600).INFO] -- ask takes 3.0782s\n",
      "lr 0.0014, beta_1 0.8509, beta_2 0.9921, l2 0.0148, filters 47, kernel_sz 4,  dense_sz 102, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.45s.  Loss: tr - 0.031, val - 0.027.  Accuracy: tr - 0.99, val - 0.992.\n",
      "Fold 1 trained 10 epochs in 7.741s.  Loss: tr - 0.032, val - 0.026.  Accuracy: tr - 0.991, val - 0.993.\n",
      "Fold 2 trained 10 epochs in 7.412s.  Loss: tr - 0.036, val - 0.042.  Accuracy: tr - 0.989, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 7.398s.  Loss: tr - 0.223, val - 0.174.  Accuracy: tr - 0.918, val - 0.928.\n",
      "Fold 4 trained 10 epochs in 7.407s.  Loss: tr - 0.039, val - 0.028.  Accuracy: tr - 0.987, val - 0.991.\n",
      "Fold 5 trained 10 epochs in 7.403s.  Loss: tr - 0.068, val - 0.059.  Accuracy: tr - 0.976, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 7.435s.  Loss: tr - 0.047, val - 0.039.  Accuracy: tr - 0.984, val - 0.988.\n",
      "Fold 7 trained 10 epochs in 7.76s.  Loss: tr - 0.065, val - 0.054.  Accuracy: tr - 0.98, val - 0.98.\n",
      "Fold 8 trained 10 epochs in 7.406s.  Loss: tr - 0.058, val - 0.045.  Accuracy: tr - 0.98, val - 0.985.\n",
      "Fold 9 trained 10 epochs in 7.436s.  Loss: tr - 0.069, val - 0.065.  Accuracy: tr - 0.977, val - 0.978.\n",
      "Cross-validation completed in 74.852s. Mean validation loss 0.056 and acc 0.981\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:58:58,356 - [BO (140569664263600).INFO] -- evaluate takes 74.8552s\n",
      "05/15/2024 07:58:58,357 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:58:58,357 - [BO (140569664263600).INFO] -- #1 - fitness: 0.05605660025030375, solution: [0.0013878106613925813, 0.8508670804101761, 0.9921410749074717, 0.01477427393104637, 47, 4, 102, 0, 1]\n",
      "05/15/2024 07:58:58,375 - [BO (140569664263600).INFO] -- model r2: 0.5725701398801653, MAPE: 0.5510565219360127\n",
      "05/15/2024 07:58:58,376 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:58:58,376 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:58:58,376 - [BO (140569664263600).INFO] -- tell takes 0.0199s\n",
      "05/15/2024 07:58:58,377 - [BO (140569664263600).INFO] -- iteration 26 starts...\n",
      "05/15/2024 07:58:59,620 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.2428s\n",
      "05/15/2024 07:58:59,621 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:58:59,621 - [BO (140569664263600).INFO] -- #1 - [0.0062564827954748655, 0.8865625627417537, 0.9958678379470439, 0.012001792442146757, 52, 3, 143, 0, 1]\n",
      "05/15/2024 07:58:59,621 - [BO (140569664263600).INFO] -- ask takes 1.2442s\n",
      "lr 0.0063, beta_1 0.8866, beta_2 0.9959, l2 0.012, filters 52, kernel_sz 3,  dense_sz 143, activs relu, padding 1\n",
      "Fold 0 trained 6 epochs in 3.738s.  Loss: tr - 35.595, val - 43.817.  Accuracy: tr - 0.559, val - 0.562.\n",
      "Fold 1 trained 6 epochs in 3.763s.  Loss: tr - 46.101, val - 56.743.  Accuracy: tr - 0.458, val - 0.433.\n",
      "Fold 2 trained 6 epochs in 3.713s.  Loss: tr - 34.961, val - 40.979.  Accuracy: tr - 0.558, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 6.451s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.594, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 6.246s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 6.159s.  Loss: tr - 0.653, val - 0.627.  Accuracy: tr - 0.613, val - 0.655.\n",
      "Fold 6 trained 6 epochs in 3.77s.  Loss: tr - 34.34, val - 43.833.  Accuracy: tr - 0.563, val - 0.562.\n",
      "Fold 7 trained 6 epochs in 3.76s.  Loss: tr - 35.533, val - 42.431.  Accuracy: tr - 0.558, val - 0.576.\n",
      "Fold 8 trained 6 epochs in 3.711s.  Loss: tr - 34.209, val - 41.106.  Accuracy: tr - 0.559, val - 0.589.\n",
      "Fold 9 trained 6 epochs in 3.749s.  Loss: tr - 35.941, val - 42.712.  Accuracy: tr - 0.551, val - 0.573.\n",
      "Cross-validation completed in 45.065s. Mean validation loss 31.355 and acc 0.573\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 07:59:44,690 - [BO (140569664263600).INFO] -- evaluate takes 45.0682s\n",
      "05/15/2024 07:59:44,690 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 07:59:44,690 - [BO (140569664263600).INFO] -- #1 - fitness: 31.355286288261414, solution: [0.0062564827954748655, 0.8865625627417537, 0.9958678379470439, 0.012001792442146757, 52, 3, 143, 0, 1]\n",
      "05/15/2024 07:59:44,708 - [BO (140569664263600).INFO] -- model r2: 0.5092515453995132, MAPE: 0.5942199645903028\n",
      "05/15/2024 07:59:44,708 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 07:59:44,709 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 07:59:44,709 - [BO (140569664263600).INFO] -- tell takes 0.0190s\n",
      "05/15/2024 07:59:44,709 - [BO (140569664263600).INFO] -- iteration 27 starts...\n",
      "05/15/2024 07:59:46,259 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5498s\n",
      "05/15/2024 07:59:46,260 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 07:59:46,260 - [BO (140569664263600).INFO] -- #1 - [0.001720443688965925, 0.9039059970595548, 0.9971768020332165, 0.014701248754069682, 52, 3, 113, 0, 0]\n",
      "05/15/2024 07:59:46,261 - [BO (140569664263600).INFO] -- ask takes 1.5512s\n",
      "lr 0.0017, beta_1 0.9039, beta_2 0.9972, l2 0.0147, filters 52, kernel_sz 3,  dense_sz 113, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.296s.  Loss: tr - 0.044, val - 0.031.  Accuracy: tr - 0.988, val - 0.99.\n",
      "Fold 1 trained 10 epochs in 5.012s.  Loss: tr - 0.063, val - 0.051.  Accuracy: tr - 0.977, val - 0.984.\n",
      "Fold 2 trained 10 epochs in 5.085s.  Loss: tr - 0.064, val - 0.053.  Accuracy: tr - 0.976, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 5.035s.  Loss: tr - 0.049, val - 0.046.  Accuracy: tr - 0.984, val - 0.986.\n",
      "Fold 4 trained 10 epochs in 5.095s.  Loss: tr - 0.039, val - 0.033.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 5 trained 10 epochs in 5.368s.  Loss: tr - 0.028, val - 0.031.  Accuracy: tr - 0.991, val - 0.991.\n",
      "Fold 6 trained 10 epochs in 5.006s.  Loss: tr - 0.069, val - 0.066.  Accuracy: tr - 0.975, val - 0.975.\n",
      "Fold 7 trained 10 epochs in 5.013s.  Loss: tr - 0.055, val - 0.059.  Accuracy: tr - 0.98, val - 0.981.\n",
      "Fold 8 trained 10 epochs in 4.993s.  Loss: tr - 0.071, val - 0.067.  Accuracy: tr - 0.973, val - 0.976.\n",
      "Fold 9 trained 10 epochs in 5.01s.  Loss: tr - 0.049, val - 0.044.  Accuracy: tr - 0.984, val - 0.983.\n",
      "Cross-validation completed in 50.916s. Mean validation loss 0.048 and acc 0.984\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:00:37,180 - [BO (140569664263600).INFO] -- evaluate takes 50.9194s\n",
      "05/15/2024 08:00:37,181 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:00:37,181 - [BO (140569664263600).INFO] -- #1 - fitness: 0.04808393977582455, solution: [0.001720443688965925, 0.9039059970595548, 0.9971768020332165, 0.014701248754069682, 52, 3, 113, 0, 0]\n",
      "05/15/2024 08:00:37,198 - [BO (140569664263600).INFO] -- model r2: 0.6458752771813245, MAPE: 0.5911727862282917\n",
      "05/15/2024 08:00:37,199 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:00:37,199 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:00:37,200 - [BO (140569664263600).INFO] -- tell takes 0.0190s\n",
      "05/15/2024 08:00:37,200 - [BO (140569664263600).INFO] -- iteration 28 starts...\n",
      "05/15/2024 08:00:39,526 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.3265s\n",
      "05/15/2024 08:00:39,527 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:00:39,528 - [BO (140569664263600).INFO] -- #1 - [0.0014299636828172195, 0.8541381027601445, 0.992161857846973, 0.01547877355363725, 52, 2, 71, 0, 1]\n",
      "05/15/2024 08:00:39,528 - [BO (140569664263600).INFO] -- ask takes 2.3279s\n",
      "lr 0.0014, beta_1 0.8541, beta_2 0.9922, l2 0.0155, filters 52, kernel_sz 2,  dense_sz 71, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.719s.  Loss: tr - 0.088, val - 0.069.  Accuracy: tr - 0.969, val - 0.98.\n",
      "Fold 1 trained 10 epochs in 5.407s.  Loss: tr - 0.038, val - 0.033.  Accuracy: tr - 0.989, val - 0.992.\n",
      "Fold 2 trained 10 epochs in 5.423s.  Loss: tr - 0.081, val - 0.075.  Accuracy: tr - 0.972, val - 0.98.\n",
      "Fold 3 trained 10 epochs in 5.433s.  Loss: tr - 0.059, val - 0.055.  Accuracy: tr - 0.982, val - 0.983.\n",
      "Fold 4 trained 10 epochs in 5.424s.  Loss: tr - 0.055, val - 0.056.  Accuracy: tr - 0.982, val - 0.981.\n",
      "Fold 5 trained 10 epochs in 5.69s.  Loss: tr - 0.042, val - 0.042.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 6 trained 10 epochs in 5.453s.  Loss: tr - 0.154, val - 0.092.  Accuracy: tr - 0.947, val - 0.973.\n",
      "Fold 7 trained 10 epochs in 5.456s.  Loss: tr - 0.046, val - 0.043.  Accuracy: tr - 0.986, val - 0.987.\n",
      "Fold 8 trained 10 epochs in 5.436s.  Loss: tr - 0.239, val - 0.178.  Accuracy: tr - 0.919, val - 0.94.\n",
      "Fold 9 trained 10 epochs in 5.475s.  Loss: tr - 0.168, val - 0.138.  Accuracy: tr - 0.931, val - 0.948.\n",
      "Cross-validation completed in 54.922s. Mean validation loss 0.078 and acc 0.975\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:01:34,453 - [BO (140569664263600).INFO] -- evaluate takes 54.9251s\n",
      "05/15/2024 08:01:34,454 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:01:34,454 - [BO (140569664263600).INFO] -- #1 - fitness: 0.07812550216913224, solution: [0.0014299636828172195, 0.8541381027601445, 0.992161857846973, 0.01547877355363725, 52, 2, 71, 0, 1]\n",
      "05/15/2024 08:01:34,471 - [BO (140569664263600).INFO] -- model r2: 0.66864897480055, MAPE: 0.5576375650797737\n",
      "05/15/2024 08:01:34,472 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:01:34,472 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:01:34,473 - [BO (140569664263600).INFO] -- tell takes 0.0190s\n",
      "05/15/2024 08:01:34,473 - [BO (140569664263600).INFO] -- iteration 29 starts...\n",
      "05/15/2024 08:01:36,765 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.2916s\n",
      "05/15/2024 08:01:36,766 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:01:36,766 - [BO (140569664263600).INFO] -- #1 - [0.005358360828872393, 0.8697067337834553, 0.9986661071153092, 0.011814123748294617, 60, 3, 128, 1, 0]\n",
      "05/15/2024 08:01:36,766 - [BO (140569664263600).INFO] -- ask takes 2.2932s\n",
      "lr 0.0054, beta_1 0.8697, beta_2 0.9987, l2 0.0118, filters 60, kernel_sz 3,  dense_sz 128, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 5.947s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.587, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 5.672s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.586, val - 0.587.\n",
      "Fold 2 trained 10 epochs in 5.683s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.584, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 5.666s.  Loss: tr - 0.661, val - 0.65.  Accuracy: tr - 0.584, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 5.637s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.588, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 5.672s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.593, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 5.908s.  Loss: tr - 0.657, val - 0.663.  Accuracy: tr - 0.591, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.634s.  Loss: tr - 0.658, val - 0.665.  Accuracy: tr - 0.583, val - 0.574.\n",
      "Fold 8 trained 7 epochs in 4.033s.  Loss: tr - 0.663, val - 0.658.  Accuracy: tr - 0.581, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 5.633s.  Loss: tr - 0.664, val - 0.66.  Accuracy: tr - 0.58, val - 0.592.\n",
      "Cross-validation completed in 55.489s. Mean validation loss 0.657 and acc 0.593\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:02:32,259 - [BO (140569664263600).INFO] -- evaluate takes 55.4923s\n",
      "05/15/2024 08:02:32,259 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:02:32,260 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6568542122840881, solution: [0.005358360828872393, 0.8697067337834553, 0.9986661071153092, 0.011814123748294617, 60, 3, 128, 1, 0]\n",
      "05/15/2024 08:02:32,277 - [BO (140569664263600).INFO] -- model r2: 0.6573889034046903, MAPE: 0.42563621192062345\n",
      "05/15/2024 08:02:32,278 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:02:32,278 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:02:32,279 - [BO (140569664263600).INFO] -- tell takes 0.0195s\n",
      "05/15/2024 08:02:32,279 - [BO (140569664263600).INFO] -- iteration 30 starts...\n",
      "05/15/2024 08:02:33,299 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.0193s\n",
      "05/15/2024 08:02:33,299 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:02:33,300 - [BO (140569664263600).INFO] -- #1 - [0.006568059467655097, 0.873548590054517, 0.9943980002364777, 0.007570024764053031, 52, 2, 93, 1, 1]\n",
      "05/15/2024 08:02:33,300 - [BO (140569664263600).INFO] -- ask takes 1.0207s\n",
      "lr 0.0066, beta_1 0.8735, beta_2 0.9944, l2 0.0076, filters 52, kernel_sz 2,  dense_sz 93, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 5.509s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.579, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 5.468s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.586, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 5.76s.  Loss: tr - 0.666, val - 0.655.  Accuracy: tr - 0.577, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 5.443s.  Loss: tr - 0.66, val - 0.651.  Accuracy: tr - 0.589, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 5.472s.  Loss: tr - 0.66, val - 0.657.  Accuracy: tr - 0.584, val - 0.587.\n",
      "Fold 5 trained 10 epochs in 5.464s.  Loss: tr - 0.662, val - 0.651.  Accuracy: tr - 0.581, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 5.463s.  Loss: tr - 0.666, val - 0.662.  Accuracy: tr - 0.576, val - 0.585.\n",
      "Fold 7 trained 10 epochs in 5.82s.  Loss: tr - 0.657, val - 0.664.  Accuracy: tr - 0.593, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.48s.  Loss: tr - 0.664, val - 0.659.  Accuracy: tr - 0.579, val - 0.589.\n",
      "Fold 9 trained 10 epochs in 5.487s.  Loss: tr - 0.66, val - 0.661.  Accuracy: tr - 0.582, val - 0.592.\n",
      "Cross-validation completed in 55.371s. Mean validation loss 0.657 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:03:28,674 - [BO (140569664263600).INFO] -- evaluate takes 55.3739s\n",
      "05/15/2024 08:03:28,674 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:03:28,675 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6572854697704316, solution: [0.006568059467655097, 0.873548590054517, 0.9943980002364777, 0.007570024764053031, 52, 2, 93, 1, 1]\n",
      "05/15/2024 08:03:28,693 - [BO (140569664263600).INFO] -- model r2: 0.5720002167401398, MAPE: 0.845794773854219\n",
      "05/15/2024 08:03:28,693 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:03:28,694 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:03:28,694 - [BO (140569664263600).INFO] -- tell takes 0.0199s\n",
      "05/15/2024 08:03:28,695 - [BO (140569664263600).INFO] -- iteration 31 starts...\n",
      "05/15/2024 08:03:29,856 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.1611s\n",
      "05/15/2024 08:03:29,857 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:03:29,858 - [BO (140569664263600).INFO] -- #1 - [0.0011091093220041096, 0.8500504674145681, 0.9958792408526236, 0.014596610929657954, 63, 2, 32, 0, 1]\n",
      "05/15/2024 08:03:29,858 - [BO (140569664263600).INFO] -- ask takes 1.1626s\n",
      "lr 0.0011, beta_1 0.8501, beta_2 0.9959, l2 0.0146, filters 63, kernel_sz 2,  dense_sz 32, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.134s.  Loss: tr - 0.191, val - 0.149.  Accuracy: tr - 0.926, val - 0.951.\n",
      "Fold 1 trained 10 epochs in 8.196s.  Loss: tr - 0.132, val - 0.093.  Accuracy: tr - 0.954, val - 0.967.\n",
      "Fold 2 trained 10 epochs in 8.362s.  Loss: tr - 0.457, val - 0.427.  Accuracy: tr - 0.779, val - 0.792.\n",
      "Fold 3 trained 10 epochs in 8.128s.  Loss: tr - 0.141, val - 0.138.  Accuracy: tr - 0.948, val - 0.948.\n",
      "Fold 4 trained 10 epochs in 8.175s.  Loss: tr - 0.467, val - 0.443.  Accuracy: tr - 0.78, val - 0.795.\n",
      "Fold 5 trained 10 epochs in 8.135s.  Loss: tr - 0.356, val - 0.327.  Accuracy: tr - 0.823, val - 0.842.\n",
      "Fold 6 trained 10 epochs in 8.095s.  Loss: tr - 0.169, val - 0.13.  Accuracy: tr - 0.94, val - 0.958.\n",
      "Fold 7 trained 10 epochs in 8.375s.  Loss: tr - 0.079, val - 0.063.  Accuracy: tr - 0.976, val - 0.977.\n",
      "Fold 8 trained 10 epochs in 8.132s.  Loss: tr - 0.379, val - 0.327.  Accuracy: tr - 0.817, val - 0.847.\n",
      "Fold 9 trained 10 epochs in 8.113s.  Loss: tr - 0.154, val - 0.127.  Accuracy: tr - 0.942, val - 0.955.\n",
      "Cross-validation completed in 81.848s. Mean validation loss 0.223 and acc 0.903\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:04:51,710 - [BO (140569664263600).INFO] -- evaluate takes 81.8515s\n",
      "05/15/2024 08:04:51,710 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:04:51,710 - [BO (140569664263600).INFO] -- #1 - fitness: 0.22252389043569565, solution: [0.0011091093220041096, 0.8500504674145681, 0.9958792408526236, 0.014596610929657954, 63, 2, 32, 0, 1]\n",
      "05/15/2024 08:04:51,730 - [BO (140569664263600).INFO] -- model r2: 0.645892635848581, MAPE: 0.6304002999919844\n",
      "05/15/2024 08:04:51,730 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:04:51,731 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:04:51,731 - [BO (140569664263600).INFO] -- tell takes 0.0210s\n",
      "05/15/2024 08:04:51,731 - [BO (140569664263600).INFO] -- iteration 32 starts...\n",
      "05/15/2024 08:04:54,356 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.6244s\n",
      "05/15/2024 08:04:54,357 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:04:54,357 - [BO (140569664263600).INFO] -- #1 - [0.0058262692340774165, 0.854856031003246, 0.9949364714632692, 0.019742114663526326, 37, 2, 120, 1, 1]\n",
      "05/15/2024 08:04:54,358 - [BO (140569664263600).INFO] -- ask takes 2.6260s\n",
      "lr 0.0058, beta_1 0.8549, beta_2 0.9949, l2 0.0197, filters 37, kernel_sz 2,  dense_sz 120, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 4.209s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.591, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 4.211s.  Loss: tr - 0.682, val - 0.657.  Accuracy: tr - 0.557, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 4.217s.  Loss: tr - 0.658, val - 0.654.  Accuracy: tr - 0.593, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 4.288s.  Loss: tr - 0.664, val - 0.65.  Accuracy: tr - 0.569, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 4.524s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.592, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 4.276s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.594, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 4.2s.  Loss: tr - 0.657, val - 0.662.  Accuracy: tr - 0.593, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 4.228s.  Loss: tr - 0.658, val - 0.665.  Accuracy: tr - 0.588, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 4.237s.  Loss: tr - 0.657, val - 0.657.  Accuracy: tr - 0.59, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 4.51s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.591, val - 0.592.\n",
      "Cross-validation completed in 42.903s. Mean validation loss 0.657 and acc 0.595\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:05:37,264 - [BO (140569664263600).INFO] -- evaluate takes 42.9063s\n",
      "05/15/2024 08:05:37,265 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:05:37,265 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6565915644168854, solution: [0.0058262692340774165, 0.854856031003246, 0.9949364714632692, 0.019742114663526326, 37, 2, 120, 1, 1]\n",
      "05/15/2024 08:05:37,284 - [BO (140569664263600).INFO] -- model r2: 0.5908606254650071, MAPE: 0.7685445400196673\n",
      "05/15/2024 08:05:37,285 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:05:37,285 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:05:37,285 - [BO (140569664263600).INFO] -- tell takes 0.0205s\n",
      "05/15/2024 08:05:37,285 - [BO (140569664263600).INFO] -- iteration 33 starts...\n",
      "05/15/2024 08:05:39,205 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.9192s\n",
      "05/15/2024 08:05:39,206 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:05:39,206 - [BO (140569664263600).INFO] -- #1 - [0.001425307575663814, 0.9125268515691074, 0.9929231686663671, 0.007564137847656376, 40, 4, 118, 0, 1]\n",
      "05/15/2024 08:05:39,206 - [BO (140569664263600).INFO] -- ask takes 1.9206s\n",
      "lr 0.0014, beta_1 0.9125, beta_2 0.9929, l2 0.0076, filters 40, kernel_sz 4,  dense_sz 118, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 5.716s.  Loss: tr - 0.043, val - 0.033.  Accuracy: tr - 0.987, val - 0.988.\n",
      "Fold 1 trained 10 epochs in 5.702s.  Loss: tr - 0.056, val - 0.043.  Accuracy: tr - 0.98, val - 0.983.\n",
      "Fold 2 trained 10 epochs in 5.598s.  Loss: tr - 0.048, val - 0.047.  Accuracy: tr - 0.983, val - 0.987.\n",
      "Fold 3 trained 10 epochs in 5.598s.  Loss: tr - 0.086, val - 0.081.  Accuracy: tr - 0.969, val - 0.972.\n",
      "Fold 4 trained 10 epochs in 5.605s.  Loss: tr - 0.044, val - 0.039.  Accuracy: tr - 0.984, val - 0.988.\n",
      "Fold 5 trained 10 epochs in 5.584s.  Loss: tr - 0.05, val - 0.045.  Accuracy: tr - 0.983, val - 0.986.\n",
      "Fold 6 trained 10 epochs in 5.924s.  Loss: tr - 0.049, val - 0.04.  Accuracy: tr - 0.983, val - 0.988.\n",
      "Fold 7 trained 10 epochs in 5.582s.  Loss: tr - 0.043, val - 0.038.  Accuracy: tr - 0.986, val - 0.986.\n",
      "Fold 8 trained 10 epochs in 5.579s.  Loss: tr - 0.074, val - 0.045.  Accuracy: tr - 0.973, val - 0.982.\n",
      "Fold 9 trained 10 epochs in 5.604s.  Loss: tr - 0.04, val - 0.038.  Accuracy: tr - 0.987, val - 0.986.\n",
      "Cross-validation completed in 56.494s. Mean validation loss 0.045 and acc 0.985\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:06:35,704 - [BO (140569664263600).INFO] -- evaluate takes 56.4971s\n",
      "05/15/2024 08:06:35,704 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:06:35,704 - [BO (140569664263600).INFO] -- #1 - fitness: 0.04480002298951149, solution: [0.001425307575663814, 0.9125268515691074, 0.9929231686663671, 0.007564137847656376, 40, 4, 118, 0, 1]\n",
      "05/15/2024 08:06:35,722 - [BO (140569664263600).INFO] -- model r2: 0.5853652586926328, MAPE: 0.7353632848574424\n",
      "05/15/2024 08:06:35,723 - [BO (140569664263600).INFO] -- fopt: [0.0418186]\n",
      "05/15/2024 08:06:35,723 - [BO (140569664263600).INFO] -- xopt: [0.0003984857558857353, 0.8526153343358599, 0.9906089102312503, 0.013692815829915545, 54, 5, 128, 0, 1]\n",
      "\n",
      "05/15/2024 08:06:35,723 - [BO (140569664263600).INFO] -- tell takes 0.0192s\n",
      "05/15/2024 08:06:35,723 - [BO (140569664263600).INFO] -- iteration 34 starts...\n",
      "05/15/2024 08:06:38,472 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.7488s\n",
      "05/15/2024 08:06:38,473 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:06:38,474 - [BO (140569664263600).INFO] -- #1 - [0.001315006142881965, 0.8638629169730448, 0.9919726765435116, 0.01851826840591084, 42, 3, 149, 0, 0]\n",
      "05/15/2024 08:06:38,474 - [BO (140569664263600).INFO] -- ask takes 2.7503s\n",
      "lr 0.0013, beta_1 0.8639, beta_2 0.992, l2 0.0185, filters 42, kernel_sz 3,  dense_sz 149, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.391s.  Loss: tr - 0.037, val - 0.028.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 1 trained 10 epochs in 4.335s.  Loss: tr - 0.036, val - 0.033.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 4.687s.  Loss: tr - 0.035, val - 0.034.  Accuracy: tr - 0.989, val - 0.992.\n",
      "Fold 3 trained 10 epochs in 4.471s.  Loss: tr - 0.028, val - 0.033.  Accuracy: tr - 0.991, val - 0.99.\n",
      "Fold 4 trained 10 epochs in 4.329s.  Loss: tr - 0.066, val - 0.059.  Accuracy: tr - 0.978, val - 0.98.\n",
      "Fold 5 trained 10 epochs in 4.401s.  Loss: tr - 0.039, val - 0.048.  Accuracy: tr - 0.988, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 4.368s.  Loss: tr - 0.043, val - 0.045.  Accuracy: tr - 0.986, val - 0.984.\n",
      "Fold 7 trained 10 epochs in 4.608s.  Loss: tr - 0.032, val - 0.033.  Accuracy: tr - 0.991, val - 0.988.\n",
      "Fold 8 trained 10 epochs in 4.366s.  Loss: tr - 0.044, val - 0.039.  Accuracy: tr - 0.987, val - 0.987.\n",
      "Fold 9 trained 10 epochs in 4.384s.  Loss: tr - 0.069, val - 0.056.  Accuracy: tr - 0.977, val - 0.98.\n",
      "Cross-validation completed in 44.345s. Mean validation loss 0.041 and acc 0.986\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:07:22,823 - [BO (140569664263600).INFO] -- evaluate takes 44.3485s\n",
      "05/15/2024 08:07:22,823 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:07:22,824 - [BO (140569664263600).INFO] -- #1 - fitness: 0.040892111323773864, solution: [0.001315006142881965, 0.8638629169730448, 0.9919726765435116, 0.01851826840591084, 42, 3, 149, 0, 0]\n",
      "05/15/2024 08:07:22,841 - [BO (140569664263600).INFO] -- model r2: 0.5094805241065465, MAPE: 0.6422739611014407\n",
      "05/15/2024 08:07:22,842 - [BO (140569664263600).INFO] -- fopt: [0.04089211]\n",
      "05/15/2024 08:07:22,842 - [BO (140569664263600).INFO] -- xopt: [0.001315006142881965, 0.8638629169730448, 0.9919726765435116, 0.01851826840591084, 42, 3, 149, 0, 0]\n",
      "\n",
      "05/15/2024 08:07:22,842 - [BO (140569664263600).INFO] -- tell takes 0.0193s\n",
      "05/15/2024 08:07:22,843 - [BO (140569664263600).INFO] -- iteration 35 starts...\n",
      "05/15/2024 08:07:25,149 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.3063s\n",
      "05/15/2024 08:07:25,150 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:07:25,150 - [BO (140569664263600).INFO] -- #1 - [0.004763561076114195, 0.9024713954122673, 0.9964311795892832, 0.01935969642933604, 32, 4, 112, 1, 1]\n",
      "05/15/2024 08:07:25,151 - [BO (140569664263600).INFO] -- ask takes 2.3077s\n",
      "lr 0.0048, beta_1 0.9025, beta_2 0.9964, l2 0.0194, filters 32, kernel_sz 4,  dense_sz 112, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 3.546s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.588, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 3.58s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 3.868s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.589, val - 0.583.\n",
      "Fold 3 trained 10 epochs in 3.549s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.586, val - 0.593.\n",
      "Fold 4 trained 10 epochs in 3.555s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 3.574s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.59, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 3.575s.  Loss: tr - 0.657, val - 0.661.  Accuracy: tr - 0.591, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 3.798s.  Loss: tr - 0.659, val - 0.665.  Accuracy: tr - 0.587, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 3.538s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.586, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 3.585s.  Loss: tr - 0.661, val - 0.66.  Accuracy: tr - 0.584, val - 0.589.\n",
      "Cross-validation completed in 36.172s. Mean validation loss 0.656 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:08:01,326 - [BO (140569664263600).INFO] -- evaluate takes 36.1750s\n",
      "05/15/2024 08:08:01,326 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:08:01,327 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6564951300621032, solution: [0.004763561076114195, 0.9024713954122673, 0.9964311795892832, 0.01935969642933604, 32, 4, 112, 1, 1]\n",
      "05/15/2024 08:08:01,345 - [BO (140569664263600).INFO] -- model r2: 0.6459886688796073, MAPE: 0.4651023184844315\n",
      "05/15/2024 08:08:01,345 - [BO (140569664263600).INFO] -- fopt: [0.04089211]\n",
      "05/15/2024 08:08:01,346 - [BO (140569664263600).INFO] -- xopt: [0.001315006142881965, 0.8638629169730448, 0.9919726765435116, 0.01851826840591084, 42, 3, 149, 0, 0]\n",
      "\n",
      "05/15/2024 08:08:01,346 - [BO (140569664263600).INFO] -- tell takes 0.0195s\n",
      "05/15/2024 08:08:01,346 - [BO (140569664263600).INFO] -- iteration 36 starts...\n",
      "05/15/2024 08:08:02,487 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.1404s\n",
      "05/15/2024 08:08:02,488 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:08:02,488 - [BO (140569664263600).INFO] -- #1 - [0.004820432531726644, 0.8600127571609767, 0.99107818237862, 0.014304898747344068, 22, 4, 130, 0, 1]\n",
      "05/15/2024 08:08:02,488 - [BO (140569664263600).INFO] -- ask takes 1.1419s\n",
      "lr 0.0048, beta_1 0.86, beta_2 0.9911, l2 0.0143, filters 22, kernel_sz 4,  dense_sz 130, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 4.316s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 4.337s.  Loss: tr - 0.646, val - 0.624.  Accuracy: tr - 0.613, val - 0.663.\n",
      "Fold 2 trained 10 epochs in 4.277s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.603, val - 0.613.\n",
      "Fold 3 trained 10 epochs in 4.56s.  Loss: tr - 0.044, val - 0.043.  Accuracy: tr - 0.986, val - 0.984.\n",
      "Fold 4 trained 10 epochs in 4.253s.  Loss: tr - 0.522, val - 0.375.  Accuracy: tr - 0.759, val - 0.863.\n",
      "Fold 5 trained 10 epochs in 4.261s.  Loss: tr - 0.626, val - 0.565.  Accuracy: tr - 0.685, val - 0.731.\n",
      "Fold 6 trained 10 epochs in 4.303s.  Loss: tr - 0.654, val - 0.655.  Accuracy: tr - 0.602, val - 0.614.\n",
      "Fold 7 trained 10 epochs in 4.309s.  Loss: tr - 0.656, val - 0.664.  Accuracy: tr - 0.597, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 4.297s.  Loss: tr - 0.668, val - 0.657.  Accuracy: tr - 0.587, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 4.347s.  Loss: tr - 0.611, val - 0.53.  Accuracy: tr - 0.706, val - 0.828.\n",
      "Cross-validation completed in 43.265s. Mean validation loss 0.542 and acc 0.708\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:08:45,757 - [BO (140569664263600).INFO] -- evaluate takes 43.2687s\n",
      "05/15/2024 08:08:45,758 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:08:45,758 - [BO (140569664263600).INFO] -- #1 - fitness: 0.541880552470684, solution: [0.004820432531726644, 0.8600127571609767, 0.99107818237862, 0.014304898747344068, 22, 4, 130, 0, 1]\n",
      "05/15/2024 08:08:45,776 - [BO (140569664263600).INFO] -- model r2: 0.6250617817839976, MAPE: 0.7121495032927182\n",
      "05/15/2024 08:08:45,777 - [BO (140569664263600).INFO] -- fopt: [0.04089211]\n",
      "05/15/2024 08:08:45,777 - [BO (140569664263600).INFO] -- xopt: [0.001315006142881965, 0.8638629169730448, 0.9919726765435116, 0.01851826840591084, 42, 3, 149, 0, 0]\n",
      "\n",
      "05/15/2024 08:08:45,777 - [BO (140569664263600).INFO] -- tell takes 0.0199s\n",
      "05/15/2024 08:08:45,778 - [BO (140569664263600).INFO] -- iteration 37 starts...\n",
      "05/15/2024 08:08:46,859 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.0808s\n",
      "05/15/2024 08:08:46,859 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:08:46,860 - [BO (140569664263600).INFO] -- #1 - [0.009158655563734302, 0.8840461096150417, 0.9943007217526578, 0.014166915425170498, 24, 4, 131, 0, 1]\n",
      "05/15/2024 08:08:46,860 - [BO (140569664263600).INFO] -- ask takes 1.0822s\n",
      "lr 0.0092, beta_1 0.884, beta_2 0.9943, l2 0.0142, filters 24, kernel_sz 4,  dense_sz 131, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 3.132s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 2.814s.  Loss: tr - 0.667, val - 0.663.  Accuracy: tr - 0.589, val - 0.586.\n",
      "Fold 2 trained 10 epochs in 2.806s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 2.778s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.588, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 2.847s.  Loss: tr - 43.965, val - 42.321.  Accuracy: tr - 0.56, val - 0.572.\n",
      "Fold 5 trained 10 epochs in 3.08s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.594, val - 0.597.\n",
      "Fold 6 trained 6 epochs in 1.744s.  Loss: tr - 5.294, val - 0.678.  Accuracy: tr - 0.517, val - 0.564.\n",
      "Fold 7 trained 6 epochs in 1.71s.  Loss: tr - 32.393, val - 41.259.  Accuracy: tr - 0.558, val - 0.587.\n",
      "Fold 8 trained 6 epochs in 1.736s.  Loss: tr - 36.754, val - 44.419.  Accuracy: tr - 0.555, val - 0.556.\n",
      "Fold 9 trained 10 epochs in 2.793s.  Loss: tr - 0.674, val - 0.668.  Accuracy: tr - 0.581, val - 0.589.\n",
      "Cross-validation completed in 25.444s. Mean validation loss 13.262 and acc 0.583\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:09:12,307 - [BO (140569664263600).INFO] -- evaluate takes 25.4470s\n",
      "05/15/2024 08:09:12,308 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:09:12,308 - [BO (140569664263600).INFO] -- #1 - fitness: 13.261926352977753, solution: [0.009158655563734302, 0.8840461096150417, 0.9943007217526578, 0.014166915425170498, 24, 4, 131, 0, 1]\n",
      "05/15/2024 08:09:12,326 - [BO (140569664263600).INFO] -- model r2: 0.7309263624677935, MAPE: 0.5734174788384242\n",
      "05/15/2024 08:09:12,326 - [BO (140569664263600).INFO] -- fopt: [0.04089211]\n",
      "05/15/2024 08:09:12,327 - [BO (140569664263600).INFO] -- xopt: [0.001315006142881965, 0.8638629169730448, 0.9919726765435116, 0.01851826840591084, 42, 3, 149, 0, 0]\n",
      "\n",
      "05/15/2024 08:09:12,327 - [BO (140569664263600).INFO] -- tell takes 0.0192s\n",
      "05/15/2024 08:09:12,327 - [BO (140569664263600).INFO] -- iteration 38 starts...\n",
      "05/15/2024 08:09:14,781 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.4537s\n",
      "05/15/2024 08:09:14,782 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:09:14,782 - [BO (140569664263600).INFO] -- #1 - [0.0013337898455780602, 0.9097923107531188, 0.9942762269552038, 0.015003369920654331, 48, 3, 136, 0, 0]\n",
      "05/15/2024 08:09:14,783 - [BO (140569664263600).INFO] -- ask takes 2.4552s\n",
      "lr 0.0013, beta_1 0.9098, beta_2 0.9943, l2 0.015, filters 48, kernel_sz 3,  dense_sz 136, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.525s.  Loss: tr - 0.042, val - 0.033.  Accuracy: tr - 0.987, val - 0.989.\n",
      "Fold 1 trained 10 epochs in 4.85s.  Loss: tr - 0.037, val - 0.028.  Accuracy: tr - 0.986, val - 0.99.\n",
      "Fold 2 trained 10 epochs in 4.515s.  Loss: tr - 0.028, val - 0.033.  Accuracy: tr - 0.991, val - 0.991.\n",
      "Fold 3 trained 10 epochs in 4.582s.  Loss: tr - 0.055, val - 0.061.  Accuracy: tr - 0.981, val - 0.98.\n",
      "Fold 4 trained 10 epochs in 4.527s.  Loss: tr - 0.036, val - 0.031.  Accuracy: tr - 0.988, val - 0.989.\n",
      "Fold 5 trained 10 epochs in 4.638s.  Loss: tr - 0.037, val - 0.041.  Accuracy: tr - 0.988, val - 0.985.\n",
      "Fold 6 trained 10 epochs in 4.849s.  Loss: tr - 0.038, val - 0.04.  Accuracy: tr - 0.987, val - 0.986.\n",
      "Fold 7 trained 10 epochs in 4.558s.  Loss: tr - 0.076, val - 0.061.  Accuracy: tr - 0.972, val - 0.977.\n",
      "Fold 8 trained 10 epochs in 4.586s.  Loss: tr - 0.043, val - 0.037.  Accuracy: tr - 0.986, val - 0.988.\n",
      "Fold 9 trained 10 epochs in 4.562s.  Loss: tr - 0.047, val - 0.042.  Accuracy: tr - 0.984, val - 0.985.\n",
      "Cross-validation completed in 46.197s. Mean validation loss 0.041 and acc 0.986\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:10:00,983 - [BO (140569664263600).INFO] -- evaluate takes 46.2002s\n",
      "05/15/2024 08:10:00,984 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:10:00,984 - [BO (140569664263600).INFO] -- #1 - fitness: 0.040747479908168314, solution: [0.0013337898455780602, 0.9097923107531188, 0.9942762269552038, 0.015003369920654331, 48, 3, 136, 0, 0]\n",
      "05/15/2024 08:10:01,02 - [BO (140569664263600).INFO] -- model r2: 0.6918000671011011, MAPE: 0.5500927141750519\n",
      "05/15/2024 08:10:01,02 - [BO (140569664263600).INFO] -- fopt: [0.04074748]\n",
      "05/15/2024 08:10:01,03 - [BO (140569664263600).INFO] -- xopt: [0.0013337898455780602, 0.9097923107531188, 0.9942762269552038, 0.015003369920654331, 48, 3, 136, 0, 0]\n",
      "\n",
      "05/15/2024 08:10:01,03 - [BO (140569664263600).INFO] -- tell takes 0.0195s\n",
      "05/15/2024 08:10:01,03 - [BO (140569664263600).INFO] -- iteration 39 starts...\n",
      "05/15/2024 08:10:03,143 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.1399s\n",
      "05/15/2024 08:10:03,144 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:10:03,145 - [BO (140569664263600).INFO] -- #1 - [0.003951929417829109, 0.9141538712751096, 0.9941716528426745, 0.01979752998726888, 52, 3, 119, 1, 0]\n",
      "05/15/2024 08:10:03,145 - [BO (140569664263600).INFO] -- ask takes 2.1413s\n",
      "lr 0.004, beta_1 0.9142, beta_2 0.9942, l2 0.0198, filters 52, kernel_sz 3,  dense_sz 119, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 5.046s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.59, val - 0.594.\n",
      "Fold 1 trained 10 epochs in 5.098s.  Loss: tr - 0.661, val - 0.656.  Accuracy: tr - 0.592, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 5.386s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.584, val - 0.583.\n",
      "Fold 3 trained 10 epochs in 5.13s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.581, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 5.083s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 5.148s.  Loss: tr - 0.659, val - 0.652.  Accuracy: tr - 0.587, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 5.05s.  Loss: tr - 0.658, val - 0.661.  Accuracy: tr - 0.587, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.09s.  Loss: tr - 0.657, val - 0.665.  Accuracy: tr - 0.59, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.424s.  Loss: tr - 0.659, val - 0.658.  Accuracy: tr - 0.582, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 5.105s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.587, val - 0.592.\n",
      "Cross-validation completed in 51.563s. Mean validation loss 0.657 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:10:54,711 - [BO (140569664263600).INFO] -- evaluate takes 51.5663s\n",
      "05/15/2024 08:10:54,712 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:10:54,712 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6566510081291199, solution: [0.003951929417829109, 0.9141538712751096, 0.9941716528426745, 0.01979752998726888, 52, 3, 119, 1, 0]\n",
      "05/15/2024 08:10:54,731 - [BO (140569664263600).INFO] -- model r2: 0.5891440268234941, MAPE: 0.6229757256999698\n",
      "05/15/2024 08:10:54,731 - [BO (140569664263600).INFO] -- fopt: [0.04074748]\n",
      "05/15/2024 08:10:54,732 - [BO (140569664263600).INFO] -- xopt: [0.0013337898455780602, 0.9097923107531188, 0.9942762269552038, 0.015003369920654331, 48, 3, 136, 0, 0]\n",
      "\n",
      "05/15/2024 08:10:54,732 - [BO (140569664263600).INFO] -- tell takes 0.0201s\n",
      "05/15/2024 08:10:54,732 - [BO (140569664263600).INFO] -- iteration 40 starts...\n",
      "05/15/2024 08:10:56,624 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.8906s\n",
      "05/15/2024 08:10:56,625 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:10:56,625 - [BO (140569664263600).INFO] -- #1 - [0.007057585890765719, 0.8709837806515478, 0.9908112862894968, 0.007519045826069364, 37, 3, 124, 1, 1]\n",
      "05/15/2024 08:10:56,625 - [BO (140569664263600).INFO] -- ask takes 1.8923s\n",
      "lr 0.0071, beta_1 0.871, beta_2 0.9908, l2 0.0075, filters 37, kernel_sz 3,  dense_sz 124, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 6.965s.  Loss: tr - 0.674, val - 0.656.  Accuracy: tr - 0.557, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 6.974s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.585, val - 0.582.\n",
      "Fold 2 trained 10 epochs in 7.005s.  Loss: tr - 0.699, val - 0.658.  Accuracy: tr - 0.526, val - 0.583.\n",
      "Fold 3 trained 10 epochs in 7.386s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.587, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 6.999s.  Loss: tr - 0.667, val - 0.657.  Accuracy: tr - 0.573, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 7.005s.  Loss: tr - 0.671, val - 0.654.  Accuracy: tr - 0.58, val - 0.584.\n",
      "Fold 6 trained 10 epochs in 7.037s.  Loss: tr - 0.661, val - 0.663.  Accuracy: tr - 0.584, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 6.985s.  Loss: tr - 0.665, val - 0.664.  Accuracy: tr - 0.578, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 7.016s.  Loss: tr - 0.666, val - 0.658.  Accuracy: tr - 0.582, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 7.238s.  Loss: tr - 0.702, val - 0.661.  Accuracy: tr - 0.537, val - 0.589.\n",
      "Cross-validation completed in 70.614s. Mean validation loss 0.658 and acc 0.591\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:12:07,243 - [BO (140569664263600).INFO] -- evaluate takes 70.6173s\n",
      "05/15/2024 08:12:07,243 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:12:07,244 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6577674090862274, solution: [0.007057585890765719, 0.8709837806515478, 0.9908112862894968, 0.007519045826069364, 37, 3, 124, 1, 1]\n",
      "05/15/2024 08:12:07,262 - [BO (140569664263600).INFO] -- model r2: 0.6709849048027561, MAPE: 0.5333618703501004\n",
      "05/15/2024 08:12:07,263 - [BO (140569664263600).INFO] -- fopt: [0.04074748]\n",
      "05/15/2024 08:12:07,263 - [BO (140569664263600).INFO] -- xopt: [0.0013337898455780602, 0.9097923107531188, 0.9942762269552038, 0.015003369920654331, 48, 3, 136, 0, 0]\n",
      "\n",
      "05/15/2024 08:12:07,263 - [BO (140569664263600).INFO] -- tell takes 0.0198s\n",
      "05/15/2024 08:12:07,263 - [BO (140569664263600).INFO] -- iteration 41 starts...\n",
      "05/15/2024 08:12:08,866 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.6027s\n",
      "05/15/2024 08:12:08,867 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:12:08,868 - [BO (140569664263600).INFO] -- #1 - [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "05/15/2024 08:12:08,868 - [BO (140569664263600).INFO] -- ask takes 1.6041s\n",
      "lr 0.0008, beta_1 0.8979, beta_2 0.9948, l2 0.0199, filters 58, kernel_sz 3,  dense_sz 141, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.853s.  Loss: tr - 0.034, val - 0.026.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 1 trained 10 epochs in 6.809s.  Loss: tr - 0.046, val - 0.042.  Accuracy: tr - 0.985, val - 0.986.\n",
      "Fold 2 trained 10 epochs in 6.804s.  Loss: tr - 0.039, val - 0.041.  Accuracy: tr - 0.988, val - 0.987.\n",
      "Fold 3 trained 10 epochs in 6.855s.  Loss: tr - 0.033, val - 0.032.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 6.841s.  Loss: tr - 0.032, val - 0.03.  Accuracy: tr - 0.991, val - 0.992.\n",
      "Fold 5 trained 10 epochs in 7.117s.  Loss: tr - 0.027, val - 0.034.  Accuracy: tr - 0.992, val - 0.992.\n",
      "Fold 6 trained 10 epochs in 6.848s.  Loss: tr - 0.031, val - 0.041.  Accuracy: tr - 0.99, val - 0.989.\n",
      "Fold 7 trained 10 epochs in 6.814s.  Loss: tr - 0.042, val - 0.035.  Accuracy: tr - 0.987, val - 0.988.\n",
      "Fold 8 trained 10 epochs in 6.796s.  Loss: tr - 0.03, val - 0.028.  Accuracy: tr - 0.991, val - 0.992.\n",
      "Fold 9 trained 10 epochs in 6.815s.  Loss: tr - 0.043, val - 0.041.  Accuracy: tr - 0.986, val - 0.986.\n",
      "Cross-validation completed in 68.555s. Mean validation loss 0.035 and acc 0.989\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:13:17,427 - [BO (140569664263600).INFO] -- evaluate takes 68.5585s\n",
      "05/15/2024 08:13:17,427 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:13:17,427 - [BO (140569664263600).INFO] -- #1 - fitness: 0.0348746920004487, solution: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "05/15/2024 08:13:17,447 - [BO (140569664263600).INFO] -- model r2: 0.8242426727833703, MAPE: 0.39506923992275117\n",
      "05/15/2024 08:13:17,448 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:13:17,448 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:13:17,448 - [BO (140569664263600).INFO] -- tell takes 0.0214s\n",
      "05/15/2024 08:13:17,449 - [BO (140569664263600).INFO] -- iteration 42 starts...\n",
      "05/15/2024 08:13:18,561 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.1121s\n",
      "05/15/2024 08:13:18,562 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:13:18,562 - [BO (140569664263600).INFO] -- #1 - [0.008137025350148376, 0.9366392545500491, 0.9940143126044647, 0.014543404012698167, 25, 3, 135, 1, 0]\n",
      "05/15/2024 08:13:18,562 - [BO (140569664263600).INFO] -- ask takes 1.1135s\n",
      "lr 0.0081, beta_1 0.9366, beta_2 0.994, l2 0.0145, filters 25, kernel_sz 3,  dense_sz 135, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.123s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.589, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 3.902s.  Loss: tr - 0.66, val - 0.655.  Accuracy: tr - 0.589, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 3.877s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.588, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 3.835s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.588, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 3.893s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.585, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 4.119s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.584, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 3.917s.  Loss: tr - 0.66, val - 0.661.  Accuracy: tr - 0.586, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 3.864s.  Loss: tr - 0.66, val - 0.665.  Accuracy: tr - 0.589, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 3.89s.  Loss: tr - 0.658, val - 0.657.  Accuracy: tr - 0.592, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 3.863s.  Loss: tr - 0.657, val - 0.66.  Accuracy: tr - 0.593, val - 0.589.\n",
      "Cross-validation completed in 39.287s. Mean validation loss 0.657 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:13:57,853 - [BO (140569664263600).INFO] -- evaluate takes 39.2899s\n",
      "05/15/2024 08:13:57,853 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:13:57,853 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6565837860107422, solution: [0.008137025350148376, 0.9366392545500491, 0.9940143126044647, 0.014543404012698167, 25, 3, 135, 1, 0]\n",
      "05/15/2024 08:13:57,871 - [BO (140569664263600).INFO] -- model r2: 0.5591360524992695, MAPE: 0.7404337483758303\n",
      "05/15/2024 08:13:57,872 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:13:57,872 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:13:57,872 - [BO (140569664263600).INFO] -- tell takes 0.0195s\n",
      "05/15/2024 08:13:57,873 - [BO (140569664263600).INFO] -- iteration 43 starts...\n",
      "05/15/2024 08:13:59,325 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.4519s\n",
      "05/15/2024 08:13:59,326 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:13:59,326 - [BO (140569664263600).INFO] -- #1 - [0.0012904797382561855, 0.8868664644304891, 0.995106805756306, 0.019782790961011223, 46, 4, 141, 0, 1]\n",
      "05/15/2024 08:13:59,326 - [BO (140569664263600).INFO] -- ask takes 1.4533s\n",
      "lr 0.0013, beta_1 0.8869, beta_2 0.9951, l2 0.0198, filters 46, kernel_sz 4,  dense_sz 141, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.171s.  Loss: tr - 0.128, val - 0.091.  Accuracy: tr - 0.952, val - 0.966.\n",
      "Fold 1 trained 10 epochs in 7.511s.  Loss: tr - 0.029, val - 0.024.  Accuracy: tr - 0.991, val - 0.993.\n",
      "Fold 2 trained 10 epochs in 7.143s.  Loss: tr - 0.036, val - 0.048.  Accuracy: tr - 0.988, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 7.174s.  Loss: tr - 0.041, val - 0.033.  Accuracy: tr - 0.989, val - 0.989.\n",
      "Fold 4 trained 10 epochs in 7.132s.  Loss: tr - 0.047, val - 0.03.  Accuracy: tr - 0.984, val - 0.99.\n",
      "Fold 5 trained 10 epochs in 7.177s.  Loss: tr - 0.099, val - 0.071.  Accuracy: tr - 0.964, val - 0.976.\n",
      "Fold 6 trained 10 epochs in 7.421s.  Loss: tr - 0.047, val - 0.048.  Accuracy: tr - 0.983, val - 0.984.\n",
      "Fold 7 trained 10 epochs in 7.18s.  Loss: tr - 0.064, val - 0.056.  Accuracy: tr - 0.981, val - 0.982.\n",
      "Fold 8 trained 10 epochs in 7.148s.  Loss: tr - 0.064, val - 0.048.  Accuracy: tr - 0.976, val - 0.984.\n",
      "Fold 9 trained 10 epochs in 7.143s.  Loss: tr - 0.059, val - 0.06.  Accuracy: tr - 0.979, val - 0.979.\n",
      "Cross-validation completed in 72.203s. Mean validation loss 0.051 and acc 0.983\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:15:11,533 - [BO (140569664263600).INFO] -- evaluate takes 72.2063s\n",
      "05/15/2024 08:15:11,533 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:15:11,534 - [BO (140569664263600).INFO] -- #1 - fitness: 0.05084989406168461, solution: [0.0012904797382561855, 0.8868664644304891, 0.995106805756306, 0.019782790961011223, 46, 4, 141, 0, 1]\n",
      "05/15/2024 08:15:11,553 - [BO (140569664263600).INFO] -- model r2: 0.7229042751571033, MAPE: 0.5646209004643266\n",
      "05/15/2024 08:15:11,554 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:15:11,554 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:15:11,555 - [BO (140569664263600).INFO] -- tell takes 0.0213s\n",
      "05/15/2024 08:15:11,555 - [BO (140569664263600).INFO] -- iteration 44 starts...\n",
      "05/15/2024 08:15:12,573 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.0177s\n",
      "05/15/2024 08:15:12,574 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:15:12,574 - [BO (140569664263600).INFO] -- #1 - [0.0012427835150379315, 0.8567238557278255, 0.9943358773784852, 0.019123281449137008, 30, 3, 143, 0, 0]\n",
      "05/15/2024 08:15:12,574 - [BO (140569664263600).INFO] -- ask takes 1.0193s\n",
      "lr 0.0012, beta_1 0.8567, beta_2 0.9943, l2 0.0191, filters 30, kernel_sz 3,  dense_sz 143, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.259s.  Loss: tr - 0.047, val - 0.036.  Accuracy: tr - 0.984, val - 0.99.\n",
      "Fold 1 trained 10 epochs in 4.5s.  Loss: tr - 0.038, val - 0.034.  Accuracy: tr - 0.988, val - 0.99.\n",
      "Fold 2 trained 10 epochs in 4.229s.  Loss: tr - 0.032, val - 0.036.  Accuracy: tr - 0.99, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 4.198s.  Loss: tr - 0.032, val - 0.036.  Accuracy: tr - 0.991, val - 0.99.\n",
      "Fold 4 trained 10 epochs in 4.229s.  Loss: tr - 0.038, val - 0.031.  Accuracy: tr - 0.988, val - 0.991.\n",
      "Fold 5 trained 10 epochs in 4.269s.  Loss: tr - 0.038, val - 0.046.  Accuracy: tr - 0.989, val - 0.984.\n",
      "Fold 6 trained 10 epochs in 4.495s.  Loss: tr - 0.035, val - 0.042.  Accuracy: tr - 0.989, val - 0.986.\n",
      "Fold 7 trained 10 epochs in 4.231s.  Loss: tr - 0.028, val - 0.029.  Accuracy: tr - 0.992, val - 0.99.\n",
      "Fold 8 trained 10 epochs in 4.229s.  Loss: tr - 0.036, val - 0.035.  Accuracy: tr - 0.989, val - 0.989.\n",
      "Fold 9 trained 10 epochs in 4.268s.  Loss: tr - 0.042, val - 0.038.  Accuracy: tr - 0.987, val - 0.985.\n",
      "Cross-validation completed in 42.911s. Mean validation loss 0.036 and acc 0.988\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:15:55,490 - [BO (140569664263600).INFO] -- evaluate takes 42.9147s\n",
      "05/15/2024 08:15:55,490 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:15:55,490 - [BO (140569664263600).INFO] -- #1 - fitness: 0.03629339225590229, solution: [0.0012427835150379315, 0.8567238557278255, 0.9943358773784852, 0.019123281449137008, 30, 3, 143, 0, 0]\n",
      "05/15/2024 08:15:55,508 - [BO (140569664263600).INFO] -- model r2: 0.6351328590222189, MAPE: 0.5748269905002009\n",
      "05/15/2024 08:15:55,509 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:15:55,509 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:15:55,510 - [BO (140569664263600).INFO] -- tell takes 0.0196s\n",
      "05/15/2024 08:15:55,510 - [BO (140569664263600).INFO] -- iteration 45 starts...\n",
      "05/15/2024 08:15:56,780 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.2699s\n",
      "05/15/2024 08:15:56,781 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:15:56,781 - [BO (140569664263600).INFO] -- #1 - [0.003809779894587325, 0.876270336217467, 0.9906390356086001, 0.018386878555373996, 31, 3, 145, 0, 0]\n",
      "05/15/2024 08:15:56,782 - [BO (140569664263600).INFO] -- ask takes 1.2715s\n",
      "lr 0.0038, beta_1 0.8763, beta_2 0.9906, l2 0.0184, filters 31, kernel_sz 3,  dense_sz 145, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.313s.  Loss: tr - 0.036, val - 0.026.  Accuracy: tr - 0.99, val - 0.992.\n",
      "Fold 1 trained 10 epochs in 4.588s.  Loss: tr - 0.07, val - 0.051.  Accuracy: tr - 0.977, val - 0.983.\n",
      "Fold 2 trained 10 epochs in 4.252s.  Loss: tr - 0.05, val - 0.049.  Accuracy: tr - 0.984, val - 0.984.\n",
      "Fold 3 trained 10 epochs in 4.283s.  Loss: tr - 0.153, val - 0.128.  Accuracy: tr - 0.946, val - 0.953.\n",
      "Fold 4 trained 10 epochs in 4.307s.  Loss: tr - 0.055, val - 0.053.  Accuracy: tr - 0.98, val - 0.978.\n",
      "Fold 5 trained 10 epochs in 4.299s.  Loss: tr - 0.046, val - 0.047.  Accuracy: tr - 0.986, val - 0.985.\n",
      "Fold 6 trained 10 epochs in 4.582s.  Loss: tr - 0.053, val - 0.053.  Accuracy: tr - 0.98, val - 0.98.\n",
      "Fold 7 trained 10 epochs in 4.302s.  Loss: tr - 0.042, val - 0.045.  Accuracy: tr - 0.988, val - 0.986.\n",
      "Fold 8 trained 10 epochs in 4.315s.  Loss: tr - 0.133, val - 0.09.  Accuracy: tr - 0.953, val - 0.967.\n",
      "Fold 9 trained 10 epochs in 4.284s.  Loss: tr - 0.065, val - 0.052.  Accuracy: tr - 0.98, val - 0.983.\n",
      "Cross-validation completed in 43.529s. Mean validation loss 0.059 and acc 0.979\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:16:40,314 - [BO (140569664263600).INFO] -- evaluate takes 43.5317s\n",
      "05/15/2024 08:16:40,314 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:16:40,314 - [BO (140569664263600).INFO] -- #1 - fitness: 0.05940248500555754, solution: [0.003809779894587325, 0.876270336217467, 0.9906390356086001, 0.018386878555373996, 31, 3, 145, 0, 0]\n",
      "05/15/2024 08:16:40,333 - [BO (140569664263600).INFO] -- model r2: 0.6014050328459807, MAPE: 0.5524332762662829\n",
      "05/15/2024 08:16:40,333 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:16:40,333 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:16:40,334 - [BO (140569664263600).INFO] -- tell takes 0.0197s\n",
      "05/15/2024 08:16:40,334 - [BO (140569664263600).INFO] -- iteration 46 starts...\n",
      "05/15/2024 08:16:43,676 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 3.3417s\n",
      "05/15/2024 08:16:43,677 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:16:43,677 - [BO (140569664263600).INFO] -- #1 - [0.006385527813484784, 0.8870166448650882, 0.9969992132757085, 0.017974402742976417, 54, 2, 130, 1, 0]\n",
      "05/15/2024 08:16:43,677 - [BO (140569664263600).INFO] -- ask takes 3.3433s\n",
      "lr 0.0064, beta_1 0.887, beta_2 0.997, l2 0.018, filters 54, kernel_sz 2,  dense_sz 130, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.711s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.587, val - 0.586.\n",
      "Fold 1 trained 10 epochs in 5.055s.  Loss: tr - 0.662, val - 0.656.  Accuracy: tr - 0.582, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 4.735s.  Loss: tr - 0.663, val - 0.655.  Accuracy: tr - 0.586, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 4.698s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 4.711s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 4.789s.  Loss: tr - 0.66, val - 0.653.  Accuracy: tr - 0.587, val - 0.584.\n",
      "Fold 6 trained 10 epochs in 4.753s.  Loss: tr - 0.664, val - 0.661.  Accuracy: tr - 0.587, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.005s.  Loss: tr - 0.67, val - 0.664.  Accuracy: tr - 0.578, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 4.735s.  Loss: tr - 0.657, val - 0.657.  Accuracy: tr - 0.593, val - 0.589.\n",
      "Fold 9 trained 10 epochs in 4.747s.  Loss: tr - 0.66, val - 0.659.  Accuracy: tr - 0.594, val - 0.592.\n",
      "Cross-validation completed in 47.943s. Mean validation loss 0.657 and acc 0.591\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:17:31,624 - [BO (140569664263600).INFO] -- evaluate takes 47.9460s\n",
      "05/15/2024 08:17:31,624 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:17:31,624 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6567538619041443, solution: [0.006385527813484784, 0.8870166448650882, 0.9969992132757085, 0.017974402742976417, 54, 2, 130, 1, 0]\n",
      "05/15/2024 08:17:31,643 - [BO (140569664263600).INFO] -- model r2: 0.6690722019691686, MAPE: 0.6098805204816133\n",
      "05/15/2024 08:17:31,643 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:17:31,643 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:17:31,644 - [BO (140569664263600).INFO] -- tell takes 0.0197s\n",
      "05/15/2024 08:17:31,644 - [BO (140569664263600).INFO] -- iteration 47 starts...\n",
      "05/15/2024 08:17:33,188 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5436s\n",
      "05/15/2024 08:17:33,189 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:17:33,189 - [BO (140569664263600).INFO] -- #1 - [0.0023144276582737345, 0.8638718528204405, 0.995236360069945, 0.018245630225982204, 49, 4, 146, 0, 1]\n",
      "05/15/2024 08:17:33,189 - [BO (140569664263600).INFO] -- ask takes 1.5451s\n",
      "lr 0.0023, beta_1 0.8639, beta_2 0.9952, l2 0.0182, filters 49, kernel_sz 4,  dense_sz 146, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.835s.  Loss: tr - 0.379, val - 0.272.  Accuracy: tr - 0.872, val - 0.884.\n",
      "Fold 1 trained 10 epochs in 7.864s.  Loss: tr - 0.655, val - 0.651.  Accuracy: tr - 0.593, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 7.828s.  Loss: tr - 0.305, val - 0.287.  Accuracy: tr - 0.883, val - 0.858.\n",
      "Fold 3 trained 10 epochs in 8.112s.  Loss: tr - 0.052, val - 0.045.  Accuracy: tr - 0.982, val - 0.985.\n",
      "Fold 4 trained 10 epochs in 7.823s.  Loss: tr - 0.107, val - 0.063.  Accuracy: tr - 0.962, val - 0.982.\n",
      "Fold 5 trained 10 epochs in 7.853s.  Loss: tr - 0.044, val - 0.04.  Accuracy: tr - 0.987, val - 0.987.\n",
      "Fold 6 trained 10 epochs in 7.797s.  Loss: tr - 0.111, val - 0.083.  Accuracy: tr - 0.961, val - 0.971.\n",
      "Fold 7 trained 10 epochs in 7.878s.  Loss: tr - 0.594, val - 0.525.  Accuracy: tr - 0.701, val - 0.735.\n",
      "Fold 8 trained 10 epochs in 8.071s.  Loss: tr - 0.609, val - 0.619.  Accuracy: tr - 0.685, val - 0.673.\n",
      "Fold 9 trained 10 epochs in 7.87s.  Loss: tr - 0.424, val - 0.266.  Accuracy: tr - 0.821, val - 0.883.\n",
      "Cross-validation completed in 78.935s. Mean validation loss 0.285 and acc 0.855\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:18:52,127 - [BO (140569664263600).INFO] -- evaluate takes 78.9378s\n",
      "05/15/2024 08:18:52,128 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:18:52,128 - [BO (140569664263600).INFO] -- #1 - fitness: 0.285105062276125, solution: [0.0023144276582737345, 0.8638718528204405, 0.995236360069945, 0.018245630225982204, 49, 4, 146, 0, 1]\n",
      "05/15/2024 08:18:52,146 - [BO (140569664263600).INFO] -- model r2: 0.7456878742041717, MAPE: 0.5194376123050617\n",
      "05/15/2024 08:18:52,147 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:18:52,147 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:18:52,147 - [BO (140569664263600).INFO] -- tell takes 0.0194s\n",
      "05/15/2024 08:18:52,147 - [BO (140569664263600).INFO] -- iteration 48 starts...\n",
      "05/15/2024 08:18:53,414 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.2660s\n",
      "05/15/2024 08:18:53,414 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:18:53,415 - [BO (140569664263600).INFO] -- #1 - [0.006055824281027253, 0.889041205452603, 0.9995637882718053, 0.01923971506094344, 45, 4, 129, 1, 0]\n",
      "05/15/2024 08:18:53,415 - [BO (140569664263600).INFO] -- ask takes 1.2674s\n",
      "lr 0.0061, beta_1 0.889, beta_2 0.9996, l2 0.0192, filters 45, kernel_sz 4,  dense_sz 129, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 6.222s.  Loss: tr - 0.663, val - 0.656.  Accuracy: tr - 0.584, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 6.209s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 6.233s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 6.245s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.585, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 6.535s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 6.249s.  Loss: tr - 0.662, val - 0.651.  Accuracy: tr - 0.584, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 6.21s.  Loss: tr - 0.656, val - 0.661.  Accuracy: tr - 0.593, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 6.225s.  Loss: tr - 0.661, val - 0.665.  Accuracy: tr - 0.588, val - 0.587.\n",
      "Fold 8 trained 10 epochs in 6.223s.  Loss: tr - 0.658, val - 0.658.  Accuracy: tr - 0.589, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 6.573s.  Loss: tr - 0.661, val - 0.66.  Accuracy: tr - 0.59, val - 0.592.\n",
      "Cross-validation completed in 62.93s. Mean validation loss 0.656 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:19:56,348 - [BO (140569664263600).INFO] -- evaluate takes 62.9330s\n",
      "05/15/2024 08:19:56,349 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:19:56,349 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6564513266086578, solution: [0.006055824281027253, 0.889041205452603, 0.9995637882718053, 0.01923971506094344, 45, 4, 129, 1, 0]\n",
      "05/15/2024 08:19:56,367 - [BO (140569664263600).INFO] -- model r2: 0.6888679044853641, MAPE: 0.4631453233425573\n",
      "05/15/2024 08:19:56,368 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:19:56,368 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:19:56,368 - [BO (140569664263600).INFO] -- tell takes 0.0199s\n",
      "05/15/2024 08:19:56,369 - [BO (140569664263600).INFO] -- iteration 49 starts...\n",
      "05/15/2024 08:19:58,450 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.0805s\n",
      "05/15/2024 08:19:58,450 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:19:58,451 - [BO (140569664263600).INFO] -- #1 - [0.0012147631822608724, 0.8988650398094036, 0.9924162820479121, 0.012043902414719476, 61, 3, 144, 0, 0]\n",
      "05/15/2024 08:19:58,451 - [BO (140569664263600).INFO] -- ask takes 2.0820s\n",
      "lr 0.0012, beta_1 0.8989, beta_2 0.9924, l2 0.012, filters 61, kernel_sz 3,  dense_sz 144, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.698s.  Loss: tr - 0.039, val - 0.037.  Accuracy: tr - 0.987, val - 0.985.\n",
      "Fold 1 trained 10 epochs in 5.693s.  Loss: tr - 0.045, val - 0.03.  Accuracy: tr - 0.987, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 5.722s.  Loss: tr - 0.035, val - 0.04.  Accuracy: tr - 0.988, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 5.747s.  Loss: tr - 0.054, val - 0.057.  Accuracy: tr - 0.982, val - 0.981.\n",
      "Fold 4 trained 10 epochs in 6.032s.  Loss: tr - 0.057, val - 0.051.  Accuracy: tr - 0.979, val - 0.979.\n",
      "Fold 5 trained 10 epochs in 5.703s.  Loss: tr - 0.099, val - 0.087.  Accuracy: tr - 0.963, val - 0.968.\n",
      "Fold 6 trained 10 epochs in 5.684s.  Loss: tr - 0.074, val - 0.068.  Accuracy: tr - 0.975, val - 0.976.\n",
      "Fold 7 trained 10 epochs in 5.705s.  Loss: tr - 0.034, val - 0.036.  Accuracy: tr - 0.989, val - 0.986.\n",
      "Fold 8 trained 10 epochs in 5.695s.  Loss: tr - 0.04, val - 0.039.  Accuracy: tr - 0.986, val - 0.988.\n",
      "Fold 9 trained 10 epochs in 5.965s.  Loss: tr - 0.033, val - 0.026.  Accuracy: tr - 0.99, val - 0.991.\n",
      "Cross-validation completed in 57.648s. Mean validation loss 0.047 and acc 0.983\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:20:56,102 - [BO (140569664263600).INFO] -- evaluate takes 57.6506s\n",
      "05/15/2024 08:20:56,102 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:20:56,103 - [BO (140569664263600).INFO] -- #1 - fitness: 0.047027863562107086, solution: [0.0012147631822608724, 0.8988650398094036, 0.9924162820479121, 0.012043902414719476, 61, 3, 144, 0, 0]\n",
      "05/15/2024 08:20:56,121 - [BO (140569664263600).INFO] -- model r2: 0.69125646641481, MAPE: 0.4782018213366727\n",
      "05/15/2024 08:20:56,122 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:20:56,123 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:20:56,123 - [BO (140569664263600).INFO] -- tell takes 0.0207s\n",
      "05/15/2024 08:20:56,123 - [BO (140569664263600).INFO] -- iteration 50 starts...\n",
      "05/15/2024 08:20:57,632 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5084s\n",
      "05/15/2024 08:20:57,633 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:20:57,633 - [BO (140569664263600).INFO] -- #1 - [0.0007039037888762303, 0.8837608928463588, 0.9973658627963181, 0.01166884993455876, 49, 3, 148, 0, 0]\n",
      "05/15/2024 08:20:57,633 - [BO (140569664263600).INFO] -- ask takes 1.5098s\n",
      "lr 0.0007, beta_1 0.8838, beta_2 0.9974, l2 0.0117, filters 49, kernel_sz 3,  dense_sz 148, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.921s.  Loss: tr - 0.044, val - 0.04.  Accuracy: tr - 0.985, val - 0.986.\n",
      "Fold 1 trained 10 epochs in 4.912s.  Loss: tr - 0.039, val - 0.035.  Accuracy: tr - 0.987, val - 0.987.\n",
      "Fold 2 trained 10 epochs in 4.909s.  Loss: tr - 0.052, val - 0.045.  Accuracy: tr - 0.983, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 4.954s.  Loss: tr - 0.034, val - 0.041.  Accuracy: tr - 0.989, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 4.952s.  Loss: tr - 0.047, val - 0.041.  Accuracy: tr - 0.983, val - 0.985.\n",
      "Fold 5 trained 10 epochs in 4.987s.  Loss: tr - 0.033, val - 0.036.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 6 trained 10 epochs in 5.253s.  Loss: tr - 0.039, val - 0.047.  Accuracy: tr - 0.987, val - 0.985.\n",
      "Fold 7 trained 10 epochs in 4.952s.  Loss: tr - 0.044, val - 0.045.  Accuracy: tr - 0.985, val - 0.984.\n",
      "Fold 8 trained 10 epochs in 4.996s.  Loss: tr - 0.044, val - 0.04.  Accuracy: tr - 0.985, val - 0.987.\n",
      "Fold 9 trained 10 epochs in 5.015s.  Loss: tr - 0.044, val - 0.037.  Accuracy: tr - 0.986, val - 0.986.\n",
      "Cross-validation completed in 49.855s. Mean validation loss 0.041 and acc 0.986\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:21:47,492 - [BO (140569664263600).INFO] -- evaluate takes 49.8583s\n",
      "05/15/2024 08:21:47,492 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:21:47,493 - [BO (140569664263600).INFO] -- #1 - fitness: 0.04069724939763546, solution: [0.0007039037888762303, 0.8837608928463588, 0.9973658627963181, 0.01166884993455876, 49, 3, 148, 0, 0]\n",
      "05/15/2024 08:21:47,512 - [BO (140569664263600).INFO] -- model r2: 0.7022029005262007, MAPE: 0.3742994501529964\n",
      "05/15/2024 08:21:47,513 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:21:47,513 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:21:47,513 - [BO (140569664263600).INFO] -- tell takes 0.0213s\n",
      "05/15/2024 08:21:47,514 - [BO (140569664263600).INFO] -- iteration 51 starts...\n",
      "05/15/2024 08:21:48,861 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3469s\n",
      "05/15/2024 08:21:48,862 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:21:48,862 - [BO (140569664263600).INFO] -- #1 - [0.0064745976152663105, 0.9178025548769196, 0.9908387314957097, 0.005393572589181449, 25, 4, 148, 1, 0]\n",
      "05/15/2024 08:21:48,862 - [BO (140569664263600).INFO] -- ask takes 1.3484s\n",
      "lr 0.0065, beta_1 0.9178, beta_2 0.9908, l2 0.0054, filters 25, kernel_sz 4,  dense_sz 148, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.304s.  Loss: tr - 0.658, val - 0.656.  Accuracy: tr - 0.591, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 4.571s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.585, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 4.302s.  Loss: tr - 0.659, val - 0.658.  Accuracy: tr - 0.591, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 4.329s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.59, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 4.306s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.59, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 4.361s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.591, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 4.588s.  Loss: tr - 0.658, val - 0.661.  Accuracy: tr - 0.59, val - 0.577.\n",
      "Fold 7 trained 10 epochs in 4.309s.  Loss: tr - 0.658, val - 0.665.  Accuracy: tr - 0.591, val - 0.587.\n",
      "Fold 8 trained 10 epochs in 4.306s.  Loss: tr - 0.659, val - 0.657.  Accuracy: tr - 0.588, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 4.331s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.589, val - 0.592.\n",
      "Cross-validation completed in 43.711s. Mean validation loss 0.657 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:22:32,577 - [BO (140569664263600).INFO] -- evaluate takes 43.7142s\n",
      "05/15/2024 08:22:32,577 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:22:32,578 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6568916440010071, solution: [0.0064745976152663105, 0.9178025548769196, 0.9908387314957097, 0.005393572589181449, 25, 4, 148, 1, 0]\n",
      "05/15/2024 08:22:32,597 - [BO (140569664263600).INFO] -- model r2: 0.6318481620224695, MAPE: 0.6599720985786102\n",
      "05/15/2024 08:22:32,597 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:22:32,597 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:22:32,598 - [BO (140569664263600).INFO] -- tell takes 0.0204s\n",
      "05/15/2024 08:22:32,598 - [BO (140569664263600).INFO] -- iteration 52 starts...\n",
      "05/15/2024 08:22:34,301 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.7030s\n",
      "05/15/2024 08:22:34,302 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:22:34,302 - [BO (140569664263600).INFO] -- #1 - [0.0006919615394571401, 0.8981148106823024, 0.9960114707659152, 0.012176152557109242, 38, 3, 104, 0, 1]\n",
      "05/15/2024 08:22:34,303 - [BO (140569664263600).INFO] -- ask takes 1.7044s\n",
      "lr 0.0007, beta_1 0.8981, beta_2 0.996, l2 0.0122, filters 38, kernel_sz 3,  dense_sz 104, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.088s.  Loss: tr - 0.058, val - 0.043.  Accuracy: tr - 0.982, val - 0.988.\n",
      "Fold 1 trained 10 epochs in 7.101s.  Loss: tr - 0.045, val - 0.036.  Accuracy: tr - 0.984, val - 0.989.\n",
      "Fold 2 trained 10 epochs in 7.355s.  Loss: tr - 0.053, val - 0.048.  Accuracy: tr - 0.982, val - 0.987.\n",
      "Fold 3 trained 10 epochs in 7.124s.  Loss: tr - 0.064, val - 0.056.  Accuracy: tr - 0.978, val - 0.983.\n",
      "Fold 4 trained 10 epochs in 7.106s.  Loss: tr - 0.035, val - 0.032.  Accuracy: tr - 0.989, val - 0.992.\n",
      "Fold 5 trained 10 epochs in 7.144s.  Loss: tr - 0.035, val - 0.043.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 6 trained 10 epochs in 7.074s.  Loss: tr - 0.026, val - 0.029.  Accuracy: tr - 0.992, val - 0.993.\n",
      "Fold 7 trained 10 epochs in 7.102s.  Loss: tr - 0.061, val - 0.053.  Accuracy: tr - 0.982, val - 0.982.\n",
      "Fold 8 trained 10 epochs in 7.361s.  Loss: tr - 0.078, val - 0.066.  Accuracy: tr - 0.974, val - 0.976.\n",
      "Fold 9 trained 10 epochs in 7.138s.  Loss: tr - 0.031, val - 0.03.  Accuracy: tr - 0.99, val - 0.991.\n",
      "Cross-validation completed in 71.597s. Mean validation loss 0.043 and acc 0.987\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:23:45,903 - [BO (140569664263600).INFO] -- evaluate takes 71.6004s\n",
      "05/15/2024 08:23:45,904 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:23:45,904 - [BO (140569664263600).INFO] -- #1 - fitness: 0.043414989672601226, solution: [0.0006919615394571401, 0.8981148106823024, 0.9960114707659152, 0.012176152557109242, 38, 3, 104, 0, 1]\n",
      "05/15/2024 08:23:45,924 - [BO (140569664263600).INFO] -- model r2: 0.6439477943756295, MAPE: 0.5566794427454105\n",
      "05/15/2024 08:23:45,924 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:23:45,924 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:23:45,925 - [BO (140569664263600).INFO] -- tell takes 0.0209s\n",
      "05/15/2024 08:23:45,925 - [BO (140569664263600).INFO] -- iteration 53 starts...\n",
      "05/15/2024 08:23:47,184 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.2591s\n",
      "05/15/2024 08:23:47,185 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:23:47,185 - [BO (140569664263600).INFO] -- #1 - [0.0061259326658433125, 0.8729078008899944, 0.993885627691714, 0.016170495192843904, 36, 4, 129, 0, 0]\n",
      "05/15/2024 08:23:47,186 - [BO (140569664263600).INFO] -- ask takes 1.2606s\n",
      "lr 0.0061, beta_1 0.8729, beta_2 0.9939, l2 0.0162, filters 36, kernel_sz 4,  dense_sz 129, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.324s.  Loss: tr - 0.674, val - 0.66.  Accuracy: tr - 0.588, val - 0.589.\n",
      "Fold 1 trained 10 epochs in 5.279s.  Loss: tr - 0.326, val - 0.342.  Accuracy: tr - 0.857, val - 0.858.\n",
      "Fold 2 trained 10 epochs in 5.299s.  Loss: tr - 0.659, val - 0.654.  Accuracy: tr - 0.579, val - 0.591.\n",
      "Fold 3 trained 6 epochs in 3.208s.  Loss: tr - 34.357, val - 39.832.  Accuracy: tr - 0.559, val - 0.602.\n",
      "Fold 4 trained 10 epochs in 5.608s.  Loss: tr - 0.645, val - 0.631.  Accuracy: tr - 0.662, val - 0.656.\n",
      "Fold 5 trained 10 epochs in 5.273s.  Loss: tr - 0.666, val - 0.651.  Accuracy: tr - 0.58, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 5.305s.  Loss: tr - 0.666, val - 0.661.  Accuracy: tr - 0.585, val - 0.584.\n",
      "Fold 7 trained 10 epochs in 5.36s.  Loss: tr - 0.658, val - 0.664.  Accuracy: tr - 0.594, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.271s.  Loss: tr - 0.673, val - 0.669.  Accuracy: tr - 0.576, val - 0.572.\n",
      "Fold 9 trained 10 epochs in 5.532s.  Loss: tr - 0.659, val - 0.662.  Accuracy: tr - 0.589, val - 0.592.\n",
      "Cross-validation completed in 51.463s. Mean validation loss 4.543 and acc 0.623\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:24:38,652 - [BO (140569664263600).INFO] -- evaluate takes 51.4661s\n",
      "05/15/2024 08:24:38,652 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:24:38,653 - [BO (140569664263600).INFO] -- #1 - fitness: 4.542560383677483, solution: [0.0061259326658433125, 0.8729078008899944, 0.993885627691714, 0.016170495192843904, 36, 4, 129, 0, 0]\n",
      "05/15/2024 08:24:38,672 - [BO (140569664263600).INFO] -- model r2: 0.7020093554032496, MAPE: 0.5651509820909557\n",
      "05/15/2024 08:24:38,672 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:24:38,672 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:24:38,673 - [BO (140569664263600).INFO] -- tell takes 0.0202s\n",
      "05/15/2024 08:24:38,673 - [BO (140569664263600).INFO] -- iteration 54 starts...\n",
      "05/15/2024 08:24:40,120 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.4465s\n",
      "05/15/2024 08:24:40,120 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:24:40,121 - [BO (140569664263600).INFO] -- #1 - [0.0038156980362429958, 0.8817008122246026, 0.9950631871179835, 0.005336064247438571, 41, 4, 148, 0, 1]\n",
      "05/15/2024 08:24:40,121 - [BO (140569664263600).INFO] -- ask takes 1.4479s\n",
      "lr 0.0038, beta_1 0.8817, beta_2 0.9951, l2 0.0053, filters 41, kernel_sz 4,  dense_sz 148, activs relu, padding 1\n",
      "Fold 0 trained 6 epochs in 4.106s.  Loss: tr - 39.648, val - 48.448.  Accuracy: tr - 0.533, val - 0.516.\n",
      "Fold 1 trained 10 epochs in 6.852s.  Loss: tr - 0.656, val - 0.653.  Accuracy: tr - 0.595, val - 0.594.\n",
      "Fold 2 trained 6 epochs in 4.127s.  Loss: tr - 33.931, val - 41.794.  Accuracy: tr - 0.563, val - 0.582.\n",
      "Fold 3 trained 10 epochs in 6.834s.  Loss: tr - 0.052, val - 0.041.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 6.856s.  Loss: tr - 0.538, val - 0.507.  Accuracy: tr - 0.729, val - 0.735.\n",
      "Fold 5 trained 10 epochs in 6.858s.  Loss: tr - 0.064, val - 0.06.  Accuracy: tr - 0.977, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 7.158s.  Loss: tr - 0.115, val - 0.077.  Accuracy: tr - 0.955, val - 0.971.\n",
      "Fold 7 trained 10 epochs in 6.86s.  Loss: tr - 0.226, val - 0.148.  Accuracy: tr - 0.917, val - 0.946.\n",
      "Fold 8 trained 10 epochs in 6.935s.  Loss: tr - 0.613, val - 0.559.  Accuracy: tr - 0.702, val - 0.779.\n",
      "Fold 9 trained 10 epochs in 6.833s.  Loss: tr - 0.458, val - 0.338.  Accuracy: tr - 0.799, val - 0.821.\n",
      "Cross-validation completed in 63.423s. Mean validation loss 9.262 and acc 0.791\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:25:43,548 - [BO (140569664263600).INFO] -- evaluate takes 63.4264s\n",
      "05/15/2024 08:25:43,548 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:25:43,548 - [BO (140569664263600).INFO] -- #1 - fitness: 9.262363623827696, solution: [0.0038156980362429958, 0.8817008122246026, 0.9950631871179835, 0.005336064247438571, 41, 4, 148, 0, 1]\n",
      "05/15/2024 08:25:43,566 - [BO (140569664263600).INFO] -- model r2: 0.6825449806355237, MAPE: 0.4996754086580382\n",
      "05/15/2024 08:25:43,567 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:25:43,567 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:25:43,568 - [BO (140569664263600).INFO] -- tell takes 0.0197s\n",
      "05/15/2024 08:25:43,568 - [BO (140569664263600).INFO] -- iteration 55 starts...\n",
      "05/15/2024 08:25:44,963 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3949s\n",
      "05/15/2024 08:25:44,964 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:25:44,964 - [BO (140569664263600).INFO] -- #1 - [0.0066915293104884655, 0.9023334551447058, 0.9979894959670965, 0.019760245662012184, 41, 3, 117, 1, 0]\n",
      "05/15/2024 08:25:44,965 - [BO (140569664263600).INFO] -- ask takes 1.3964s\n",
      "lr 0.0067, beta_1 0.9023, beta_2 0.998, l2 0.0198, filters 41, kernel_sz 3,  dense_sz 117, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.263s.  Loss: tr - 0.662, val - 0.656.  Accuracy: tr - 0.588, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 4.324s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 4.536s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 4.266s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.59, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 4.283s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 4.302s.  Loss: tr - 0.662, val - 0.651.  Accuracy: tr - 0.583, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 4.31s.  Loss: tr - 0.66, val - 0.662.  Accuracy: tr - 0.588, val - 0.577.\n",
      "Fold 7 trained 10 epochs in 4.554s.  Loss: tr - 0.658, val - 0.664.  Accuracy: tr - 0.589, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 4.352s.  Loss: tr - 0.658, val - 0.657.  Accuracy: tr - 0.593, val - 0.589.\n",
      "Fold 9 trained 10 epochs in 4.281s.  Loss: tr - 0.657, val - 0.66.  Accuracy: tr - 0.591, val - 0.589.\n",
      "Cross-validation completed in 43.475s. Mean validation loss 0.656 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:26:28,443 - [BO (140569664263600).INFO] -- evaluate takes 43.4784s\n",
      "05/15/2024 08:26:28,444 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:26:28,444 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6563499987125396, solution: [0.0066915293104884655, 0.9023334551447058, 0.9979894959670965, 0.019760245662012184, 41, 3, 117, 1, 0]\n",
      "05/15/2024 08:26:28,463 - [BO (140569664263600).INFO] -- model r2: 0.7103013477547796, MAPE: 0.5442241726257501\n",
      "05/15/2024 08:26:28,464 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:26:28,464 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:26:28,465 - [BO (140569664263600).INFO] -- tell takes 0.0208s\n",
      "05/15/2024 08:26:28,465 - [BO (140569664263600).INFO] -- iteration 56 starts...\n",
      "05/15/2024 08:26:30,242 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.7769s\n",
      "05/15/2024 08:26:30,243 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:26:30,243 - [BO (140569664263600).INFO] -- #1 - [0.006456564437920632, 0.8934398925658216, 0.9964126314972006, 0.009625703403194963, 59, 4, 76, 1, 0]\n",
      "05/15/2024 08:26:30,243 - [BO (140569664263600).INFO] -- ask takes 1.7784s\n",
      "lr 0.0065, beta_1 0.8934, beta_2 0.9964, l2 0.0096, filters 59, kernel_sz 4,  dense_sz 76, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 7.479s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.592, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 7.451s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 7.727s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.592, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 7.489s.  Loss: tr - 0.663, val - 0.65.  Accuracy: tr - 0.579, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 7.453s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.591, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 7.536s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.592, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 7.457s.  Loss: tr - 0.657, val - 0.662.  Accuracy: tr - 0.592, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 7.772s.  Loss: tr - 0.656, val - 0.664.  Accuracy: tr - 0.592, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 7.458s.  Loss: tr - 0.659, val - 0.657.  Accuracy: tr - 0.584, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 7.482s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.586, val - 0.589.\n",
      "Cross-validation completed in 75.309s. Mean validation loss 0.656 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:27:45,556 - [BO (140569664263600).INFO] -- evaluate takes 75.3125s\n",
      "05/15/2024 08:27:45,557 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:27:45,557 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6564508497714996, solution: [0.006456564437920632, 0.8934398925658216, 0.9964126314972006, 0.009625703403194963, 59, 4, 76, 1, 0]\n",
      "05/15/2024 08:27:45,576 - [BO (140569664263600).INFO] -- model r2: 0.7451033273053254, MAPE: 0.4411218291338425\n",
      "05/15/2024 08:27:45,576 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:27:45,577 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:27:45,577 - [BO (140569664263600).INFO] -- tell takes 0.0202s\n",
      "05/15/2024 08:27:45,577 - [BO (140569664263600).INFO] -- iteration 57 starts...\n",
      "05/15/2024 08:27:47,31 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.4534s\n",
      "05/15/2024 08:27:47,31 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:27:47,32 - [BO (140569664263600).INFO] -- #1 - [0.007834675839636235, 0.883455770199661, 0.9989203764222493, 0.009770140972223599, 29, 3, 137, 1, 0]\n",
      "05/15/2024 08:27:47,32 - [BO (140569664263600).INFO] -- ask takes 1.4549s\n",
      "lr 0.0078, beta_1 0.8835, beta_2 0.9989, l2 0.0098, filters 29, kernel_sz 3,  dense_sz 137, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 2.696s.  Loss: tr - 0.66, val - 0.657.  Accuracy: tr - 0.586, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 2.714s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.586, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 2.699s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.583, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 2.671s.  Loss: tr - 0.662, val - 0.651.  Accuracy: tr - 0.581, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 2.958s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.591, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 2.725s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.588, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 2.681s.  Loss: tr - 0.658, val - 0.662.  Accuracy: tr - 0.59, val - 0.57.\n",
      "Fold 7 trained 10 epochs in 2.651s.  Loss: tr - 0.658, val - 0.667.  Accuracy: tr - 0.594, val - 0.574.\n",
      "Fold 8 trained 10 epochs in 2.655s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.584, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 2.961s.  Loss: tr - 0.659, val - 0.66.  Accuracy: tr - 0.587, val - 0.589.\n",
      "Cross-validation completed in 27.414s. Mean validation loss 0.657 and acc 0.591\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:28:14,450 - [BO (140569664263600).INFO] -- evaluate takes 27.4175s\n",
      "05/15/2024 08:28:14,450 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:28:14,451 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6569593131542206, solution: [0.007834675839636235, 0.883455770199661, 0.9989203764222493, 0.009770140972223599, 29, 3, 137, 1, 0]\n",
      "05/15/2024 08:28:14,469 - [BO (140569664263600).INFO] -- model r2: 0.7421221255064132, MAPE: 0.3506392250279354\n",
      "05/15/2024 08:28:14,470 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:28:14,470 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:28:14,471 - [BO (140569664263600).INFO] -- tell takes 0.0203s\n",
      "05/15/2024 08:28:14,471 - [BO (140569664263600).INFO] -- iteration 58 starts...\n",
      "05/15/2024 08:28:16,551 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.0797s\n",
      "05/15/2024 08:28:16,552 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:28:16,552 - [BO (140569664263600).INFO] -- #1 - [0.005390368092476057, 0.9064399711237435, 0.9991494542936542, 0.019881124125365215, 46, 3, 145, 1, 1]\n",
      "05/15/2024 08:28:16,552 - [BO (140569664263600).INFO] -- ask takes 2.0811s\n",
      "lr 0.0054, beta_1 0.9064, beta_2 0.9991, l2 0.0199, filters 46, kernel_sz 3,  dense_sz 145, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 5.629s.  Loss: tr - 0.663, val - 0.657.  Accuracy: tr - 0.583, val - 0.588.\n",
      "Fold 1 trained 10 epochs in 5.654s.  Loss: tr - 0.66, val - 0.655.  Accuracy: tr - 0.592, val - 0.583.\n",
      "Fold 2 trained 10 epochs in 5.643s.  Loss: tr - 0.666, val - 0.655.  Accuracy: tr - 0.583, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 5.658s.  Loss: tr - 0.684, val - 0.65.  Accuracy: tr - 0.56, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 5.882s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.586, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 5.616s.  Loss: tr - 0.697, val - 0.651.  Accuracy: tr - 0.569, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 5.626s.  Loss: tr - 0.661, val - 0.661.  Accuracy: tr - 0.586, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.6s.  Loss: tr - 0.662, val - 0.666.  Accuracy: tr - 0.587, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.613s.  Loss: tr - 0.661, val - 0.659.  Accuracy: tr - 0.59, val - 0.589.\n",
      "Fold 9 trained 10 epochs in 5.67s.  Loss: tr - 0.674, val - 0.662.  Accuracy: tr - 0.589, val - 0.592.\n",
      "Cross-validation completed in 56.597s. Mean validation loss 0.657 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:29:13,153 - [BO (140569664263600).INFO] -- evaluate takes 56.6001s\n",
      "05/15/2024 08:29:13,153 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:29:13,153 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6572227358818055, solution: [0.005390368092476057, 0.9064399711237435, 0.9991494542936542, 0.019881124125365215, 46, 3, 145, 1, 1]\n",
      "05/15/2024 08:29:13,172 - [BO (140569664263600).INFO] -- model r2: 0.6833542913284201, MAPE: 0.5467124835242245\n",
      "05/15/2024 08:29:13,173 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:29:13,173 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:29:13,173 - [BO (140569664263600).INFO] -- tell takes 0.0202s\n",
      "05/15/2024 08:29:13,173 - [BO (140569664263600).INFO] -- iteration 59 starts...\n",
      "05/15/2024 08:29:14,207 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.0339s\n",
      "05/15/2024 08:29:14,208 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:29:14,209 - [BO (140569664263600).INFO] -- #1 - [0.006219667567619414, 0.8939590888332878, 0.9964286543124505, 0.012357789247194828, 43, 3, 42, 1, 0]\n",
      "05/15/2024 08:29:14,209 - [BO (140569664263600).INFO] -- ask takes 1.0353s\n",
      "lr 0.0062, beta_1 0.894, beta_2 0.9964, l2 0.0124, filters 43, kernel_sz 3,  dense_sz 42, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 4.424s.  Loss: tr - 0.657, val - 0.656.  Accuracy: tr - 0.589, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 4.693s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 4.388s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 4.424s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.593, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 4.373s.  Loss: tr - 0.663, val - 0.655.  Accuracy: tr - 0.583, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 4.402s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.59, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 4.411s.  Loss: tr - 0.658, val - 0.661.  Accuracy: tr - 0.584, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 4.66s.  Loss: tr - 0.656, val - 0.665.  Accuracy: tr - 0.592, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 4.423s.  Loss: tr - 0.663, val - 0.657.  Accuracy: tr - 0.578, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 4.404s.  Loss: tr - 0.657, val - 0.66.  Accuracy: tr - 0.594, val - 0.592.\n",
      "Cross-validation completed in 44.607s. Mean validation loss 0.656 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:29:58,819 - [BO (140569664263600).INFO] -- evaluate takes 44.6100s\n",
      "05/15/2024 08:29:58,820 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:29:58,820 - [BO (140569664263600).INFO] -- #1 - fitness: 0.656419426202774, solution: [0.006219667567619414, 0.8939590888332878, 0.9964286543124505, 0.012357789247194828, 43, 3, 42, 1, 0]\n",
      "05/15/2024 08:29:58,838 - [BO (140569664263600).INFO] -- model r2: 0.7694883053950563, MAPE: 0.281929434853931\n",
      "05/15/2024 08:29:58,839 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:29:58,839 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:29:58,840 - [BO (140569664263600).INFO] -- tell takes 0.0202s\n",
      "05/15/2024 08:29:58,840 - [BO (140569664263600).INFO] -- iteration 60 starts...\n",
      "05/15/2024 08:30:00,975 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.1346s\n",
      "05/15/2024 08:30:00,976 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:30:00,976 - [BO (140569664263600).INFO] -- #1 - [0.0036172976249026424, 0.9404022473446451, 0.9959752097123892, 0.015134114424121475, 58, 3, 126, 0, 1]\n",
      "05/15/2024 08:30:00,976 - [BO (140569664263600).INFO] -- ask takes 2.1362s\n",
      "lr 0.0036, beta_1 0.9404, beta_2 0.996, l2 0.0151, filters 58, kernel_sz 3,  dense_sz 126, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.803s.  Loss: tr - 0.649, val - 0.641.  Accuracy: tr - 0.606, val - 0.638.\n",
      "Fold 1 trained 10 epochs in 6.83s.  Loss: tr - 0.189, val - 0.159.  Accuracy: tr - 0.931, val - 0.948.\n",
      "Fold 2 trained 7 epochs in 4.81s.  Loss: tr - 41.683, val - 40.92.  Accuracy: tr - 0.583, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 7.032s.  Loss: tr - 0.351, val - 0.272.  Accuracy: tr - 0.853, val - 0.892.\n",
      "Fold 4 trained 10 epochs in 6.802s.  Loss: tr - 0.136, val - 0.073.  Accuracy: tr - 0.949, val - 0.974.\n",
      "Fold 5 trained 6 epochs in 4.124s.  Loss: tr - 33.124, val - 40.316.  Accuracy: tr - 0.552, val - 0.597.\n",
      "Fold 6 trained 6 epochs in 4.148s.  Loss: tr - 35.815, val - 43.629.  Accuracy: tr - 0.548, val - 0.564.\n",
      "Fold 7 trained 10 epochs in 6.772s.  Loss: tr - 0.345, val - 0.264.  Accuracy: tr - 0.837, val - 0.88.\n",
      "Fold 8 trained 10 epochs in 6.767s.  Loss: tr - 0.305, val - 0.166.  Accuracy: tr - 0.878, val - 0.94.\n",
      "Fold 9 trained 6 epochs in 4.107s.  Loss: tr - 35.431, val - 46.687.  Accuracy: tr - 0.528, val - 0.533.\n",
      "Cross-validation completed in 58.198s. Mean validation loss 17.313 and acc 0.756\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:30:59,178 - [BO (140569664263600).INFO] -- evaluate takes 58.2012s\n",
      "05/15/2024 08:30:59,178 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:30:59,179 - [BO (140569664263600).INFO] -- #1 - fitness: 17.31271569505334, solution: [0.0036172976249026424, 0.9404022473446451, 0.9959752097123892, 0.015134114424121475, 58, 3, 126, 0, 1]\n",
      "05/15/2024 08:30:59,197 - [BO (140569664263600).INFO] -- model r2: 0.708871258726508, MAPE: 0.46441262098255803\n",
      "05/15/2024 08:30:59,198 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:30:59,198 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:30:59,199 - [BO (140569664263600).INFO] -- tell takes 0.0205s\n",
      "05/15/2024 08:30:59,199 - [BO (140569664263600).INFO] -- iteration 61 starts...\n",
      "05/15/2024 08:31:02,512 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 3.3120s\n",
      "05/15/2024 08:31:02,513 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:31:02,513 - [BO (140569664263600).INFO] -- #1 - [0.0019914887675607494, 0.9145134232316087, 0.9976498854454625, 0.009371767791520092, 38, 2, 144, 0, 0]\n",
      "05/15/2024 08:31:02,513 - [BO (140569664263600).INFO] -- ask takes 3.3137s\n",
      "lr 0.002, beta_1 0.9145, beta_2 0.9976, l2 0.0094, filters 38, kernel_sz 2,  dense_sz 144, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 3.835s.  Loss: tr - 0.049, val - 0.04.  Accuracy: tr - 0.984, val - 0.987.\n",
      "Fold 1 trained 10 epochs in 3.513s.  Loss: tr - 0.025, val - 0.022.  Accuracy: tr - 0.993, val - 0.994.\n",
      "Fold 2 trained 10 epochs in 3.498s.  Loss: tr - 0.12, val - 0.091.  Accuracy: tr - 0.956, val - 0.968.\n",
      "Fold 3 trained 10 epochs in 3.552s.  Loss: tr - 0.038, val - 0.043.  Accuracy: tr - 0.987, val - 0.982.\n",
      "Fold 4 trained 10 epochs in 3.566s.  Loss: tr - 0.068, val - 0.056.  Accuracy: tr - 0.978, val - 0.98.\n",
      "Fold 5 trained 10 epochs in 3.521s.  Loss: tr - 0.032, val - 0.031.  Accuracy: tr - 0.991, val - 0.991.\n",
      "Fold 6 trained 10 epochs in 3.519s.  Loss: tr - 0.042, val - 0.042.  Accuracy: tr - 0.986, val - 0.984.\n",
      "Fold 7 trained 10 epochs in 3.802s.  Loss: tr - 0.033, val - 0.034.  Accuracy: tr - 0.989, val - 0.988.\n",
      "Fold 8 trained 10 epochs in 3.564s.  Loss: tr - 0.062, val - 0.044.  Accuracy: tr - 0.98, val - 0.986.\n",
      "Fold 9 trained 10 epochs in 3.519s.  Loss: tr - 0.042, val - 0.039.  Accuracy: tr - 0.986, val - 0.988.\n",
      "Cross-validation completed in 35.893s. Mean validation loss 0.044 and acc 0.985\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:31:38,410 - [BO (140569664263600).INFO] -- evaluate takes 35.8963s\n",
      "05/15/2024 08:31:38,411 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:31:38,411 - [BO (140569664263600).INFO] -- #1 - fitness: 0.04425326101481915, solution: [0.0019914887675607494, 0.9145134232316087, 0.9976498854454625, 0.009371767791520092, 38, 2, 144, 0, 0]\n",
      "05/15/2024 08:31:38,432 - [BO (140569664263600).INFO] -- model r2: 0.7356258342016999, MAPE: 0.41929607387476064\n",
      "05/15/2024 08:31:38,433 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:31:38,433 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:31:38,433 - [BO (140569664263600).INFO] -- tell takes 0.0229s\n",
      "05/15/2024 08:31:38,434 - [BO (140569664263600).INFO] -- iteration 62 starts...\n",
      "05/15/2024 08:31:40,988 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.5545s\n",
      "05/15/2024 08:31:40,989 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:31:40,990 - [BO (140569664263600).INFO] -- #1 - [0.0002913775396216266, 0.8847920680788461, 0.9946440321810158, 0.012401097823667091, 36, 2, 148, 0, 1]\n",
      "05/15/2024 08:31:40,990 - [BO (140569664263600).INFO] -- ask takes 2.5560s\n",
      "lr 0.0003, beta_1 0.8848, beta_2 0.9946, l2 0.0124, filters 36, kernel_sz 2,  dense_sz 148, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 4.089s.  Loss: tr - 0.509, val - 0.471.  Accuracy: tr - 0.813, val - 0.847.\n",
      "Fold 1 trained 10 epochs in 4.088s.  Loss: tr - 0.532, val - 0.495.  Accuracy: tr - 0.821, val - 0.866.\n",
      "Fold 2 trained 10 epochs in 4.357s.  Loss: tr - 0.232, val - 0.175.  Accuracy: tr - 0.941, val - 0.954.\n",
      "Fold 3 trained 10 epochs in 4.125s.  Loss: tr - 0.332, val - 0.303.  Accuracy: tr - 0.89, val - 0.901.\n",
      "Fold 4 trained 10 epochs in 4.073s.  Loss: tr - 0.498, val - 0.458.  Accuracy: tr - 0.847, val - 0.859.\n",
      "Fold 5 trained 10 epochs in 4.115s.  Loss: tr - 0.397, val - 0.36.  Accuracy: tr - 0.87, val - 0.877.\n",
      "Fold 6 trained 10 epochs in 4.08s.  Loss: tr - 0.579, val - 0.562.  Accuracy: tr - 0.775, val - 0.837.\n",
      "Fold 7 trained 10 epochs in 4.378s.  Loss: tr - 0.11, val - 0.089.  Accuracy: tr - 0.971, val - 0.977.\n",
      "Fold 8 trained 10 epochs in 4.142s.  Loss: tr - 0.248, val - 0.2.  Accuracy: tr - 0.936, val - 0.949.\n",
      "Fold 9 trained 10 epochs in 4.113s.  Loss: tr - 0.565, val - 0.545.  Accuracy: tr - 0.763, val - 0.769.\n",
      "Cross-validation completed in 41.564s. Mean validation loss 0.366 and acc 0.883\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:32:22,558 - [BO (140569664263600).INFO] -- evaluate takes 41.5674s\n",
      "05/15/2024 08:32:22,558 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:32:22,558 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3657357141375542, solution: [0.0002913775396216266, 0.8847920680788461, 0.9946440321810158, 0.012401097823667091, 36, 2, 148, 0, 1]\n",
      "05/15/2024 08:32:22,577 - [BO (140569664263600).INFO] -- model r2: 0.7125680347903638, MAPE: 0.38178264126713835\n",
      "05/15/2024 08:32:22,578 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:32:22,578 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:32:22,579 - [BO (140569664263600).INFO] -- tell takes 0.0206s\n",
      "05/15/2024 08:32:22,579 - [BO (140569664263600).INFO] -- iteration 63 starts...\n",
      "05/15/2024 08:32:25,295 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.7158s\n",
      "05/15/2024 08:32:25,296 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:32:25,296 - [BO (140569664263600).INFO] -- #1 - [0.0034169930175373085, 0.8664847383370263, 0.9938523620772695, 0.007635291771444681, 53, 4, 150, 0, 0]\n",
      "05/15/2024 08:32:25,296 - [BO (140569664263600).INFO] -- ask takes 2.7173s\n",
      "lr 0.0034, beta_1 0.8665, beta_2 0.9939, l2 0.0076, filters 53, kernel_sz 4,  dense_sz 150, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 7.068s.  Loss: tr - 0.587, val - 0.628.  Accuracy: tr - 0.707, val - 0.675.\n",
      "Fold 1 trained 10 epochs in 7.095s.  Loss: tr - 0.645, val - 0.618.  Accuracy: tr - 0.635, val - 0.653.\n",
      "Fold 2 trained 10 epochs in 7.107s.  Loss: tr - 0.629, val - 0.602.  Accuracy: tr - 0.662, val - 0.664.\n",
      "Fold 3 trained 10 epochs in 7.381s.  Loss: tr - 0.646, val - 0.62.  Accuracy: tr - 0.622, val - 0.669.\n",
      "Fold 4 trained 10 epochs in 7.115s.  Loss: tr - 0.602, val - 0.551.  Accuracy: tr - 0.684, val - 0.723.\n",
      "Fold 5 trained 10 epochs in 7.093s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.594, val - 0.594.\n",
      "Fold 6 trained 6 epochs in 4.291s.  Loss: tr - 33.366, val - 41.641.  Accuracy: tr - 0.579, val - 0.584.\n",
      "Fold 7 trained 10 epochs in 7.134s.  Loss: tr - 0.643, val - 0.65.  Accuracy: tr - 0.636, val - 0.643.\n",
      "Fold 8 trained 10 epochs in 7.14s.  Loss: tr - 44.008, val - 43.96.  Accuracy: tr - 0.537, val - 0.56.\n",
      "Fold 9 trained 10 epochs in 7.379s.  Loss: tr - 0.656, val - 0.661.  Accuracy: tr - 0.595, val - 0.603.\n",
      "Cross-validation completed in 68.806s. Mean validation loss 9.058 and acc 0.637\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:33:34,106 - [BO (140569664263600).INFO] -- evaluate takes 68.8094s\n",
      "05/15/2024 08:33:34,106 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:33:34,107 - [BO (140569664263600).INFO] -- #1 - fitness: 9.058188313245774, solution: [0.0034169930175373085, 0.8664847383370263, 0.9938523620772695, 0.007635291771444681, 53, 4, 150, 0, 0]\n",
      "05/15/2024 08:33:34,127 - [BO (140569664263600).INFO] -- model r2: 0.7327543952557238, MAPE: 0.4732514728626754\n",
      "05/15/2024 08:33:34,128 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:33:34,128 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:33:34,128 - [BO (140569664263600).INFO] -- tell takes 0.0218s\n",
      "05/15/2024 08:33:34,128 - [BO (140569664263600).INFO] -- iteration 64 starts...\n",
      "05/15/2024 08:33:36,704 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.5753s\n",
      "05/15/2024 08:33:36,705 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:33:36,705 - [BO (140569664263600).INFO] -- #1 - [0.0009277915396063292, 0.8855090951631319, 0.9961605945783493, 0.008601849370845035, 56, 2, 145, 0, 1]\n",
      "05/15/2024 08:33:36,705 - [BO (140569664263600).INFO] -- ask takes 2.5767s\n",
      "lr 0.0009, beta_1 0.8855, beta_2 0.9962, l2 0.0086, filters 56, kernel_sz 2,  dense_sz 145, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.042s.  Loss: tr - 0.081, val - 0.065.  Accuracy: tr - 0.974, val - 0.98.\n",
      "Fold 1 trained 10 epochs in 8.085s.  Loss: tr - 0.087, val - 0.071.  Accuracy: tr - 0.973, val - 0.981.\n",
      "Fold 2 trained 10 epochs in 8.179s.  Loss: tr - 0.115, val - 0.086.  Accuracy: tr - 0.96, val - 0.973.\n",
      "Fold 3 trained 10 epochs in 8.126s.  Loss: tr - 0.123, val - 0.098.  Accuracy: tr - 0.967, val - 0.968.\n",
      "Fold 4 trained 10 epochs in 8.192s.  Loss: tr - 0.047, val - 0.037.  Accuracy: tr - 0.986, val - 0.99.\n",
      "Fold 5 trained 10 epochs in 8.457s.  Loss: tr - 0.097, val - 0.081.  Accuracy: tr - 0.967, val - 0.975.\n",
      "Fold 6 trained 10 epochs in 8.152s.  Loss: tr - 0.122, val - 0.098.  Accuracy: tr - 0.958, val - 0.969.\n",
      "Fold 7 trained 10 epochs in 8.175s.  Loss: tr - 0.124, val - 0.091.  Accuracy: tr - 0.959, val - 0.972.\n",
      "Fold 8 trained 10 epochs in 8.134s.  Loss: tr - 0.029, val - 0.027.  Accuracy: tr - 0.992, val - 0.992.\n",
      "Fold 9 trained 10 epochs in 8.18s.  Loss: tr - 0.127, val - 0.102.  Accuracy: tr - 0.956, val - 0.97.\n",
      "Cross-validation completed in 81.725s. Mean validation loss 0.076 and acc 0.977\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:34:58,434 - [BO (140569664263600).INFO] -- evaluate takes 81.7281s\n",
      "05/15/2024 08:34:58,434 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:34:58,435 - [BO (140569664263600).INFO] -- #1 - fitness: 0.07556131742894649, solution: [0.0009277915396063292, 0.8855090951631319, 0.9961605945783493, 0.008601849370845035, 56, 2, 145, 0, 1]\n",
      "05/15/2024 08:34:58,455 - [BO (140569664263600).INFO] -- model r2: 0.7561813103843844, MAPE: 0.48410614880103375\n",
      "05/15/2024 08:34:58,455 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:34:58,455 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:34:58,456 - [BO (140569664263600).INFO] -- tell takes 0.0218s\n",
      "05/15/2024 08:34:58,456 - [BO (140569664263600).INFO] -- iteration 65 starts...\n",
      "05/15/2024 08:34:59,449 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.9923s\n",
      "05/15/2024 08:34:59,450 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:34:59,450 - [BO (140569664263600).INFO] -- #1 - [0.007158871977683808, 0.9301700793815539, 0.9909300776683574, 0.008302362849115497, 51, 2, 129, 0, 0]\n",
      "05/15/2024 08:34:59,450 - [BO (140569664263600).INFO] -- ask takes 0.9937s\n",
      "lr 0.0072, beta_1 0.9302, beta_2 0.9909, l2 0.0083, filters 51, kernel_sz 2,  dense_sz 129, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.831s.  Loss: tr - 0.598, val - 0.545.  Accuracy: tr - 0.705, val - 0.743.\n",
      "Fold 1 trained 10 epochs in 4.545s.  Loss: tr - 0.171, val - 0.097.  Accuracy: tr - 0.937, val - 0.969.\n",
      "Fold 2 trained 10 epochs in 4.588s.  Loss: tr - 0.498, val - 0.409.  Accuracy: tr - 0.759, val - 0.85.\n",
      "Fold 3 trained 10 epochs in 4.602s.  Loss: tr - 0.569, val - 0.535.  Accuracy: tr - 0.709, val - 0.73.\n",
      "Fold 4 trained 10 epochs in 4.593s.  Loss: tr - 0.352, val - 0.268.  Accuracy: tr - 0.857, val - 0.909.\n",
      "Fold 5 trained 6 epochs in 2.794s.  Loss: tr - 35.571, val - 42.253.  Accuracy: tr - 0.55, val - 0.577.\n",
      "Fold 6 trained 10 epochs in 4.864s.  Loss: tr - 0.678, val - 0.665.  Accuracy: tr - 0.584, val - 0.55.\n",
      "Fold 7 trained 10 epochs in 4.589s.  Loss: tr - 0.627, val - 0.568.  Accuracy: tr - 0.619, val - 0.719.\n",
      "Fold 8 trained 10 epochs in 4.544s.  Loss: tr - 0.614, val - 0.557.  Accuracy: tr - 0.673, val - 0.697.\n",
      "Fold 9 trained 10 epochs in 4.605s.  Loss: tr - 0.625, val - 0.466.  Accuracy: tr - 0.713, val - 0.799.\n",
      "Cross-validation completed in 44.56s. Mean validation loss 4.636 and acc 0.754\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:35:44,14 - [BO (140569664263600).INFO] -- evaluate takes 44.5637s\n",
      "05/15/2024 08:35:44,15 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:35:44,15 - [BO (140569664263600).INFO] -- #1 - fitness: 4.63627781867981, solution: [0.007158871977683808, 0.9301700793815539, 0.9909300776683574, 0.008302362849115497, 51, 2, 129, 0, 0]\n",
      "05/15/2024 08:35:44,35 - [BO (140569664263600).INFO] -- model r2: 0.7305359139778529, MAPE: 0.4452538248221116\n",
      "05/15/2024 08:35:44,35 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:35:44,36 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:35:44,36 - [BO (140569664263600).INFO] -- tell takes 0.0213s\n",
      "05/15/2024 08:35:44,36 - [BO (140569664263600).INFO] -- iteration 66 starts...\n",
      "05/15/2024 08:35:51,486 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 7.4493s\n",
      "05/15/2024 08:35:51,487 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:35:51,487 - [BO (140569664263600).INFO] -- #1 - [0.0007656802105027523, 0.8637724015946838, 0.9940150845491159, 0.017130983211965965, 57, 3, 144, 0, 0]\n",
      "05/15/2024 08:35:51,487 - [BO (140569664263600).INFO] -- ask takes 7.4508s\n",
      "lr 0.0008, beta_1 0.8638, beta_2 0.994, l2 0.0171, filters 57, kernel_sz 3,  dense_sz 144, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.497s.  Loss: tr - 0.039, val - 0.03.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 1 trained 10 epochs in 5.77s.  Loss: tr - 0.046, val - 0.037.  Accuracy: tr - 0.986, val - 0.989.\n",
      "Fold 2 trained 10 epochs in 5.429s.  Loss: tr - 0.038, val - 0.037.  Accuracy: tr - 0.989, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 5.474s.  Loss: tr - 0.045, val - 0.052.  Accuracy: tr - 0.985, val - 0.984.\n",
      "Fold 4 trained 10 epochs in 5.443s.  Loss: tr - 0.04, val - 0.036.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 5 trained 10 epochs in 5.447s.  Loss: tr - 0.044, val - 0.045.  Accuracy: tr - 0.986, val - 0.984.\n",
      "Fold 6 trained 10 epochs in 5.426s.  Loss: tr - 0.043, val - 0.045.  Accuracy: tr - 0.987, val - 0.986.\n",
      "Fold 7 trained 10 epochs in 5.506s.  Loss: tr - 0.043, val - 0.038.  Accuracy: tr - 0.985, val - 0.986.\n",
      "Fold 8 trained 10 epochs in 5.746s.  Loss: tr - 0.037, val - 0.034.  Accuracy: tr - 0.988, val - 0.987.\n",
      "Fold 9 trained 10 epochs in 5.461s.  Loss: tr - 0.034, val - 0.032.  Accuracy: tr - 0.989, val - 0.988.\n",
      "Cross-validation completed in 55.205s. Mean validation loss 0.039 and acc 0.987\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:36:46,695 - [BO (140569664263600).INFO] -- evaluate takes 55.2079s\n",
      "05/15/2024 08:36:46,696 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:36:46,696 - [BO (140569664263600).INFO] -- #1 - fitness: 0.03853753488510847, solution: [0.0007656802105027523, 0.8637724015946838, 0.9940150845491159, 0.017130983211965965, 57, 3, 144, 0, 0]\n",
      "05/15/2024 08:36:46,715 - [BO (140569664263600).INFO] -- model r2: 0.7011788176175638, MAPE: 0.4669497873671258\n",
      "05/15/2024 08:36:46,716 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:36:46,716 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:36:46,716 - [BO (140569664263600).INFO] -- tell takes 0.0206s\n",
      "05/15/2024 08:36:46,717 - [BO (140569664263600).INFO] -- iteration 67 starts...\n",
      "05/15/2024 08:36:49,681 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.9640s\n",
      "05/15/2024 08:36:49,682 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:36:49,682 - [BO (140569664263600).INFO] -- #1 - [0.00048692268203058314, 0.8955327121891825, 0.9902779745258027, 0.01433997640443613, 52, 4, 137, 1, 1]\n",
      "05/15/2024 08:36:49,682 - [BO (140569664263600).INFO] -- ask takes 2.9655s\n",
      "lr 0.0005, beta_1 0.8955, beta_2 0.9903, l2 0.0143, filters 52, kernel_sz 4,  dense_sz 137, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 8.046s.  Loss: tr - 0.101, val - 0.098.  Accuracy: tr - 0.989, val - 0.989.\n",
      "Fold 1 trained 10 epochs in 7.978s.  Loss: tr - 0.113, val - 0.105.  Accuracy: tr - 0.987, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 7.952s.  Loss: tr - 0.105, val - 0.103.  Accuracy: tr - 0.989, val - 0.989.\n",
      "Fold 3 trained 10 epochs in 8.234s.  Loss: tr - 0.099, val - 0.096.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 4 trained 10 epochs in 7.954s.  Loss: tr - 0.111, val - 0.106.  Accuracy: tr - 0.988, val - 0.989.\n",
      "Fold 5 trained 10 epochs in 8.018s.  Loss: tr - 0.106, val - 0.106.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 6 trained 10 epochs in 8.007s.  Loss: tr - 0.107, val - 0.111.  Accuracy: tr - 0.988, val - 0.982.\n",
      "Fold 7 trained 10 epochs in 7.958s.  Loss: tr - 0.114, val - 0.113.  Accuracy: tr - 0.985, val - 0.984.\n",
      "Fold 8 trained 10 epochs in 8.265s.  Loss: tr - 0.102, val - 0.098.  Accuracy: tr - 0.989, val - 0.988.\n",
      "Fold 9 trained 10 epochs in 7.955s.  Loss: tr - 0.111, val - 0.107.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Cross-validation completed in 80.37s. Mean validation loss 0.104 and acc 0.988\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:38:10,55 - [BO (140569664263600).INFO] -- evaluate takes 80.3728s\n",
      "05/15/2024 08:38:10,56 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:38:10,56 - [BO (140569664263600).INFO] -- #1 - fitness: 0.10430284738540649, solution: [0.00048692268203058314, 0.8955327121891825, 0.9902779745258027, 0.01433997640443613, 52, 4, 137, 1, 1]\n",
      "05/15/2024 08:38:10,75 - [BO (140569664263600).INFO] -- model r2: 0.8002998417563016, MAPE: 0.4376204428379295\n",
      "05/15/2024 08:38:10,76 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:38:10,76 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:38:10,77 - [BO (140569664263600).INFO] -- tell takes 0.0210s\n",
      "05/15/2024 08:38:10,77 - [BO (140569664263600).INFO] -- iteration 68 starts...\n",
      "05/15/2024 08:38:10,706 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.6283s\n",
      "05/15/2024 08:38:10,706 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:38:10,707 - [BO (140569664263600).INFO] -- #1 - [0.0011516760542725425, 0.9089610576784936, 0.9902430844796012, 0.0017629269614562214, 52, 3, 145, 1, 0]\n",
      "05/15/2024 08:38:10,707 - [BO (140569664263600).INFO] -- ask takes 0.6299s\n",
      "lr 0.0012, beta_1 0.909, beta_2 0.9902, l2 0.0018, filters 52, kernel_sz 3,  dense_sz 145, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 5.128s.  Loss: tr - 0.058, val - 0.05.  Accuracy: tr - 0.992, val - 0.995.\n",
      "Fold 1 trained 10 epochs in 5.066s.  Loss: tr - 0.063, val - 0.058.  Accuracy: tr - 0.99, val - 0.992.\n",
      "Fold 2 trained 10 epochs in 5.116s.  Loss: tr - 0.061, val - 0.06.  Accuracy: tr - 0.992, val - 0.991.\n",
      "Fold 3 trained 10 epochs in 5.082s.  Loss: tr - 0.06, val - 0.061.  Accuracy: tr - 0.992, val - 0.992.\n",
      "Fold 4 trained 10 epochs in 5.366s.  Loss: tr - 0.082, val - 0.078.  Accuracy: tr - 0.987, val - 0.985.\n",
      "Fold 5 trained 10 epochs in 5.046s.  Loss: tr - 0.078, val - 0.076.  Accuracy: tr - 0.988, val - 0.987.\n",
      "Fold 6 trained 10 epochs in 5.104s.  Loss: tr - 0.07, val - 0.066.  Accuracy: tr - 0.99, val - 0.991.\n",
      "Fold 7 trained 10 epochs in 5.104s.  Loss: tr - 0.078, val - 0.082.  Accuracy: tr - 0.987, val - 0.983.\n",
      "Fold 8 trained 10 epochs in 5.11s.  Loss: tr - 0.071, val - 0.066.  Accuracy: tr - 0.989, val - 0.988.\n",
      "Fold 9 trained 10 epochs in 5.326s.  Loss: tr - 0.059, val - 0.058.  Accuracy: tr - 0.992, val - 0.99.\n",
      "Cross-validation completed in 51.453s. Mean validation loss 0.065 and acc 0.99\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:39:02,163 - [BO (140569664263600).INFO] -- evaluate takes 51.4558s\n",
      "05/15/2024 08:39:02,164 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:39:02,164 - [BO (140569664263600).INFO] -- #1 - fitness: 0.06543789207935333, solution: [0.0011516760542725425, 0.9089610576784936, 0.9902430844796012, 0.0017629269614562214, 52, 3, 145, 1, 0]\n",
      "05/15/2024 08:39:02,183 - [BO (140569664263600).INFO] -- model r2: 0.7852300619173763, MAPE: 0.4873173143618141\n",
      "05/15/2024 08:39:02,183 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:39:02,184 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:39:02,184 - [BO (140569664263600).INFO] -- tell takes 0.0204s\n",
      "05/15/2024 08:39:02,184 - [BO (140569664263600).INFO] -- iteration 69 starts...\n",
      "05/15/2024 08:39:05,376 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 3.1919s\n",
      "05/15/2024 08:39:05,377 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:39:05,378 - [BO (140569664263600).INFO] -- #1 - [0.0036353057343731583, 0.9030200523928092, 0.9998648783902238, 0.01797238629017501, 25, 2, 77, 0, 0]\n",
      "05/15/2024 08:39:05,378 - [BO (140569664263600).INFO] -- ask takes 3.1935s\n",
      "lr 0.0036, beta_1 0.903, beta_2 0.9999, l2 0.018, filters 25, kernel_sz 2,  dense_sz 77, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 2.292s.  Loss: tr - 0.034, val - 0.028.  Accuracy: tr - 0.989, val - 0.993.\n",
      "Fold 1 trained 10 epochs in 2.3s.  Loss: tr - 0.052, val - 0.046.  Accuracy: tr - 0.983, val - 0.985.\n",
      "Fold 2 trained 10 epochs in 2.301s.  Loss: tr - 0.079, val - 0.059.  Accuracy: tr - 0.976, val - 0.981.\n",
      "Fold 3 trained 10 epochs in 2.307s.  Loss: tr - 0.033, val - 0.037.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 2.315s.  Loss: tr - 0.075, val - 0.061.  Accuracy: tr - 0.977, val - 0.98.\n",
      "Fold 5 trained 10 epochs in 2.609s.  Loss: tr - 0.079, val - 0.059.  Accuracy: tr - 0.973, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 2.274s.  Loss: tr - 0.154, val - 0.09.  Accuracy: tr - 0.942, val - 0.979.\n",
      "Fold 7 trained 10 epochs in 2.328s.  Loss: tr - 0.035, val - 0.04.  Accuracy: tr - 0.99, val - 0.985.\n",
      "Fold 8 trained 10 epochs in 2.278s.  Loss: tr - 0.225, val - 0.254.  Accuracy: tr - 0.918, val - 0.902.\n",
      "Fold 9 trained 10 epochs in 2.257s.  Loss: tr - 0.057, val - 0.051.  Accuracy: tr - 0.984, val - 0.984.\n",
      "Cross-validation completed in 23.265s. Mean validation loss 0.073 and acc 0.976\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:39:28,646 - [BO (140569664263600).INFO] -- evaluate takes 23.2675s\n",
      "05/15/2024 08:39:28,646 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:39:28,646 - [BO (140569664263600).INFO] -- #1 - fitness: 0.07273922227323056, solution: [0.0036353057343731583, 0.9030200523928092, 0.9998648783902238, 0.01797238629017501, 25, 2, 77, 0, 0]\n",
      "05/15/2024 08:39:28,666 - [BO (140569664263600).INFO] -- model r2: 0.7393672250551884, MAPE: 0.5421066981345823\n",
      "05/15/2024 08:39:28,666 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:39:28,666 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:39:28,667 - [BO (140569664263600).INFO] -- tell takes 0.0206s\n",
      "05/15/2024 08:39:28,667 - [BO (140569664263600).INFO] -- iteration 70 starts...\n",
      "05/15/2024 08:39:30,336 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.6689s\n",
      "05/15/2024 08:39:30,337 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:39:30,337 - [BO (140569664263600).INFO] -- #1 - [0.006278827468875228, 0.9299456569969059, 0.9958441306114836, 0.019969872673423286, 53, 4, 50, 1, 0]\n",
      "05/15/2024 08:39:30,338 - [BO (140569664263600).INFO] -- ask takes 1.6705s\n",
      "lr 0.0063, beta_1 0.9299, beta_2 0.9958, l2 0.02, filters 53, kernel_sz 4,  dense_sz 50, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 7.498s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 7.119s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 7.12s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.593, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 7.097s.  Loss: tr - 0.661, val - 0.65.  Accuracy: tr - 0.584, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 7.108s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.594.\n",
      "Fold 5 trained 10 epochs in 7.429s.  Loss: tr - 0.668, val - 0.651.  Accuracy: tr - 0.576, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 7.08s.  Loss: tr - 0.656, val - 0.661.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 7.098s.  Loss: tr - 0.656, val - 0.664.  Accuracy: tr - 0.592, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 7.156s.  Loss: tr - 0.657, val - 0.657.  Accuracy: tr - 0.592, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 7.124s.  Loss: tr - 0.656, val - 0.66.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Cross-validation completed in 71.834s. Mean validation loss 0.656 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:40:42,175 - [BO (140569664263600).INFO] -- evaluate takes 71.8373s\n",
      "05/15/2024 08:40:42,176 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:40:42,176 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6561481595039368, solution: [0.006278827468875228, 0.9299456569969059, 0.9958441306114836, 0.019969872673423286, 53, 4, 50, 1, 0]\n",
      "05/15/2024 08:40:42,195 - [BO (140569664263600).INFO] -- model r2: 0.8003624525182769, MAPE: 0.3831929894004491\n",
      "05/15/2024 08:40:42,196 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:40:42,196 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:40:42,196 - [BO (140569664263600).INFO] -- tell takes 0.0207s\n",
      "05/15/2024 08:40:42,197 - [BO (140569664263600).INFO] -- iteration 71 starts...\n",
      "05/15/2024 08:40:43,543 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3466s\n",
      "05/15/2024 08:40:43,544 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:40:43,545 - [BO (140569664263600).INFO] -- #1 - [0.00375936168185123, 0.9123963209918468, 0.9997310531136642, 0.013342641464593633, 19, 3, 147, 0, 1]\n",
      "05/15/2024 08:40:43,545 - [BO (140569664263600).INFO] -- ask takes 1.3481s\n",
      "lr 0.0038, beta_1 0.9124, beta_2 0.9997, l2 0.0133, filters 19, kernel_sz 3,  dense_sz 147, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 2.563s.  Loss: tr - 0.068, val - 0.054.  Accuracy: tr - 0.978, val - 0.985.\n",
      "Fold 1 trained 10 epochs in 2.203s.  Loss: tr - 0.05, val - 0.043.  Accuracy: tr - 0.984, val - 0.986.\n",
      "Fold 2 trained 10 epochs in 2.271s.  Loss: tr - 0.071, val - 0.057.  Accuracy: tr - 0.972, val - 0.983.\n",
      "Fold 3 trained 10 epochs in 2.288s.  Loss: tr - 0.125, val - 0.115.  Accuracy: tr - 0.953, val - 0.959.\n",
      "Fold 4 trained 10 epochs in 2.307s.  Loss: tr - 0.39, val - 0.239.  Accuracy: tr - 0.848, val - 0.892.\n",
      "Fold 5 trained 10 epochs in 2.514s.  Loss: tr - 0.649, val - 0.638.  Accuracy: tr - 0.64, val - 0.719.\n",
      "Fold 6 trained 10 epochs in 2.263s.  Loss: tr - 0.103, val - 0.086.  Accuracy: tr - 0.964, val - 0.969.\n",
      "Fold 7 trained 10 epochs in 2.283s.  Loss: tr - 0.085, val - 0.067.  Accuracy: tr - 0.971, val - 0.977.\n",
      "Fold 8 trained 10 epochs in 2.208s.  Loss: tr - 0.276, val - 0.14.  Accuracy: tr - 0.897, val - 0.951.\n",
      "Fold 9 trained 10 epochs in 2.23s.  Loss: tr - 0.036, val - 0.029.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Cross-validation completed in 23.134s. Mean validation loss 0.147 and acc 0.941\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:41:06,683 - [BO (140569664263600).INFO] -- evaluate takes 23.1376s\n",
      "05/15/2024 08:41:06,683 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:41:06,684 - [BO (140569664263600).INFO] -- #1 - fitness: 0.14688148070126772, solution: [0.00375936168185123, 0.9123963209918468, 0.9997310531136642, 0.013342641464593633, 19, 3, 147, 0, 1]\n",
      "05/15/2024 08:41:06,703 - [BO (140569664263600).INFO] -- model r2: 0.7557732872334257, MAPE: 0.4451255666687735\n",
      "05/15/2024 08:41:06,704 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:41:06,704 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:41:06,704 - [BO (140569664263600).INFO] -- tell takes 0.0210s\n",
      "05/15/2024 08:41:06,704 - [BO (140569664263600).INFO] -- iteration 72 starts...\n",
      "05/15/2024 08:41:09,529 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.8247s\n",
      "05/15/2024 08:41:09,530 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:41:09,531 - [BO (140569664263600).INFO] -- #1 - [0.0014024756194023055, 0.9094077463699852, 0.9959810162696031, 0.005315178098283748, 41, 2, 51, 0, 0]\n",
      "05/15/2024 08:41:09,531 - [BO (140569664263600).INFO] -- ask takes 2.8261s\n",
      "lr 0.0014, beta_1 0.9094, beta_2 0.996, l2 0.0053, filters 41, kernel_sz 2,  dense_sz 51, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 3.792s.  Loss: tr - 0.053, val - 0.042.  Accuracy: tr - 0.982, val - 0.986.\n",
      "Fold 1 trained 10 epochs in 4.077s.  Loss: tr - 0.073, val - 0.07.  Accuracy: tr - 0.974, val - 0.975.\n",
      "Fold 2 trained 10 epochs in 3.809s.  Loss: tr - 0.186, val - 0.146.  Accuracy: tr - 0.925, val - 0.94.\n",
      "Fold 3 trained 10 epochs in 3.834s.  Loss: tr - 0.169, val - 0.143.  Accuracy: tr - 0.939, val - 0.951.\n",
      "Fold 4 trained 10 epochs in 3.76s.  Loss: tr - 0.336, val - 0.308.  Accuracy: tr - 0.837, val - 0.846.\n",
      "Fold 5 trained 10 epochs in 3.804s.  Loss: tr - 0.039, val - 0.035.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 6 trained 10 epochs in 3.83s.  Loss: tr - 0.099, val - 0.088.  Accuracy: tr - 0.965, val - 0.965.\n",
      "Fold 7 trained 10 epochs in 4.039s.  Loss: tr - 0.066, val - 0.055.  Accuracy: tr - 0.981, val - 0.984.\n",
      "Fold 8 trained 10 epochs in 3.781s.  Loss: tr - 0.039, val - 0.033.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 9 trained 10 epochs in 3.794s.  Loss: tr - 0.305, val - 0.268.  Accuracy: tr - 0.856, val - 0.881.\n",
      "Cross-validation completed in 38.526s. Mean validation loss 0.119 and acc 0.951\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:41:48,60 - [BO (140569664263600).INFO] -- evaluate takes 38.5290s\n",
      "05/15/2024 08:41:48,60 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:41:48,61 - [BO (140569664263600).INFO] -- #1 - fitness: 0.11880184747278691, solution: [0.0014024756194023055, 0.9094077463699852, 0.9959810162696031, 0.005315178098283748, 41, 2, 51, 0, 0]\n",
      "05/15/2024 08:41:48,80 - [BO (140569664263600).INFO] -- model r2: 0.7348385319057653, MAPE: 0.41720743896812107\n",
      "05/15/2024 08:41:48,81 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:41:48,81 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:41:48,81 - [BO (140569664263600).INFO] -- tell takes 0.0209s\n",
      "05/15/2024 08:41:48,82 - [BO (140569664263600).INFO] -- iteration 73 starts...\n",
      "05/15/2024 08:41:49,41 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.9594s\n",
      "05/15/2024 08:41:49,42 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:41:49,42 - [BO (140569664263600).INFO] -- #1 - [0.0036971383382416474, 0.8787844741058256, 0.9901493155625061, 0.019426511112578936, 60, 3, 53, 0, 0]\n",
      "05/15/2024 08:41:49,43 - [BO (140569664263600).INFO] -- ask takes 0.9608s\n",
      "lr 0.0037, beta_1 0.8788, beta_2 0.9901, l2 0.0194, filters 60, kernel_sz 3,  dense_sz 53, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.562s.  Loss: tr - 0.654, val - 0.648.  Accuracy: tr - 0.594, val - 0.598.\n",
      "Fold 1 trained 10 epochs in 5.59s.  Loss: tr - 0.657, val - 0.656.  Accuracy: tr - 0.594, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 5.875s.  Loss: tr - 0.494, val - 0.311.  Accuracy: tr - 0.785, val - 0.877.\n",
      "Fold 3 trained 10 epochs in 5.63s.  Loss: tr - 0.432, val - 0.311.  Accuracy: tr - 0.805, val - 0.889.\n",
      "Fold 4 trained 10 epochs in 5.617s.  Loss: tr - 0.563, val - 0.455.  Accuracy: tr - 0.756, val - 0.82.\n",
      "Fold 5 trained 10 epochs in 5.573s.  Loss: tr - 0.159, val - 0.106.  Accuracy: tr - 0.944, val - 0.963.\n",
      "Fold 6 trained 10 epochs in 5.626s.  Loss: tr - 0.65, val - 0.649.  Accuracy: tr - 0.613, val - 0.626.\n",
      "Fold 7 trained 10 epochs in 5.607s.  Loss: tr - 0.59, val - 0.556.  Accuracy: tr - 0.687, val - 0.689.\n",
      "Fold 8 trained 10 epochs in 5.919s.  Loss: tr - 0.561, val - 0.462.  Accuracy: tr - 0.729, val - 0.81.\n",
      "Fold 9 trained 10 epochs in 5.59s.  Loss: tr - 0.512, val - 0.466.  Accuracy: tr - 0.764, val - 0.765.\n",
      "Cross-validation completed in 56.594s. Mean validation loss 0.462 and acc 0.763\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:42:45,640 - [BO (140569664263600).INFO] -- evaluate takes 56.5968s\n",
      "05/15/2024 08:42:45,640 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:42:45,640 - [BO (140569664263600).INFO] -- #1 - fitness: 0.46196950376033785, solution: [0.0036971383382416474, 0.8787844741058256, 0.9901493155625061, 0.019426511112578936, 60, 3, 53, 0, 0]\n",
      "05/15/2024 08:42:45,660 - [BO (140569664263600).INFO] -- model r2: 0.7138778255426514, MAPE: 0.4545989823418694\n",
      "05/15/2024 08:42:45,661 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:42:45,661 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:42:45,662 - [BO (140569664263600).INFO] -- tell takes 0.0214s\n",
      "05/15/2024 08:42:45,662 - [BO (140569664263600).INFO] -- iteration 74 starts...\n",
      "05/15/2024 08:42:47,722 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.0595s\n",
      "05/15/2024 08:42:47,722 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:42:47,723 - [BO (140569664263600).INFO] -- #1 - [0.0034123403388880485, 0.8845593488071681, 0.9954043534626213, 0.00975686636284456, 55, 3, 51, 0, 1]\n",
      "05/15/2024 08:42:47,723 - [BO (140569664263600).INFO] -- ask takes 2.0610s\n",
      "lr 0.0034, beta_1 0.8846, beta_2 0.9954, l2 0.0098, filters 55, kernel_sz 3,  dense_sz 51, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.387s.  Loss: tr - 0.644, val - 0.567.  Accuracy: tr - 0.699, val - 0.735.\n",
      "Fold 1 trained 10 epochs in 6.379s.  Loss: tr - 0.058, val - 0.046.  Accuracy: tr - 0.981, val - 0.983.\n",
      "Fold 2 trained 10 epochs in 6.399s.  Loss: tr - 0.338, val - 0.259.  Accuracy: tr - 0.849, val - 0.89.\n",
      "Fold 3 trained 10 epochs in 6.659s.  Loss: tr - 0.591, val - 0.494.  Accuracy: tr - 0.723, val - 0.86.\n",
      "Fold 4 trained 10 epochs in 6.369s.  Loss: tr - 0.453, val - 0.412.  Accuracy: tr - 0.768, val - 0.786.\n",
      "Fold 5 trained 10 epochs in 6.373s.  Loss: tr - 0.428, val - 0.346.  Accuracy: tr - 0.816, val - 0.865.\n",
      "Fold 6 trained 10 epochs in 6.367s.  Loss: tr - 0.285, val - 0.271.  Accuracy: tr - 0.88, val - 0.898.\n",
      "Fold 7 trained 10 epochs in 6.396s.  Loss: tr - 0.652, val - 0.659.  Accuracy: tr - 0.611, val - 0.577.\n",
      "Fold 8 trained 10 epochs in 6.643s.  Loss: tr - 0.512, val - 0.429.  Accuracy: tr - 0.751, val - 0.851.\n",
      "Fold 9 trained 10 epochs in 6.402s.  Loss: tr - 0.334, val - 0.234.  Accuracy: tr - 0.861, val - 0.906.\n",
      "Cross-validation completed in 64.379s. Mean validation loss 0.372 and acc 0.835\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:43:52,105 - [BO (140569664263600).INFO] -- evaluate takes 64.3820s\n",
      "05/15/2024 08:43:52,106 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:43:52,106 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3717542439699173, solution: [0.0034123403388880485, 0.8845593488071681, 0.9954043534626213, 0.00975686636284456, 55, 3, 51, 0, 1]\n",
      "05/15/2024 08:43:52,126 - [BO (140569664263600).INFO] -- model r2: 0.7523825690651118, MAPE: 0.3679752923244413\n",
      "05/15/2024 08:43:52,126 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:43:52,126 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:43:52,127 - [BO (140569664263600).INFO] -- tell takes 0.0208s\n",
      "05/15/2024 08:43:52,127 - [BO (140569664263600).INFO] -- iteration 75 starts...\n",
      "05/15/2024 08:43:53,578 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.4511s\n",
      "05/15/2024 08:43:53,579 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:43:53,579 - [BO (140569664263600).INFO] -- #1 - [0.0010320035749576452, 0.940198499784813, 0.9931676865279194, 0.019276751379084897, 55, 3, 113, 1, 0]\n",
      "05/15/2024 08:43:53,580 - [BO (140569664263600).INFO] -- ask takes 1.4526s\n",
      "lr 0.001, beta_1 0.9402, beta_2 0.9932, l2 0.0193, filters 55, kernel_sz 3,  dense_sz 113, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 5.29s.  Loss: tr - 0.142, val - 0.144.  Accuracy: tr - 0.981, val - 0.975.\n",
      "Fold 1 trained 10 epochs in 5.232s.  Loss: tr - 0.111, val - 0.102.  Accuracy: tr - 0.985, val - 0.989.\n",
      "Fold 2 trained 10 epochs in 5.267s.  Loss: tr - 0.134, val - 0.123.  Accuracy: tr - 0.982, val - 0.982.\n",
      "Fold 3 trained 10 epochs in 5.307s.  Loss: tr - 0.127, val - 0.126.  Accuracy: tr - 0.979, val - 0.977.\n",
      "Fold 4 trained 10 epochs in 5.592s.  Loss: tr - 0.114, val - 0.104.  Accuracy: tr - 0.986, val - 0.99.\n",
      "Fold 5 trained 10 epochs in 5.255s.  Loss: tr - 0.143, val - 0.141.  Accuracy: tr - 0.982, val - 0.98.\n",
      "Fold 6 trained 10 epochs in 5.235s.  Loss: tr - 0.122, val - 0.125.  Accuracy: tr - 0.984, val - 0.978.\n",
      "Fold 7 trained 10 epochs in 5.254s.  Loss: tr - 0.143, val - 0.145.  Accuracy: tr - 0.978, val - 0.973.\n",
      "Fold 8 trained 10 epochs in 5.274s.  Loss: tr - 0.127, val - 0.117.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 9 trained 10 epochs in 5.56s.  Loss: tr - 0.117, val - 0.113.  Accuracy: tr - 0.984, val - 0.983.\n",
      "Cross-validation completed in 53.27s. Mean validation loss 0.124 and acc 0.981\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:44:46,853 - [BO (140569664263600).INFO] -- evaluate takes 53.2732s\n",
      "05/15/2024 08:44:46,854 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:44:46,854 - [BO (140569664263600).INFO] -- #1 - fitness: 0.12390438914299011, solution: [0.0010320035749576452, 0.940198499784813, 0.9931676865279194, 0.019276751379084897, 55, 3, 113, 1, 0]\n",
      "05/15/2024 08:44:46,873 - [BO (140569664263600).INFO] -- model r2: 0.7638678101284412, MAPE: 0.416064669584929\n",
      "05/15/2024 08:44:46,874 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:44:46,874 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:44:46,874 - [BO (140569664263600).INFO] -- tell takes 0.0208s\n",
      "05/15/2024 08:44:46,875 - [BO (140569664263600).INFO] -- iteration 76 starts...\n",
      "05/15/2024 08:44:48,214 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3390s\n",
      "05/15/2024 08:44:48,215 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:44:48,215 - [BO (140569664263600).INFO] -- #1 - [0.006980447456994233, 0.8865981229922448, 0.9905605073519241, 0.016457560637013265, 64, 2, 75, 1, 1]\n",
      "05/15/2024 08:44:48,215 - [BO (140569664263600).INFO] -- ask takes 1.3405s\n",
      "lr 0.007, beta_1 0.8866, beta_2 0.9906, l2 0.0165, filters 64, kernel_sz 2,  dense_sz 75, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 5.276s.  Loss: tr - 0.661, val - 0.656.  Accuracy: tr - 0.581, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 5.302s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.581, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 5.319s.  Loss: tr - 0.701, val - 0.654.  Accuracy: tr - 0.517, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 5.272s.  Loss: tr - 0.661, val - 0.652.  Accuracy: tr - 0.583, val - 0.602.\n",
      "Fold 4 trained 10 epochs in 5.644s.  Loss: tr - 0.663, val - 0.656.  Accuracy: tr - 0.58, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 5.272s.  Loss: tr - 0.661, val - 0.653.  Accuracy: tr - 0.58, val - 0.58.\n",
      "Fold 6 trained 10 epochs in 5.315s.  Loss: tr - 0.678, val - 0.661.  Accuracy: tr - 0.557, val - 0.585.\n",
      "Fold 7 trained 10 epochs in 5.282s.  Loss: tr - 0.662, val - 0.666.  Accuracy: tr - 0.587, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.295s.  Loss: tr - 0.706, val - 0.659.  Accuracy: tr - 0.54, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 5.598s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.594, val - 0.588.\n",
      "Cross-validation completed in 53.58s. Mean validation loss 0.657 and acc 0.591\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:45:41,799 - [BO (140569664263600).INFO] -- evaluate takes 53.5832s\n",
      "05/15/2024 08:45:41,799 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:45:41,799 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6571786165237427, solution: [0.006980447456994233, 0.8865981229922448, 0.9905605073519241, 0.016457560637013265, 64, 2, 75, 1, 1]\n",
      "05/15/2024 08:45:41,819 - [BO (140569664263600).INFO] -- model r2: 0.7747909110432398, MAPE: 0.34683628572871117\n",
      "05/15/2024 08:45:41,820 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:45:41,820 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:45:41,820 - [BO (140569664263600).INFO] -- tell takes 0.0214s\n",
      "05/15/2024 08:45:41,821 - [BO (140569664263600).INFO] -- iteration 77 starts...\n",
      "05/15/2024 08:45:43,150 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.3292s\n",
      "05/15/2024 08:45:43,151 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:45:43,151 - [BO (140569664263600).INFO] -- #1 - [0.0055497138631725226, 0.876582477048282, 0.9911077966538792, 0.018788212389846893, 62, 4, 74, 0, 1]\n",
      "05/15/2024 08:45:43,152 - [BO (140569664263600).INFO] -- ask takes 1.3307s\n",
      "lr 0.0055, beta_1 0.8766, beta_2 0.9911, l2 0.0188, filters 62, kernel_sz 4,  dense_sz 74, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.895s.  Loss: tr - 0.687, val - 0.686.  Accuracy: tr - 0.594, val - 0.595.\n",
      "Fold 1 trained 8 epochs in 7.131s.  Loss: tr - 51.451, val - 43.817.  Accuracy: tr - 0.485, val - 0.562.\n",
      "Fold 2 trained 6 epochs in 5.32s.  Loss: tr - 40.343, val - 47.248.  Accuracy: tr - 0.51, val - 0.528.\n",
      "Fold 3 trained 6 epochs in 5.311s.  Loss: tr - 42.789, val - 50.178.  Accuracy: tr - 0.5, val - 0.498.\n",
      "Fold 4 trained 6 epochs in 5.375s.  Loss: tr - 34.744, val - 42.176.  Accuracy: tr - 0.547, val - 0.578.\n",
      "Fold 5 trained 10 epochs in 8.829s.  Loss: tr - 2.019, val - 0.687.  Accuracy: tr - 0.491, val - 0.582.\n",
      "Fold 6 trained 6 epochs in 5.63s.  Loss: tr - 34.862, val - 43.017.  Accuracy: tr - 0.578, val - 0.57.\n",
      "Fold 7 trained 6 epochs in 5.35s.  Loss: tr - 34.694, val - 41.259.  Accuracy: tr - 0.571, val - 0.587.\n",
      "Fold 8 trained 6 epochs in 5.379s.  Loss: tr - 36.666, val - 43.909.  Accuracy: tr - 0.551, val - 0.561.\n",
      "Fold 9 trained 6 epochs in 5.314s.  Loss: tr - 31.543, val - 41.131.  Accuracy: tr - 0.563, val - 0.589.\n",
      "Cross-validation completed in 62.54s. Mean validation loss 35.411 and acc 0.565\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:46:45,695 - [BO (140569664263600).INFO] -- evaluate takes 62.5430s\n",
      "05/15/2024 08:46:45,695 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:46:45,696 - [BO (140569664263600).INFO] -- #1 - fitness: 35.41090086102486, solution: [0.0055497138631725226, 0.876582477048282, 0.9911077966538792, 0.018788212389846893, 62, 4, 74, 0, 1]\n",
      "05/15/2024 08:46:45,716 - [BO (140569664263600).INFO] -- model r2: 0.7049672203534738, MAPE: 0.5601206873847056\n",
      "05/15/2024 08:46:45,716 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:46:45,717 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:46:45,717 - [BO (140569664263600).INFO] -- tell takes 0.0217s\n",
      "05/15/2024 08:46:45,717 - [BO (140569664263600).INFO] -- iteration 78 starts...\n",
      "05/15/2024 08:46:47,575 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.8580s\n",
      "05/15/2024 08:46:47,576 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:46:47,577 - [BO (140569664263600).INFO] -- #1 - [0.0048825710024612175, 0.8825336741676932, 0.9906477444099206, 0.019722702412305348, 60, 3, 62, 1, 1]\n",
      "05/15/2024 08:46:47,577 - [BO (140569664263600).INFO] -- ask takes 1.8595s\n",
      "lr 0.0049, beta_1 0.8825, beta_2 0.9906, l2 0.0197, filters 60, kernel_sz 3,  dense_sz 62, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 6.926s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.58, val - 0.586.\n",
      "Fold 1 trained 10 epochs in 6.939s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.587, val - 0.593.\n",
      "Fold 2 trained 10 epochs in 7.153s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.59, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 6.965s.  Loss: tr - 0.66, val - 0.65.  Accuracy: tr - 0.589, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 6.924s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.586, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 6.917s.  Loss: tr - 0.665, val - 0.654.  Accuracy: tr - 0.583, val - 0.582.\n",
      "Fold 6 trained 10 epochs in 6.912s.  Loss: tr - 0.66, val - 0.661.  Accuracy: tr - 0.589, val - 0.588.\n",
      "Fold 7 trained 10 epochs in 6.89s.  Loss: tr - 0.662, val - 0.664.  Accuracy: tr - 0.586, val - 0.587.\n",
      "Fold 8 trained 10 epochs in 6.9s.  Loss: tr - 0.658, val - 0.657.  Accuracy: tr - 0.587, val - 0.603.\n",
      "Fold 9 trained 10 epochs in 7.158s.  Loss: tr - 0.657, val - 0.66.  Accuracy: tr - 0.592, val - 0.589.\n",
      "Cross-validation completed in 69.689s. Mean validation loss 0.657 and acc 0.592\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:47:57,269 - [BO (140569664263600).INFO] -- evaluate takes 69.6919s\n",
      "05/15/2024 08:47:57,270 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:47:57,270 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6568965494632721, solution: [0.0048825710024612175, 0.8825336741676932, 0.9906477444099206, 0.019722702412305348, 60, 3, 62, 1, 1]\n",
      "05/15/2024 08:47:57,290 - [BO (140569664263600).INFO] -- model r2: 0.724902848459672, MAPE: 0.6268914961079205\n",
      "05/15/2024 08:47:57,290 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:47:57,291 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:47:57,291 - [BO (140569664263600).INFO] -- tell takes 0.0213s\n",
      "05/15/2024 08:47:57,291 - [BO (140569664263600).INFO] -- iteration 79 starts...\n",
      "05/15/2024 08:47:58,880 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5884s\n",
      "05/15/2024 08:47:58,881 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:47:58,881 - [BO (140569664263600).INFO] -- #1 - [0.005538068023371027, 0.9059515819552281, 0.9983104686502017, 0.0168458141017698, 56, 3, 81, 0, 0]\n",
      "05/15/2024 08:47:58,881 - [BO (140569664263600).INFO] -- ask takes 1.5899s\n",
      "lr 0.0055, beta_1 0.906, beta_2 0.9983, l2 0.0168, filters 56, kernel_sz 3,  dense_sz 81, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.259s.  Loss: tr - 0.654, val - 0.609.  Accuracy: tr - 0.654, val - 0.674.\n",
      "Fold 1 trained 10 epochs in 5.175s.  Loss: tr - 0.655, val - 0.648.  Accuracy: tr - 0.584, val - 0.612.\n",
      "Fold 2 trained 10 epochs in 5.22s.  Loss: tr - 0.659, val - 0.653.  Accuracy: tr - 0.59, val - 0.592.\n",
      "Fold 3 trained 10 epochs in 5.174s.  Loss: tr - 0.705, val - 0.653.  Accuracy: tr - 0.572, val - 0.599.\n",
      "Fold 4 trained 6 epochs in 3.163s.  Loss: tr - 43.187, val - 52.166.  Accuracy: tr - 0.488, val - 0.478.\n",
      "Fold 5 trained 6 epochs in 3.224s.  Loss: tr - 31.425, val - 40.316.  Accuracy: tr - 0.559, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 5.452s.  Loss: tr - 0.655, val - 0.657.  Accuracy: tr - 0.602, val - 0.593.\n",
      "Fold 7 trained 10 epochs in 5.16s.  Loss: tr - 0.236, val - 0.168.  Accuracy: tr - 0.907, val - 0.93.\n",
      "Fold 8 trained 10 epochs in 5.215s.  Loss: tr - 0.651, val - 0.62.  Accuracy: tr - 0.637, val - 0.771.\n",
      "Fold 9 trained 10 epochs in 5.219s.  Loss: tr - 0.664, val - 0.662.  Accuracy: tr - 0.591, val - 0.591.\n",
      "Cross-validation completed in 48.265s. Mean validation loss 9.715 and acc 0.644\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:48:47,150 - [BO (140569664263600).INFO] -- evaluate takes 48.2684s\n",
      "05/15/2024 08:48:47,150 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:48:47,151 - [BO (140569664263600).INFO] -- #1 - fitness: 9.71515907049179, solution: [0.005538068023371027, 0.9059515819552281, 0.9983104686502017, 0.0168458141017698, 56, 3, 81, 0, 0]\n",
      "05/15/2024 08:48:47,171 - [BO (140569664263600).INFO] -- model r2: 0.676726537339506, MAPE: 0.4645649591583667\n",
      "05/15/2024 08:48:47,171 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:48:47,172 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:48:47,172 - [BO (140569664263600).INFO] -- tell takes 0.0218s\n",
      "05/15/2024 08:48:47,172 - [BO (140569664263600).INFO] -- iteration 80 starts...\n",
      "05/15/2024 08:48:47,772 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 0.5994s\n",
      "05/15/2024 08:48:47,773 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:48:47,773 - [BO (140569664263600).INFO] -- #1 - [0.006997263002042644, 0.9012858025168351, 0.9946749877785998, 0.01396687018292973, 34, 2, 89, 1, 1]\n",
      "05/15/2024 08:48:47,774 - [BO (140569664263600).INFO] -- ask takes 0.6009s\n",
      "lr 0.007, beta_1 0.9013, beta_2 0.9947, l2 0.014, filters 34, kernel_sz 2,  dense_sz 89, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 3.855s.  Loss: tr - 0.728, val - 0.656.  Accuracy: tr - 0.487, val - 0.583.\n",
      "Fold 1 trained 10 epochs in 3.863s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 4.085s.  Loss: tr - 0.662, val - 0.655.  Accuracy: tr - 0.59, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 3.887s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.592, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 3.816s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 3.881s.  Loss: tr - 0.662, val - 0.651.  Accuracy: tr - 0.58, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 3.815s.  Loss: tr - 0.657, val - 0.661.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 4.106s.  Loss: tr - 0.659, val - 0.665.  Accuracy: tr - 0.591, val - 0.574.\n",
      "Fold 8 trained 10 epochs in 3.819s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.589, val - 0.589.\n",
      "Fold 9 trained 10 epochs in 3.903s.  Loss: tr - 0.665, val - 0.66.  Accuracy: tr - 0.591, val - 0.587.\n",
      "Cross-validation completed in 39.034s. Mean validation loss 0.657 and acc 0.589\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:49:26,811 - [BO (140569664263600).INFO] -- evaluate takes 39.0373s\n",
      "05/15/2024 08:49:26,812 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:49:26,812 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6567477881908417, solution: [0.006997263002042644, 0.9012858025168351, 0.9946749877785998, 0.01396687018292973, 34, 2, 89, 1, 1]\n",
      "05/15/2024 08:49:26,832 - [BO (140569664263600).INFO] -- model r2: 0.7079126558350923, MAPE: 0.44461854407792667\n",
      "05/15/2024 08:49:26,832 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:49:26,833 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:49:26,833 - [BO (140569664263600).INFO] -- tell takes 0.0213s\n",
      "05/15/2024 08:49:26,833 - [BO (140569664263600).INFO] -- iteration 81 starts...\n",
      "05/15/2024 08:49:29,841 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 3.0076s\n",
      "05/15/2024 08:49:29,842 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:49:29,842 - [BO (140569664263600).INFO] -- #1 - [0.0009475123849017082, 0.8625938444559781, 0.9959948008167309, 0.018162943615044443, 57, 3, 128, 0, 0]\n",
      "05/15/2024 08:49:29,842 - [BO (140569664263600).INFO] -- ask takes 3.0091s\n",
      "lr 0.0009, beta_1 0.8626, beta_2 0.996, l2 0.0182, filters 57, kernel_sz 3,  dense_sz 128, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.444s.  Loss: tr - 0.031, val - 0.024.  Accuracy: tr - 0.99, val - 0.992.\n",
      "Fold 1 trained 10 epochs in 5.473s.  Loss: tr - 0.041, val - 0.035.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 2 trained 10 epochs in 5.731s.  Loss: tr - 0.033, val - 0.044.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 3 trained 10 epochs in 5.453s.  Loss: tr - 0.036, val - 0.037.  Accuracy: tr - 0.989, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 5.442s.  Loss: tr - 0.042, val - 0.035.  Accuracy: tr - 0.987, val - 0.989.\n",
      "Fold 5 trained 10 epochs in 5.509s.  Loss: tr - 0.065, val - 0.061.  Accuracy: tr - 0.979, val - 0.982.\n",
      "Fold 6 trained 10 epochs in 5.471s.  Loss: tr - 0.035, val - 0.038.  Accuracy: tr - 0.989, val - 0.99.\n",
      "Fold 7 trained 10 epochs in 5.511s.  Loss: tr - 0.043, val - 0.045.  Accuracy: tr - 0.987, val - 0.986.\n",
      "Fold 8 trained 10 epochs in 5.499s.  Loss: tr - 0.047, val - 0.042.  Accuracy: tr - 0.986, val - 0.986.\n",
      "Fold 9 trained 10 epochs in 5.784s.  Loss: tr - 0.044, val - 0.036.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Cross-validation completed in 55.32s. Mean validation loss 0.04 and acc 0.988\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:50:25,166 - [BO (140569664263600).INFO] -- evaluate takes 55.3237s\n",
      "05/15/2024 08:50:25,167 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:50:25,167 - [BO (140569664263600).INFO] -- #1 - fitness: 0.03972488716244697, solution: [0.0009475123849017082, 0.8625938444559781, 0.9959948008167309, 0.018162943615044443, 57, 3, 128, 0, 0]\n",
      "05/15/2024 08:50:25,187 - [BO (140569664263600).INFO] -- model r2: 0.6705602922923175, MAPE: 0.4296068907060984\n",
      "05/15/2024 08:50:25,188 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:50:25,188 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:50:25,188 - [BO (140569664263600).INFO] -- tell takes 0.0217s\n",
      "05/15/2024 08:50:25,189 - [BO (140569664263600).INFO] -- iteration 82 starts...\n",
      "05/15/2024 08:50:26,740 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5509s\n",
      "05/15/2024 08:50:26,741 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:50:26,741 - [BO (140569664263600).INFO] -- #1 - [0.0008332670395000965, 0.8683857612739239, 0.9925374843815302, 0.01853024387001081, 59, 4, 129, 0, 0]\n",
      "05/15/2024 08:50:26,741 - [BO (140569664263600).INFO] -- ask takes 1.5525s\n",
      "lr 0.0008, beta_1 0.8684, beta_2 0.9925, l2 0.0185, filters 59, kernel_sz 4,  dense_sz 129, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 7.464s.  Loss: tr - 0.084, val - 0.058.  Accuracy: tr - 0.971, val - 0.98.\n",
      "Fold 1 trained 10 epochs in 7.434s.  Loss: tr - 0.051, val - 0.042.  Accuracy: tr - 0.983, val - 0.988.\n",
      "Fold 2 trained 10 epochs in 7.484s.  Loss: tr - 0.08, val - 0.065.  Accuracy: tr - 0.972, val - 0.978.\n",
      "Fold 3 trained 10 epochs in 7.423s.  Loss: tr - 0.056, val - 0.062.  Accuracy: tr - 0.981, val - 0.979.\n",
      "Fold 4 trained 10 epochs in 7.758s.  Loss: tr - 0.153, val - 0.114.  Accuracy: tr - 0.942, val - 0.959.\n",
      "Fold 5 trained 10 epochs in 7.43s.  Loss: tr - 0.121, val - 0.095.  Accuracy: tr - 0.956, val - 0.967.\n",
      "Fold 6 trained 10 epochs in 7.49s.  Loss: tr - 0.054, val - 0.052.  Accuracy: tr - 0.981, val - 0.984.\n",
      "Fold 7 trained 10 epochs in 7.445s.  Loss: tr - 0.077, val - 0.075.  Accuracy: tr - 0.973, val - 0.969.\n",
      "Fold 8 trained 10 epochs in 7.452s.  Loss: tr - 0.149, val - 0.128.  Accuracy: tr - 0.945, val - 0.949.\n",
      "Fold 9 trained 10 epochs in 7.45s.  Loss: tr - 0.071, val - 0.068.  Accuracy: tr - 0.976, val - 0.977.\n",
      "Cross-validation completed in 74.834s. Mean validation loss 0.076 and acc 0.973\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:51:41,579 - [BO (140569664263600).INFO] -- evaluate takes 74.8374s\n",
      "05/15/2024 08:51:41,580 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:51:41,580 - [BO (140569664263600).INFO] -- #1 - fitness: 0.07589844018220901, solution: [0.0008332670395000965, 0.8683857612739239, 0.9925374843815302, 0.01853024387001081, 59, 4, 129, 0, 0]\n",
      "05/15/2024 08:51:41,600 - [BO (140569664263600).INFO] -- model r2: 0.714184183485105, MAPE: 0.41198339610749496\n",
      "05/15/2024 08:51:41,601 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:51:41,601 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:51:41,602 - [BO (140569664263600).INFO] -- tell takes 0.0222s\n",
      "05/15/2024 08:51:41,602 - [BO (140569664263600).INFO] -- iteration 83 starts...\n",
      "05/15/2024 08:51:46,692 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 5.0895s\n",
      "05/15/2024 08:51:46,693 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:51:46,693 - [BO (140569664263600).INFO] -- #1 - [0.0005188741350616576, 0.8695040698825368, 0.9975025891504403, 0.01953863885961851, 63, 3, 87, 0, 0]\n",
      "05/15/2024 08:51:46,693 - [BO (140569664263600).INFO] -- ask takes 5.0911s\n",
      "lr 0.0005, beta_1 0.8695, beta_2 0.9975, l2 0.0195, filters 63, kernel_sz 3,  dense_sz 87, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 6.151s.  Loss: tr - 0.051, val - 0.043.  Accuracy: tr - 0.981, val - 0.985.\n",
      "Fold 1 trained 10 epochs in 5.859s.  Loss: tr - 0.061, val - 0.058.  Accuracy: tr - 0.978, val - 0.98.\n",
      "Fold 2 trained 10 epochs in 5.873s.  Loss: tr - 0.039, val - 0.041.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 5.902s.  Loss: tr - 0.038, val - 0.045.  Accuracy: tr - 0.988, val - 0.984.\n",
      "Fold 4 trained 10 epochs in 5.9s.  Loss: tr - 0.049, val - 0.04.  Accuracy: tr - 0.985, val - 0.987.\n",
      "Fold 5 trained 10 epochs in 5.868s.  Loss: tr - 0.044, val - 0.042.  Accuracy: tr - 0.986, val - 0.985.\n",
      "Fold 6 trained 10 epochs in 6.089s.  Loss: tr - 0.057, val - 0.058.  Accuracy: tr - 0.981, val - 0.978.\n",
      "Fold 7 trained 10 epochs in 5.838s.  Loss: tr - 0.075, val - 0.064.  Accuracy: tr - 0.975, val - 0.975.\n",
      "Fold 8 trained 10 epochs in 5.904s.  Loss: tr - 0.098, val - 0.074.  Accuracy: tr - 0.966, val - 0.973.\n",
      "Fold 9 trained 10 epochs in 5.889s.  Loss: tr - 0.082, val - 0.075.  Accuracy: tr - 0.971, val - 0.974.\n",
      "Cross-validation completed in 59.278s. Mean validation loss 0.054 and acc 0.981\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:52:45,976 - [BO (140569664263600).INFO] -- evaluate takes 59.2820s\n",
      "05/15/2024 08:52:45,976 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:52:45,976 - [BO (140569664263600).INFO] -- #1 - fitness: 0.05391748696565628, solution: [0.0005188741350616576, 0.8695040698825368, 0.9975025891504403, 0.01953863885961851, 63, 3, 87, 0, 0]\n",
      "05/15/2024 08:52:45,999 - [BO (140569664263600).INFO] -- model r2: 0.7019662490636545, MAPE: 0.40750130591832795\n",
      "05/15/2024 08:52:46,00 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:52:46,01 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:52:46,01 - [BO (140569664263600).INFO] -- tell takes 0.0248s\n",
      "05/15/2024 08:52:46,01 - [BO (140569664263600).INFO] -- iteration 84 starts...\n",
      "05/15/2024 08:52:47,482 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.4812s\n",
      "05/15/2024 08:52:47,483 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:52:47,484 - [BO (140569664263600).INFO] -- #1 - [0.001245364122211722, 0.9073432967207, 0.991570440172644, 0.01369119813957902, 62, 5, 85, 0, 0]\n",
      "05/15/2024 08:52:47,484 - [BO (140569664263600).INFO] -- ask takes 1.4826s\n",
      "lr 0.0012, beta_1 0.9073, beta_2 0.9916, l2 0.0137, filters 62, kernel_sz 5,  dense_sz 85, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 4.829s.  Loss: tr - 0.413, val - 0.413.  Accuracy: tr - 0.815, val - 0.799.\n",
      "Fold 1 trained 10 epochs in 4.809s.  Loss: tr - 0.331, val - 0.269.  Accuracy: tr - 0.856, val - 0.9.\n",
      "Fold 2 trained 10 epochs in 5.101s.  Loss: tr - 0.161, val - 0.131.  Accuracy: tr - 0.935, val - 0.95.\n",
      "Fold 3 trained 10 epochs in 4.8s.  Loss: tr - 0.145, val - 0.131.  Accuracy: tr - 0.944, val - 0.953.\n",
      "Fold 4 trained 10 epochs in 4.83s.  Loss: tr - 0.395, val - 0.328.  Accuracy: tr - 0.817, val - 0.862.\n",
      "Fold 5 trained 10 epochs in 4.81s.  Loss: tr - 0.541, val - 0.499.  Accuracy: tr - 0.72, val - 0.763.\n",
      "Fold 6 trained 10 epochs in 4.775s.  Loss: tr - 0.383, val - 0.345.  Accuracy: tr - 0.828, val - 0.853.\n",
      "Fold 7 trained 10 epochs in 5.078s.  Loss: tr - 0.463, val - 0.42.  Accuracy: tr - 0.78, val - 0.815.\n",
      "Fold 8 trained 10 epochs in 4.822s.  Loss: tr - 0.391, val - 0.344.  Accuracy: tr - 0.826, val - 0.837.\n",
      "Fold 9 trained 10 epochs in 4.817s.  Loss: tr - 0.265, val - 0.238.  Accuracy: tr - 0.89, val - 0.903.\n",
      "Cross-validation completed in 48.676s. Mean validation loss 0.312 and acc 0.864\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:53:36,163 - [BO (140569664263600).INFO] -- evaluate takes 48.6789s\n",
      "05/15/2024 08:53:36,163 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:53:36,164 - [BO (140569664263600).INFO] -- #1 - fitness: 0.3117586374282837, solution: [0.001245364122211722, 0.9073432967207, 0.991570440172644, 0.01369119813957902, 62, 5, 85, 0, 0]\n",
      "05/15/2024 08:53:36,184 - [BO (140569664263600).INFO] -- model r2: 0.7757970069162419, MAPE: 0.3429451584693556\n",
      "05/15/2024 08:53:36,185 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:53:36,185 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:53:36,185 - [BO (140569664263600).INFO] -- tell takes 0.0219s\n",
      "05/15/2024 08:53:36,185 - [BO (140569664263600).INFO] -- iteration 85 starts...\n",
      "05/15/2024 08:53:37,899 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.7131s\n",
      "05/15/2024 08:53:37,900 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:53:37,900 - [BO (140569664263600).INFO] -- #1 - [0.0036736523059988316, 0.8660269288735409, 0.9921032184982354, 0.014859416215235653, 61, 3, 71, 0, 1]\n",
      "05/15/2024 08:53:37,900 - [BO (140569664263600).INFO] -- ask takes 1.7146s\n",
      "lr 0.0037, beta_1 0.866, beta_2 0.9921, l2 0.0149, filters 61, kernel_sz 3,  dense_sz 71, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.976s.  Loss: tr - 0.065, val - 0.059.  Accuracy: tr - 0.98, val - 0.979.\n",
      "Fold 1 trained 10 epochs in 6.987s.  Loss: tr - 0.659, val - 0.654.  Accuracy: tr - 0.592, val - 0.586.\n",
      "Fold 2 trained 10 epochs in 7.268s.  Loss: tr - 0.577, val - 0.491.  Accuracy: tr - 0.723, val - 0.794.\n",
      "Fold 3 trained 6 epochs in 4.194s.  Loss: tr - 34.029, val - 39.271.  Accuracy: tr - 0.561, val - 0.607.\n",
      "Fold 4 trained 6 epochs in 4.193s.  Loss: tr - 33.655, val - 40.8.  Accuracy: tr - 0.569, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 6.959s.  Loss: tr - 0.65, val - 0.637.  Accuracy: tr - 0.627, val - 0.622.\n",
      "Fold 6 trained 10 epochs in 6.99s.  Loss: tr - 0.504, val - 0.446.  Accuracy: tr - 0.777, val - 0.84.\n",
      "Fold 7 trained 6 epochs in 4.263s.  Loss: tr - 38.37, val - 46.05.  Accuracy: tr - 0.531, val - 0.54.\n",
      "Fold 8 trained 10 epochs in 6.933s.  Loss: tr - 0.645, val - 0.628.  Accuracy: tr - 0.624, val - 0.694.\n",
      "Fold 9 trained 6 epochs in 4.255s.  Loss: tr - 34.244, val - 42.966.  Accuracy: tr - 0.554, val - 0.57.\n",
      "Cross-validation completed in 59.022s. Mean validation loss 17.2 and acc 0.682\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:54:36,926 - [BO (140569664263600).INFO] -- evaluate takes 59.0257s\n",
      "05/15/2024 08:54:36,927 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:54:36,927 - [BO (140569664263600).INFO] -- #1 - fitness: 17.200381887331606, solution: [0.0036736523059988316, 0.8660269288735409, 0.9921032184982354, 0.014859416215235653, 61, 3, 71, 0, 1]\n",
      "05/15/2024 08:54:36,948 - [BO (140569664263600).INFO] -- model r2: 0.7608007363573108, MAPE: 0.4532397391381092\n",
      "05/15/2024 08:54:36,948 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:54:36,948 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:54:36,949 - [BO (140569664263600).INFO] -- tell takes 0.0221s\n",
      "05/15/2024 08:54:36,949 - [BO (140569664263600).INFO] -- iteration 86 starts...\n",
      "05/15/2024 08:54:38,475 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5254s\n",
      "05/15/2024 08:54:38,475 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:54:38,476 - [BO (140569664263600).INFO] -- #1 - [0.0004695209993558292, 0.8569311976362756, 0.9922717805409202, 0.01896852188128037, 62, 3, 137, 0, 1]\n",
      "05/15/2024 08:54:38,476 - [BO (140569664263600).INFO] -- ask takes 1.5270s\n",
      "lr 0.0005, beta_1 0.8569, beta_2 0.9923, l2 0.019, filters 62, kernel_sz 3,  dense_sz 137, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.489s.  Loss: tr - 0.035, val - 0.029.  Accuracy: tr - 0.99, val - 0.992.\n",
      "Fold 1 trained 10 epochs in 7.18s.  Loss: tr - 0.16, val - 0.124.  Accuracy: tr - 0.949, val - 0.959.\n",
      "Fold 2 trained 10 epochs in 7.193s.  Loss: tr - 0.04, val - 0.04.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 7.162s.  Loss: tr - 0.035, val - 0.035.  Accuracy: tr - 0.99, val - 0.989.\n",
      "Fold 4 trained 10 epochs in 7.163s.  Loss: tr - 0.033, val - 0.03.  Accuracy: tr - 0.99, val - 0.992.\n",
      "Fold 5 trained 10 epochs in 7.189s.  Loss: tr - 0.04, val - 0.042.  Accuracy: tr - 0.988, val - 0.99.\n",
      "Fold 6 trained 10 epochs in 7.504s.  Loss: tr - 0.054, val - 0.052.  Accuracy: tr - 0.985, val - 0.985.\n",
      "Fold 7 trained 10 epochs in 7.235s.  Loss: tr - 0.059, val - 0.054.  Accuracy: tr - 0.98, val - 0.983.\n",
      "Fold 8 trained 10 epochs in 7.18s.  Loss: tr - 0.043, val - 0.033.  Accuracy: tr - 0.987, val - 0.991.\n",
      "Fold 9 trained 10 epochs in 7.233s.  Loss: tr - 0.062, val - 0.062.  Accuracy: tr - 0.983, val - 0.983.\n",
      "Cross-validation completed in 72.532s. Mean validation loss 0.05 and acc 0.985\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:55:51,12 - [BO (140569664263600).INFO] -- evaluate takes 72.5353s\n",
      "05/15/2024 08:55:51,12 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:55:51,12 - [BO (140569664263600).INFO] -- #1 - fitness: 0.05015099737793207, solution: [0.0004695209993558292, 0.8569311976362756, 0.9922717805409202, 0.01896852188128037, 62, 3, 137, 0, 1]\n",
      "05/15/2024 08:55:51,32 - [BO (140569664263600).INFO] -- model r2: 0.7996496485161958, MAPE: 0.4873759292185762\n",
      "05/15/2024 08:55:51,33 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:55:51,33 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:55:51,34 - [BO (140569664263600).INFO] -- tell takes 0.0214s\n",
      "05/15/2024 08:55:51,34 - [BO (140569664263600).INFO] -- iteration 87 starts...\n",
      "05/15/2024 08:55:52,536 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.5016s\n",
      "05/15/2024 08:55:52,537 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:55:52,537 - [BO (140569664263600).INFO] -- #1 - [0.006045113002560612, 0.8840193540613411, 0.9987655476980796, 0.014458094107656786, 25, 4, 143, 1, 1]\n",
      "05/15/2024 08:55:52,537 - [BO (140569664263600).INFO] -- ask takes 1.5031s\n",
      "lr 0.006, beta_1 0.884, beta_2 0.9988, l2 0.0145, filters 25, kernel_sz 4,  dense_sz 143, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 4.999s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.588, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 5.249s.  Loss: tr - 0.662, val - 0.655.  Accuracy: tr - 0.582, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 5.048s.  Loss: tr - 0.666, val - 0.655.  Accuracy: tr - 0.589, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 5.026s.  Loss: tr - 0.662, val - 0.653.  Accuracy: tr - 0.585, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 5.015s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.587, val - 0.587.\n",
      "Fold 5 trained 10 epochs in 5.068s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.588, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 4.987s.  Loss: tr - 0.658, val - 0.662.  Accuracy: tr - 0.588, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.019s.  Loss: tr - 0.664, val - 0.664.  Accuracy: tr - 0.577, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.263s.  Loss: tr - 0.658, val - 0.658.  Accuracy: tr - 0.589, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 5.023s.  Loss: tr - 0.662, val - 0.662.  Accuracy: tr - 0.579, val - 0.589.\n",
      "Cross-validation completed in 50.701s. Mean validation loss 0.657 and acc 0.593\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:56:43,242 - [BO (140569664263600).INFO] -- evaluate takes 50.7044s\n",
      "05/15/2024 08:56:43,242 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:56:43,243 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6572026133537292, solution: [0.006045113002560612, 0.8840193540613411, 0.9987655476980796, 0.014458094107656786, 25, 4, 143, 1, 1]\n",
      "05/15/2024 08:56:43,263 - [BO (140569664263600).INFO] -- model r2: 0.7548203789690864, MAPE: 0.40437687808433626\n",
      "05/15/2024 08:56:43,264 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:56:43,264 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:56:43,264 - [BO (140569664263600).INFO] -- tell takes 0.0221s\n",
      "05/15/2024 08:56:43,265 - [BO (140569664263600).INFO] -- iteration 88 starts...\n",
      "05/15/2024 08:56:44,425 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.1598s\n",
      "05/15/2024 08:56:44,426 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:56:44,426 - [BO (140569664263600).INFO] -- #1 - [0.0010578303123482864, 0.8841698121986137, 0.9959574137417808, 0.013807619293601863, 20, 5, 128, 0, 0]\n",
      "05/15/2024 08:56:44,426 - [BO (140569664263600).INFO] -- ask takes 1.1612s\n",
      "lr 0.0011, beta_1 0.8842, beta_2 0.996, l2 0.0138, filters 20, kernel_sz 5,  dense_sz 128, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 3.593s.  Loss: tr - 0.172, val - 0.143.  Accuracy: tr - 0.932, val - 0.939.\n",
      "Fold 1 trained 10 epochs in 3.586s.  Loss: tr - 0.063, val - 0.05.  Accuracy: tr - 0.977, val - 0.987.\n",
      "Fold 2 trained 10 epochs in 3.613s.  Loss: tr - 0.14, val - 0.111.  Accuracy: tr - 0.951, val - 0.962.\n",
      "Fold 3 trained 10 epochs in 3.829s.  Loss: tr - 0.076, val - 0.073.  Accuracy: tr - 0.975, val - 0.977.\n",
      "Fold 4 trained 10 epochs in 3.569s.  Loss: tr - 0.343, val - 0.353.  Accuracy: tr - 0.854, val - 0.833.\n",
      "Fold 5 trained 10 epochs in 3.617s.  Loss: tr - 0.163, val - 0.129.  Accuracy: tr - 0.938, val - 0.951.\n",
      "Fold 6 trained 10 epochs in 3.558s.  Loss: tr - 0.257, val - 0.218.  Accuracy: tr - 0.893, val - 0.931.\n",
      "Fold 7 trained 10 epochs in 3.543s.  Loss: tr - 0.322, val - 0.284.  Accuracy: tr - 0.863, val - 0.882.\n",
      "Fold 8 trained 10 epochs in 3.894s.  Loss: tr - 0.242, val - 0.219.  Accuracy: tr - 0.903, val - 0.912.\n",
      "Fold 9 trained 10 epochs in 3.571s.  Loss: tr - 0.051, val - 0.05.  Accuracy: tr - 0.983, val - 0.982.\n",
      "Cross-validation completed in 36.377s. Mean validation loss 0.163 and acc 0.936\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:57:20,806 - [BO (140569664263600).INFO] -- evaluate takes 36.3800s\n",
      "05/15/2024 08:57:20,807 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:57:20,807 - [BO (140569664263600).INFO] -- #1 - fitness: 0.16310346722602845, solution: [0.0010578303123482864, 0.8841698121986137, 0.9959574137417808, 0.013807619293601863, 20, 5, 128, 0, 0]\n",
      "05/15/2024 08:57:20,827 - [BO (140569664263600).INFO] -- model r2: 0.7485810306178076, MAPE: 0.37506791882229995\n",
      "05/15/2024 08:57:20,827 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:57:20,828 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:57:20,828 - [BO (140569664263600).INFO] -- tell takes 0.0211s\n",
      "05/15/2024 08:57:20,828 - [BO (140569664263600).INFO] -- iteration 89 starts...\n",
      "05/15/2024 08:57:22,478 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.6490s\n",
      "05/15/2024 08:57:22,479 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:57:22,479 - [BO (140569664263600).INFO] -- #1 - [0.005671272616679943, 0.8808376903307644, 0.9965666974721487, 0.013759377482613654, 57, 3, 39, 1, 1]\n",
      "05/15/2024 08:57:22,479 - [BO (140569664263600).INFO] -- ask takes 1.6509s\n",
      "lr 0.0057, beta_1 0.8808, beta_2 0.9966, l2 0.0138, filters 57, kernel_sz 3,  dense_sz 39, activs tanh, padding 1\n",
      "Fold 0 trained 10 epochs in 6.724s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.589, val - 0.595.\n",
      "Fold 1 trained 10 epochs in 6.674s.  Loss: tr - 0.669, val - 0.657.  Accuracy: tr - 0.577, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 6.7s.  Loss: tr - 0.658, val - 0.654.  Accuracy: tr - 0.584, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 6.745s.  Loss: tr - 0.661, val - 0.65.  Accuracy: tr - 0.589, val - 0.607.\n",
      "Fold 4 trained 10 epochs in 6.905s.  Loss: tr - 0.661, val - 0.656.  Accuracy: tr - 0.591, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 6.63s.  Loss: tr - 0.659, val - 0.654.  Accuracy: tr - 0.585, val - 0.584.\n",
      "Fold 6 trained 10 epochs in 6.69s.  Loss: tr - 0.659, val - 0.661.  Accuracy: tr - 0.58, val - 0.578.\n",
      "Fold 7 trained 10 epochs in 6.715s.  Loss: tr - 0.656, val - 0.665.  Accuracy: tr - 0.593, val - 0.574.\n",
      "Fold 8 trained 10 epochs in 6.693s.  Loss: tr - 0.66, val - 0.657.  Accuracy: tr - 0.582, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 6.988s.  Loss: tr - 0.658, val - 0.66.  Accuracy: tr - 0.59, val - 0.589.\n",
      "Cross-validation completed in 67.468s. Mean validation loss 0.657 and acc 0.59\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:58:29,951 - [BO (140569664263600).INFO] -- evaluate takes 67.4715s\n",
      "05/15/2024 08:58:29,952 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:58:29,952 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6570921838283539, solution: [0.005671272616679943, 0.8808376903307644, 0.9965666974721487, 0.013759377482613654, 57, 3, 39, 1, 1]\n",
      "05/15/2024 08:58:29,972 - [BO (140569664263600).INFO] -- model r2: 0.7848227862016591, MAPE: 0.3804614145979088\n",
      "05/15/2024 08:58:29,972 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:58:29,973 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:58:29,973 - [BO (140569664263600).INFO] -- tell takes 0.0213s\n",
      "05/15/2024 08:58:29,973 - [BO (140569664263600).INFO] -- iteration 90 starts...\n",
      "05/15/2024 08:58:31,939 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.9660s\n",
      "05/15/2024 08:58:31,941 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:58:31,941 - [BO (140569664263600).INFO] -- #1 - [0.0045876172212967956, 0.9158471115423967, 0.9930544612672816, 0.000887493222162703, 63, 3, 117, 1, 0]\n",
      "05/15/2024 08:58:31,941 - [BO (140569664263600).INFO] -- ask takes 1.9677s\n",
      "lr 0.0046, beta_1 0.9158, beta_2 0.9931, l2 0.0009, filters 63, kernel_sz 3,  dense_sz 117, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 5.862s.  Loss: tr - 0.657, val - 0.656.  Accuracy: tr - 0.59, val - 0.596.\n",
      "Fold 1 trained 10 epochs in 5.865s.  Loss: tr - 0.663, val - 0.655.  Accuracy: tr - 0.585, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 5.904s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.583, val - 0.59.\n",
      "Fold 3 trained 10 epochs in 5.931s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.591, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 6.128s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Fold 5 trained 10 epochs in 5.864s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.588, val - 0.597.\n",
      "Fold 6 trained 10 epochs in 5.911s.  Loss: tr - 0.657, val - 0.661.  Accuracy: tr - 0.585, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 5.9s.  Loss: tr - 0.659, val - 0.665.  Accuracy: tr - 0.59, val - 0.59.\n",
      "Fold 8 trained 10 epochs in 5.879s.  Loss: tr - 0.658, val - 0.657.  Accuracy: tr - 0.592, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 6.179s.  Loss: tr - 0.657, val - 0.66.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Cross-validation completed in 59.426s. Mean validation loss 0.657 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 08:59:31,371 - [BO (140569664263600).INFO] -- evaluate takes 59.4292s\n",
      "05/15/2024 08:59:31,371 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 08:59:31,371 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6565070390701294, solution: [0.0045876172212967956, 0.9158471115423967, 0.9930544612672816, 0.000887493222162703, 63, 3, 117, 1, 0]\n",
      "05/15/2024 08:59:31,391 - [BO (140569664263600).INFO] -- model r2: 0.6556980930182419, MAPE: 0.3640743468518636\n",
      "05/15/2024 08:59:31,392 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 08:59:31,392 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 08:59:31,393 - [BO (140569664263600).INFO] -- tell takes 0.0216s\n",
      "05/15/2024 08:59:31,393 - [BO (140569664263600).INFO] -- iteration 91 starts...\n",
      "05/15/2024 08:59:33,504 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.1106s\n",
      "05/15/2024 08:59:33,505 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 08:59:33,505 - [BO (140569664263600).INFO] -- #1 - [0.0008124333236155357, 0.8611818691511033, 0.9937918464209066, 0.01682375697219933, 62, 3, 70, 0, 0]\n",
      "05/15/2024 08:59:33,505 - [BO (140569664263600).INFO] -- ask takes 2.1121s\n",
      "lr 0.0008, beta_1 0.8612, beta_2 0.9938, l2 0.0168, filters 62, kernel_sz 3,  dense_sz 70, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 5.821s.  Loss: tr - 0.067, val - 0.052.  Accuracy: tr - 0.978, val - 0.982.\n",
      "Fold 1 trained 10 epochs in 5.783s.  Loss: tr - 0.044, val - 0.036.  Accuracy: tr - 0.986, val - 0.989.\n",
      "Fold 2 trained 10 epochs in 5.8s.  Loss: tr - 0.05, val - 0.044.  Accuracy: tr - 0.982, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 5.83s.  Loss: tr - 0.054, val - 0.055.  Accuracy: tr - 0.981, val - 0.982.\n",
      "Fold 4 trained 10 epochs in 6.067s.  Loss: tr - 0.051, val - 0.043.  Accuracy: tr - 0.983, val - 0.987.\n",
      "Fold 5 trained 10 epochs in 5.812s.  Loss: tr - 0.045, val - 0.048.  Accuracy: tr - 0.985, val - 0.984.\n",
      "Fold 6 trained 10 epochs in 5.817s.  Loss: tr - 0.058, val - 0.057.  Accuracy: tr - 0.98, val - 0.98.\n",
      "Fold 7 trained 10 epochs in 5.802s.  Loss: tr - 0.077, val - 0.072.  Accuracy: tr - 0.973, val - 0.973.\n",
      "Fold 8 trained 10 epochs in 5.801s.  Loss: tr - 0.03, val - 0.031.  Accuracy: tr - 0.991, val - 0.99.\n",
      "Fold 9 trained 10 epochs in 5.793s.  Loss: tr - 0.039, val - 0.035.  Accuracy: tr - 0.989, val - 0.986.\n",
      "Cross-validation completed in 58.33s. Mean validation loss 0.047 and acc 0.984\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 09:00:31,839 - [BO (140569664263600).INFO] -- evaluate takes 58.3333s\n",
      "05/15/2024 09:00:31,839 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 09:00:31,840 - [BO (140569664263600).INFO] -- #1 - fitness: 0.047325773164629936, solution: [0.0008124333236155357, 0.8611818691511033, 0.9937918464209066, 0.01682375697219933, 62, 3, 70, 0, 0]\n",
      "05/15/2024 09:00:31,860 - [BO (140569664263600).INFO] -- model r2: 0.8354721190856148, MAPE: 0.36015919645030525\n",
      "05/15/2024 09:00:31,860 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 09:00:31,861 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 09:00:31,861 - [BO (140569664263600).INFO] -- tell takes 0.0218s\n",
      "05/15/2024 09:00:31,861 - [BO (140569664263600).INFO] -- iteration 92 starts...\n",
      "05/15/2024 09:00:34,474 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.6124s\n",
      "05/15/2024 09:00:34,475 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 09:00:34,475 - [BO (140569664263600).INFO] -- #1 - [0.0006558553508468192, 0.8951208836732512, 0.9968852566107117, 0.01823032304083946, 62, 3, 116, 0, 0]\n",
      "05/15/2024 09:00:34,475 - [BO (140569664263600).INFO] -- ask takes 2.6139s\n",
      "lr 0.0007, beta_1 0.8951, beta_2 0.9969, l2 0.0182, filters 62, kernel_sz 3,  dense_sz 116, activs relu, padding 0\n",
      "Fold 0 trained 10 epochs in 6.098s.  Loss: tr - 0.036, val - 0.027.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 1 trained 10 epochs in 5.835s.  Loss: tr - 0.06, val - 0.054.  Accuracy: tr - 0.978, val - 0.979.\n",
      "Fold 2 trained 10 epochs in 5.804s.  Loss: tr - 0.038, val - 0.039.  Accuracy: tr - 0.988, val - 0.99.\n",
      "Fold 3 trained 10 epochs in 5.79s.  Loss: tr - 0.038, val - 0.043.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 5.851s.  Loss: tr - 0.041, val - 0.036.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Fold 5 trained 10 epochs in 5.805s.  Loss: tr - 0.043, val - 0.044.  Accuracy: tr - 0.985, val - 0.985.\n",
      "Fold 6 trained 10 epochs in 5.798s.  Loss: tr - 0.042, val - 0.048.  Accuracy: tr - 0.986, val - 0.983.\n",
      "Fold 7 trained 10 epochs in 6.07s.  Loss: tr - 0.041, val - 0.045.  Accuracy: tr - 0.985, val - 0.983.\n",
      "Fold 8 trained 10 epochs in 5.803s.  Loss: tr - 0.068, val - 0.054.  Accuracy: tr - 0.977, val - 0.979.\n",
      "Fold 9 trained 10 epochs in 5.788s.  Loss: tr - 0.038, val - 0.034.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Cross-validation completed in 58.646s. Mean validation loss 0.042 and acc 0.986\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 09:01:33,125 - [BO (140569664263600).INFO] -- evaluate takes 58.6495s\n",
      "05/15/2024 09:01:33,126 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 09:01:33,126 - [BO (140569664263600).INFO] -- #1 - fitness: 0.04249421078711748, solution: [0.0006558553508468192, 0.8951208836732512, 0.9968852566107117, 0.01823032304083946, 62, 3, 116, 0, 0]\n",
      "05/15/2024 09:01:33,146 - [BO (140569664263600).INFO] -- model r2: 0.8112652034576529, MAPE: 0.4661591190923107\n",
      "05/15/2024 09:01:33,147 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 09:01:33,147 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 09:01:33,147 - [BO (140569664263600).INFO] -- tell takes 0.0217s\n",
      "05/15/2024 09:01:33,147 - [BO (140569664263600).INFO] -- iteration 93 starts...\n",
      "05/15/2024 09:01:34,619 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 1.4711s\n",
      "05/15/2024 09:01:34,620 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 09:01:34,620 - [BO (140569664263600).INFO] -- #1 - [0.0015165211655149208, 0.9259322559496487, 0.9914160783749887, 0.018986377782669088, 63, 4, 57, 1, 0]\n",
      "05/15/2024 09:01:34,620 - [BO (140569664263600).INFO] -- ask takes 1.4727s\n",
      "lr 0.0015, beta_1 0.9259, beta_2 0.9914, l2 0.019, filters 63, kernel_sz 4,  dense_sz 57, activs tanh, padding 0\n",
      "Fold 0 trained 10 epochs in 7.779s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.597.\n",
      "Fold 1 trained 10 epochs in 7.793s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "Fold 2 trained 10 epochs in 8.066s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Fold 3 trained 10 epochs in 7.839s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.59, val - 0.599.\n",
      "Fold 4 trained 10 epochs in 7.784s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "Fold 5 trained 10 epochs in 7.864s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.594, val - 0.594.\n",
      "Fold 6 trained 10 epochs in 7.846s.  Loss: tr - 0.655, val - 0.658.  Accuracy: tr - 0.623, val - 0.591.\n",
      "Fold 7 trained 10 epochs in 7.849s.  Loss: tr - 0.66, val - 0.664.  Accuracy: tr - 0.575, val - 0.587.\n",
      "Fold 8 trained 10 epochs in 7.862s.  Loss: tr - 0.656, val - 0.657.  Accuracy: tr - 0.59, val - 0.601.\n",
      "Fold 9 trained 10 epochs in 8.052s.  Loss: tr - 0.656, val - 0.659.  Accuracy: tr - 0.593, val - 0.592.\n",
      "Cross-validation completed in 78.739s. Mean validation loss 0.656 and acc 0.594\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 09:02:53,363 - [BO (140569664263600).INFO] -- evaluate takes 78.7423s\n",
      "05/15/2024 09:02:53,363 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 09:02:53,364 - [BO (140569664263600).INFO] -- #1 - fitness: 0.6557829856872559, solution: [0.0015165211655149208, 0.9259322559496487, 0.9914160783749887, 0.018986377782669088, 63, 4, 57, 1, 0]\n",
      "05/15/2024 09:02:53,384 - [BO (140569664263600).INFO] -- model r2: 0.7563510554893409, MAPE: 0.38126062806079164\n",
      "05/15/2024 09:02:53,384 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 09:02:53,385 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 09:02:53,385 - [BO (140569664263600).INFO] -- tell takes 0.0218s\n",
      "05/15/2024 09:02:53,385 - [BO (140569664263600).INFO] -- iteration 94 starts...\n",
      "05/15/2024 09:02:56,212 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.8261s\n",
      "05/15/2024 09:02:56,213 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 09:02:56,213 - [BO (140569664263600).INFO] -- #1 - [0.0037235861174043107, 0.8757523748508487, 0.9980433867264414, 0.019826713843510146, 36, 3, 82, 0, 1]\n",
      "05/15/2024 09:02:56,213 - [BO (140569664263600).INFO] -- ask takes 2.8278s\n",
      "lr 0.0037, beta_1 0.8758, beta_2 0.998, l2 0.0198, filters 36, kernel_sz 3,  dense_sz 82, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.135s.  Loss: tr - 0.08, val - 0.05.  Accuracy: tr - 0.971, val - 0.985.\n",
      "Fold 1 trained 10 epochs in 6.127s.  Loss: tr - 0.039, val - 0.034.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 6.169s.  Loss: tr - 0.039, val - 0.04.  Accuracy: tr - 0.988, val - 0.989.\n",
      "Fold 3 trained 10 epochs in 6.149s.  Loss: tr - 0.166, val - 0.099.  Accuracy: tr - 0.936, val - 0.965.\n",
      "Fold 4 trained 10 epochs in 6.439s.  Loss: tr - 0.614, val - 0.622.  Accuracy: tr - 0.711, val - 0.664.\n",
      "Fold 5 trained 10 epochs in 6.125s.  Loss: tr - 0.416, val - 0.396.  Accuracy: tr - 0.828, val - 0.815.\n",
      "Fold 6 trained 10 epochs in 6.172s.  Loss: tr - 0.409, val - 0.291.  Accuracy: tr - 0.821, val - 0.878.\n",
      "Fold 7 trained 10 epochs in 6.14s.  Loss: tr - 0.425, val - 0.207.  Accuracy: tr - 0.826, val - 0.932.\n",
      "Fold 8 trained 10 epochs in 6.252s.  Loss: tr - 0.244, val - 0.152.  Accuracy: tr - 0.893, val - 0.954.\n",
      "Fold 9 trained 10 epochs in 6.4s.  Loss: tr - 0.109, val - 0.095.  Accuracy: tr - 0.958, val - 0.97.\n",
      "Cross-validation completed in 62.113s. Mean validation loss 0.199 and acc 0.914\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 09:03:58,330 - [BO (140569664263600).INFO] -- evaluate takes 62.1159s\n",
      "05/15/2024 09:03:58,330 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 09:03:58,330 - [BO (140569664263600).INFO] -- #1 - fitness: 0.19857103638350965, solution: [0.0037235861174043107, 0.8757523748508487, 0.9980433867264414, 0.019826713843510146, 36, 3, 82, 0, 1]\n",
      "05/15/2024 09:03:58,352 - [BO (140569664263600).INFO] -- model r2: 0.7554792524500819, MAPE: 0.4333832256798997\n",
      "05/15/2024 09:03:58,352 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 09:03:58,352 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 09:03:58,353 - [BO (140569664263600).INFO] -- tell takes 0.0227s\n",
      "05/15/2024 09:03:58,353 - [BO (140569664263600).INFO] -- iteration 95 starts...\n",
      "05/15/2024 09:04:01,69 - [BO (140569664263600).INFO] -- arg_max_acquisition takes 2.7156s\n",
      "05/15/2024 09:04:01,70 - [BO (140569664263600).INFO] -- asking 1 points:\n",
      "05/15/2024 09:04:01,70 - [BO (140569664263600).INFO] -- #1 - [0.0005851270228556794, 0.864835702828946, 0.9958744341431747, 0.01793590617575298, 61, 3, 64, 0, 1]\n",
      "05/15/2024 09:04:01,71 - [BO (140569664263600).INFO] -- ask takes 2.7174s\n",
      "lr 0.0006, beta_1 0.8648, beta_2 0.9959, l2 0.0179, filters 61, kernel_sz 3,  dense_sz 64, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.031s.  Loss: tr - 0.049, val - 0.04.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 1 trained 10 epochs in 7.029s.  Loss: tr - 0.04, val - 0.032.  Accuracy: tr - 0.988, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 6.968s.  Loss: tr - 0.089, val - 0.068.  Accuracy: tr - 0.969, val - 0.978.\n",
      "Fold 3 trained 10 epochs in 6.982s.  Loss: tr - 0.104, val - 0.087.  Accuracy: tr - 0.963, val - 0.969.\n",
      "Fold 4 trained 10 epochs in 7.275s.  Loss: tr - 0.054, val - 0.044.  Accuracy: tr - 0.982, val - 0.984.\n",
      "Fold 5 trained 10 epochs in 7.011s.  Loss: tr - 0.036, val - 0.037.  Accuracy: tr - 0.988, val - 0.991.\n",
      "Fold 6 trained 10 epochs in 7.065s.  Loss: tr - 0.085, val - 0.072.  Accuracy: tr - 0.969, val - 0.973.\n",
      "Fold 7 trained 10 epochs in 6.977s.  Loss: tr - 0.095, val - 0.065.  Accuracy: tr - 0.967, val - 0.978.\n",
      "Fold 8 trained 10 epochs in 7.008s.  Loss: tr - 0.023, val - 0.024.  Accuracy: tr - 0.994, val - 0.993.\n",
      "Fold 9 trained 10 epochs in 7.251s.  Loss: tr - 0.061, val - 0.051.  Accuracy: tr - 0.98, val - 0.984.\n",
      "Cross-validation completed in 70.602s. Mean validation loss 0.052 and acc 0.983\n",
      "---------------------------------------------------------------------------\n",
      "05/15/2024 09:05:11,676 - [BO (140569664263600).INFO] -- evaluate takes 70.6051s\n",
      "05/15/2024 09:05:11,676 - [BO (140569664263600).INFO] -- observing 1 points:\n",
      "05/15/2024 09:05:11,677 - [BO (140569664263600).INFO] -- #1 - fitness: 0.052049780078232286, solution: [0.0005851270228556794, 0.864835702828946, 0.9958744341431747, 0.01793590617575298, 61, 3, 64, 0, 1]\n",
      "05/15/2024 09:05:11,697 - [BO (140569664263600).INFO] -- model r2: 0.8067846956225125, MAPE: 0.3270834934590634\n",
      "05/15/2024 09:05:11,697 - [BO (140569664263600).INFO] -- fopt: [0.03487469]\n",
      "05/15/2024 09:05:11,698 - [BO (140569664263600).INFO] -- xopt: [0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1]\n",
      "\n",
      "05/15/2024 09:05:11,698 - [BO (140569664263600).INFO] -- tell takes 0.0215s\n",
      "Bayes optimization takes 5684.78 seconds to tune 100 configurations\n",
      "<bayes_optim.bayes_opt.BO object at 0x7fd8ece795b0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.now().strftime( \"%Y%m%d-%H%M%S\")\n",
    "\n",
    "space = (\n",
    "    RealSpace([0.0001, 0.01], var_name='lr')\n",
    "    + RealSpace([0.85, 0.95], var_name='beta_1')\n",
    "    + RealSpace([0.99, 0.9999], var_name='beta_2')\n",
    "    + RealSpace([0.0005, 0.02], var_name='l2')\n",
    "    + IntegerSpace([16, 64], var_name='filters')\n",
    "    + IntegerSpace([2, 5], var_name='kernel_sz')\n",
    "    + IntegerSpace([25, 150], var_name='dense_sz')\n",
    "    + IntegerSpace([0, 1], var_name='activs')\n",
    "    + IntegerSpace([0, 1], var_name='padding')\n",
    "    # + RealSpace([0.0005, 0.002], var_name='dense_l2')\n",
    ")\n",
    "\n",
    "bo_model = BO_RF(n_estimators=25, levels={})\n",
    "\n",
    "MAX_CONF = 100\n",
    "optimizer = BO(\n",
    "    search_space=space,\n",
    "    obj_fun=train_model,\n",
    "    verbose=True,\n",
    "    DoE_size=5,\n",
    "    max_FEs=MAX_CONF,\n",
    "    random_seed=1,\n",
    "    model=bo_model,\n",
    "    data_file=os.path.join(BAYES_LOGS, f'BO_{now}')\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "optimizer.run()\n",
    "end = time.time()\n",
    "\n",
    "print('Bayes optimization takes {:.2f} seconds to tune {} configurations'.format(end - start, MAX_CONF))\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0008378693402342123,\n",
       " 'beta_1': 0.8978616699507355,\n",
       " 'beta_2': 0.9947658989213924,\n",
       " 'l2': 0.019890688370674918,\n",
       " 'filters': 58,\n",
       " 'kernel_sz': 3,\n",
       " 'dense_sz': 141,\n",
       " 'activs': 0,\n",
       " 'padding': 1}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.xopt.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.0008378693402342123, 0.8978616699507355, 0.9947658989213924, 0.019890688370674918, 58, 3, 141, 0, 1])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr, beta_1, beta_2, filters, kernel_sz, dense_sz, activs, padding, l2\n",
    "bayes_opt_params = optimizer.xopt.to_dict().values()\n",
    "bayes_opt_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRACE_LOGS = os.path.join(OPT_LOGS, 'f_race')\n",
    "if not os.path.exists(FRACE_LOGS):\n",
    "    os.makedirs(FRACE_LOGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Select number of candidates to test (Sobol sequence? Skip first 32 sequences if 20 candidates selected)\n",
    "- Execute one step of evaluation on each candidate, and rank results\n",
    "- Apply Friedman test for rank-variance to determine significant difference in groups\n",
    "    - If appropriate, apply Wilcoxon signed rank test post-hoc to test candidate pairs (Bonferoni correction, Holm step-down, None?)\n",
    "    - Discard significantly weaker candidates (All candidate configurations that result significantly worse than the best one are discarded, where best is determined by lowest expected rank)\n",
    "- After all steps / computational budget - number of iterations `L = 2 + round(log2(d)) = 5`, filter remaining candidate set down to min number, if exceeding\n",
    "    - Suggested minimum: `2 + round(log2(d)) = 5`\n",
    "- Rank remaining candidates, assign weight proportional to rank\n",
    "- Sample next set of candidates around elite, with distribution for each param space:\n",
    "    - Config around `Xz = (xz1, xz2, ..., xzm)` with `m` the number of parameters, where `Xz` is one elite candidate\n",
    "    - Normal distribution around mean `xzi`\n",
    "    - Std Dev is `vi . (1 / (N+1))^(l/d)` with `vi` = range of param `i`, `l` is the iteration counter, `N` is number of candidates to sample, and `d` is number of components of a configuration (`=m`)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare, wilcoxon\n",
    "from scipy.stats.qmc import Sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {\n",
    "    0: F.relu,\n",
    "    1: F.tanh\n",
    "}\n",
    "N_MIN = 5\n",
    "ALPHA = 0.05\n",
    "\n",
    "def train_one_fold(model_args, train_loader, val_loader, epochs=10, pat=3, verbose=-1):\n",
    "    lr, beta_1, beta_2, l2, filters, kernel_sz, dense_sz, activs, padding, sz = model_args\n",
    "    print(f\"lr {round(lr, 4)}, beta_1 {round(beta_1, 4)}, beta_2 {round(beta_2, 4)}, l2 {round(l2, 4)}, filters {filters}, kernel_sz {kernel_sz}, \",\n",
    "          f\"dense_sz {dense_sz}, activs {activations[activs].__name__}, padding {padding}\")\n",
    "      \n",
    "    model = HyperModel(in_size=sz, filts=filters, kerns=kernel_sz, pad=padding,\n",
    "                        activ=activations[activs], dense_sz=dense_sz, beta_1=beta_1, beta_2=beta_2, lr=lr, l2=l2)\n",
    "    history = model.train(train_loader, val_loader, epochs=epochs, patience=pat, verbose=verbose)\n",
    "    \n",
    "    ce = model.conv_epoch\n",
    "    t_loss, t_acc = history['train_loss'][ce], history['train_acc'][ce]\n",
    "    loss, acc = history['val_loss'][ce], history['val_acc'][ce]\n",
    "    \n",
    "    return model.trained_epochs, t_loss, loss, t_acc, acc\n",
    "\n",
    "def train_models_frace(model_configs: dict, loss_history: dict, acc_history: dict, #lr, beta_1, beta_2, filters, kernel_sz, dense_sz, activs, padding,\n",
    "                x_data=train_X, y_data=train_Y, batch_sz=1024, epochs=10, pat=3, verbose=-1):\n",
    "    # lr, beta_1, beta_2, filters, kernel_sz, dense_sz, activs, padding = model_args\n",
    "    \n",
    "    kfold = KFold(10, shuffle=True, random_state=1)\n",
    "    \n",
    "    accs, losses = {}, {}\n",
    "    for key in model_configs:\n",
    "        accs[key] = acc_history[key] if key in acc_history else []\n",
    "        losses[key] = loss_history[key] if key in loss_history else []\n",
    "    \n",
    "    start_t = time.time()\n",
    "    for  f, (train_ix, val_ix) in enumerate(kfold.split(x_data)):\n",
    "        trainX, trainY, valX, valY = x_data[train_ix], y_data[train_ix], x_data[val_ix], y_data[val_ix]\n",
    "        train_loader = myDataLoader(trainX, trainY, batch_sz=batch_sz)\n",
    "        val_loader = myDataLoader(valX, valY, batch_sz=batch_sz)\n",
    "        print(f\"Training fold {f} on list of {len(model_configs)} configurations.\")\n",
    "              \n",
    "        for (config_idx, model_args) in model_configs.items():\n",
    "            if len(losses[config_idx]) > f:\n",
    "                print(f\"Configuration {config_idx}, evaluated once before -- skipping.\")\n",
    "                continue\n",
    "            f_t = time.time()        \n",
    "            # model_args.append(x_data.shape[1:])\n",
    "            trained, t_loss, loss, t_acc, acc = train_one_fold(model_args + [x_data.shape[1:]], train_loader, val_loader,\n",
    "                                                               epochs=epochs, pat=pat, verbose=verbose)\n",
    "            \n",
    "            print(f\"Configuration {config_idx}, fold {f} trained {trained} epochs in {round(time.time() - f_t, 3)}s. \",\n",
    "                    f\"Loss: tr - {round(t_loss, 3)}, val - {round(loss, 3)}. \",\n",
    "                    f\"Accuracy: tr - {round(t_acc, 3)}, val - {round(acc, 3)}.\")\n",
    "            accs[config_idx].append(acc)\n",
    "            losses[config_idx].append(loss)\n",
    "            \n",
    "        # perform Friedman tests on losses to determine if configs are dropped\n",
    "        df = pd.DataFrame({key: pd.Series(value) for key, value in losses.items()}).dropna()\n",
    "        F, pval_friedman = friedmanchisquare(*df.to_numpy().T)\n",
    "        if pval_friedman < ALPHA:\n",
    "            print(f\"Significant difference in validation losses across group, after {f+1} folds ({pval_friedman})\")\n",
    "            # TODO: perform Wilcoxon test pairing highest rank configuration with the rest\n",
    "            means = df.apply('mean', axis=0).sort_values()\n",
    "            ranks = means.index\n",
    "            best_config = df[ranks[0]]\n",
    "            other_configs = df[ranks[1:]]\n",
    "        \n",
    "            alpha_adj = 1-pow(1-ALPHA, 1. / other_configs.shape[1]) # Sidak correction\n",
    "            for conf in other_configs:\n",
    "                W, pval_wilcoxon = wilcoxon(best_config, other_configs[conf], alternative='less')\n",
    "                # if alpha_adj < pval_wilcoxon < ALPHA:\n",
    "                #     print(f\"Configuration {conf} (mean vloss {means[conf]}) pvalue {pval_wilcoxon} against configuration {ranks[0]} (mean vloss {means[ranks[0]]}) -- inbetween alpha and adjusted alpha.\")\n",
    "                # if pval_wilcoxon < alpha_adj:\n",
    "                if pval_wilcoxon < ALPHA:\n",
    "                    # Drop configuration from further testing\n",
    "                    print(f\"Configuration {conf} (mean vloss {means[conf]}) dropped with pvalue {pval_wilcoxon} against configuration {ranks[0]} (mean vloss {means[ranks[0]]})\")\n",
    "                    model_configs.pop(conf)\n",
    "                    losses.pop(conf)\n",
    "                    accs.pop(conf)\n",
    "            print(\"Paired tests completed\")\n",
    "        else:\n",
    "            print(f\"Not enough evidence to support a difference across group, after {f+1} folds ({pval_friedman})\")\n",
    "\n",
    "        if len(model_configs) <= N_MIN:\n",
    "            break\n",
    "        print('-'*100)\n",
    "        \n",
    "    print(f\"Iteration completed in {round(time.time() - start_t, 3)}s. Remaining configurations {len(model_configs)}. \",\n",
    "          f\"Mean validation loss {np.round(np.mean(list(losses.values()), axis=None), 3)} and acc {np.round(np.mean(list(accs.values()), axis=None), 3)}\")\n",
    "    print('-'*75)\n",
    "    return model_configs, losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04867147199809551"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(loss_history.values()), axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_records = []\n",
    "scores_records = []\n",
    "configs_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Iteration 1 of Iterated F-race.\n",
      "Training fold 0 on list of 30 configurations.\n",
      "lr 0.0078, beta_1 0.9415, beta_2 0.9903, l2 0.0179, filters 50, kernel_sz 3,  dense_sz 59, activs tanh, padding 0\n",
      "Configuration 0, fold 0 trained 10 epochs in 5.048s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.589, val - 0.596.\n",
      "lr 0.0042, beta_1 0.8754, beta_2 0.9993, l2 0.0024, filters 38, kernel_sz 5,  dense_sz 88, activs relu, padding 1\n",
      "Configuration 1, fold 0 trained 10 epochs in 4.356s.  Loss: tr - 0.648, val - 0.639.  Accuracy: tr - 0.616, val - 0.68.\n",
      "lr 0.0022, beta_1 0.9068, beta_2 0.9946, l2 0.0109, filters 24, kernel_sz 4,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 2, fold 0 trained 10 epochs in 2.727s.  Loss: tr - 0.459, val - 0.41.  Accuracy: tr - 0.787, val - 0.777.\n",
      "lr 0.006, beta_1 0.8728, beta_2 0.995, l2 0.0093, filters 60, kernel_sz 3,  dense_sz 147, activs tanh, padding 0\n",
      "Configuration 3, fold 0 trained 10 epochs in 5.589s.  Loss: tr - 0.66, val - 0.658.  Accuracy: tr - 0.585, val - 0.595.\n",
      "lr 0.0073, beta_1 0.9183, beta_2 0.9975, l2 0.0066, filters 57, kernel_sz 3,  dense_sz 26, activs relu, padding 1\n",
      "Configuration 4, fold 0 trained 6 epochs in 3.933s.  Loss: tr - 35.542, val - 49.389.  Accuracy: tr - 0.543, val - 0.506.\n",
      "lr 0.0012, beta_1 0.8523, beta_2 0.9921, l2 0.0142, filters 21, kernel_sz 4,  dense_sz 122, activs tanh, padding 0\n",
      "Configuration 5, fold 0 trained 10 epochs in 3.59s.  Loss: tr - 0.299, val - 0.266.  Accuracy: tr - 0.902, val - 0.914.\n",
      "lr 0.0028, beta_1 0.9334, beta_2 0.9969, l2 0.0032, filters 29, kernel_sz 4,  dense_sz 85, activs tanh, padding 0\n",
      "Configuration 6, fold 0 trained 10 epochs in 4.881s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.009, beta_1 0.8995, beta_2 0.9927, l2 0.0175, filters 40, kernel_sz 2,  dense_sz 119, activs relu, padding 1\n",
      "Configuration 7, fold 0 trained 6 epochs in 2.526s.  Loss: tr - 33.447, val - 43.359.  Accuracy: tr - 0.542, val - 0.566.\n",
      "lr 0.0096, beta_1 0.9009, beta_2 0.9966, l2 0.0134, filters 35, kernel_sz 2,  dense_sz 36, activs relu, padding 1\n",
      "Configuration 8, fold 0 trained 10 epochs in 3.967s.  Loss: tr - 0.643, val - 0.629.  Accuracy: tr - 0.608, val - 0.647.\n",
      "lr 0.0034, beta_1 0.8662, beta_2 0.9936, l2 0.0069, filters 46, kernel_sz 4,  dense_sz 128, activs tanh, padding 0\n",
      "Configuration 9, fold 0 trained 10 epochs in 6.127s.  Loss: tr - 0.657, val - 0.656.  Accuracy: tr - 0.593, val - 0.595.\n",
      "lr 0.0005, beta_1 0.9473, beta_2 0.9984, l2 0.0154, filters 63, kernel_sz 5,  dense_sz 79, activs tanh, padding 0\n",
      "Configuration 10, fold 0 trained 10 epochs in 4.811s.  Loss: tr - 0.426, val - 0.395.  Accuracy: tr - 0.853, val - 0.87.\n",
      "lr 0.0067, beta_1 0.8821, beta_2 0.9918, l2 0.0048, filters 28, kernel_sz 3,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 11, fold 0 trained 10 epochs in 4.878s.  Loss: tr - 0.61, val - 0.58.  Accuracy: tr - 0.624, val - 0.664.\n",
      "lr 0.0054, beta_1 0.9276, beta_2 0.9942, l2 0.0008, filters 18, kernel_sz 2,  dense_sz 64, activs tanh, padding 0\n",
      "Configuration 12, fold 0 trained 10 epochs in 2.158s.  Loss: tr - 0.128, val - 0.106.  Accuracy: tr - 0.973, val - 0.978.\n",
      "lr 0.0016, beta_1 0.8928, beta_2 0.996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 13, fold 0 trained 10 epochs in 8.29s.  Loss: tr - 0.044, val - 0.042.  Accuracy: tr - 0.985, val - 0.986.\n",
      "lr 0.0048, beta_1 0.9241, beta_2 0.9912, l2 0.009, filters 44, kernel_sz 5,  dense_sz 45, activs relu, padding 1\n",
      "Configuration 14, fold 0 trained 6 epochs in 2.879s.  Loss: tr - 42.359, val - 50.407.  Accuracy: tr - 0.48, val - 0.496.\n",
      "lr 0.0085, beta_1 0.859, beta_2 0.9991, l2 0.0117, filters 31, kernel_sz 3,  dense_sz 141, activs tanh, padding 0\n",
      "Configuration 15, fold 0 trained 10 epochs in 4.256s.  Loss: tr - 0.665, val - 0.657.  Accuracy: tr - 0.58, val - 0.596.\n",
      "lr 0.0084, beta_1 0.9189, beta_2 0.9934, l2 0.0039, filters 59, kernel_sz 4,  dense_sz 95, activs relu, padding 0\n",
      "Configuration 16, fold 0 trained 6 epochs in 4.456s.  Loss: tr - 34.266, val - 41.832.  Accuracy: tr - 0.573, val - 0.582.\n",
      "lr 0.0047, beta_1 0.8607, beta_2 0.9963, l2 0.0164, filters 22, kernel_sz 2,  dense_sz 62, activs tanh, padding 1\n",
      "Configuration 17, fold 0 trained 10 epochs in 2.261s.  Loss: tr - 0.228, val - 0.213.  Accuracy: tr - 0.968, val - 0.969.\n",
      "lr 0.0017, beta_1 0.9294, beta_2 0.9915, l2 0.0054, filters 39, kernel_sz 3,  dense_sz 145, activs tanh, padding 1\n",
      "Configuration 18, fold 0 trained 10 epochs in 7.242s.  Loss: tr - 0.183, val - 0.163.  Accuracy: tr - 0.967, val - 0.972.\n",
      "lr 0.0054, beta_1 0.8875, beta_2 0.9981, l2 0.0149, filters 52, kernel_sz 5,  dense_sz 50, activs relu, padding 0\n",
      "Configuration 19, fold 0 trained 10 epochs in 4.612s.  Loss: tr - 2.462, val - 0.667.  Accuracy: tr - 0.569, val - 0.584.\n",
      "lr 0.0065, beta_1 0.9456, beta_2 0.9956, l2 0.0108, filters 42, kernel_sz 4,  dense_sz 123, activs tanh, padding 1\n",
      "Configuration 20, fold 0 trained 10 epochs in 6.755s.  Loss: tr - 0.658, val - 0.657.  Accuracy: tr - 0.588, val - 0.588.\n",
      "lr 0.0003, beta_1 0.8873, beta_2 0.994, l2 0.0099, filters 31, kernel_sz 2,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 21, fold 0 trained 10 epochs in 2.625s.  Loss: tr - 0.397, val - 0.396.  Accuracy: tr - 0.862, val - 0.805.\n",
      "lr 0.0036, beta_1 0.9062, beta_2 0.9987, l2 0.0185, filters 20, kernel_sz 3,  dense_sz 111, activs relu, padding 0\n",
      "Configuration 22, fold 0 trained 10 epochs in 1.863s.  Loss: tr - 0.085, val - 0.068.  Accuracy: tr - 0.972, val - 0.977.\n",
      "lr 0.0098, beta_1 0.8644, beta_2 0.9909, l2 0.0023, filters 55, kernel_sz 5,  dense_sz 83, activs tanh, padding 1\n",
      "Configuration 23, fold 0 trained 10 epochs in 5.842s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.587, val - 0.596.\n",
      "lr 0.0092, beta_1 0.9345, beta_2 0.9997, l2 0.0078, filters 26, kernel_sz 4,  dense_sz 133, activs tanh, padding 1\n",
      "Configuration 24, fold 0 trained 10 epochs in 5.05s.  Loss: tr - 0.661, val - 0.657.  Accuracy: tr - 0.585, val - 0.595.\n",
      "lr 0.003, beta_1 0.8949, beta_2 0.9905, l2 0.0124, filters 62, kernel_sz 3,  dense_sz 37, activs relu, padding 0\n",
      "Configuration 25, fold 0 trained 10 epochs in 6.026s.  Loss: tr - 0.319, val - 0.232.  Accuracy: tr - 0.873, val - 0.907.\n",
      "lr 0.0009, beta_1 0.9138, beta_2 0.9953, l2 0.0015, filters 48, kernel_sz 2,  dense_sz 105, activs relu, padding 0\n",
      "Configuration 26, fold 0 trained 10 epochs in 3.971s.  Loss: tr - 0.071, val - 0.055.  Accuracy: tr - 0.977, val - 0.984.\n",
      "lr 0.0071, beta_1 0.8533, beta_2 0.9948, l2 0.0188, filters 36, kernel_sz 4,  dense_sz 73, activs tanh, padding 1\n",
      "Configuration 27, fold 0 trained 10 epochs in 6.059s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.595.\n",
      "lr 0.006, beta_1 0.9113, beta_2 0.9924, l2 0.016, filters 33, kernel_sz 5,  dense_sz 100, activs relu, padding 0\n",
      "Configuration 28, fold 0 trained 10 epochs in 4.746s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.588, val - 0.596.\n",
      "lr 0.0023, beta_1 0.8718, beta_2 0.9978, l2 0.0047, filters 46, kernel_sz 3,  dense_sz 72, activs tanh, padding 1\n",
      "Configuration 29, fold 0 trained 10 epochs in 5.573s.  Loss: tr - 0.658, val - 0.656.  Accuracy: tr - 0.591, val - 0.588.\n",
      "Not enough evidence to support a difference across group, after 1 folds (0.46506624123787865)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 1 on list of 30 configurations.\n",
      "lr 0.0078, beta_1 0.9415, beta_2 0.9903, l2 0.0179, filters 50, kernel_sz 3,  dense_sz 59, activs tanh, padding 0\n",
      "Configuration 0, fold 1 trained 10 epochs in 5.069s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "lr 0.0042, beta_1 0.8754, beta_2 0.9993, l2 0.0024, filters 38, kernel_sz 5,  dense_sz 88, activs relu, padding 1\n",
      "Configuration 1, fold 1 trained 10 epochs in 4.671s.  Loss: tr - 0.682, val - 0.681.  Accuracy: tr - 0.566, val - 0.561.\n",
      "lr 0.0022, beta_1 0.9068, beta_2 0.9946, l2 0.0109, filters 24, kernel_sz 4,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 2, fold 1 trained 10 epochs in 2.726s.  Loss: tr - 0.039, val - 0.033.  Accuracy: tr - 0.989, val - 0.988.\n",
      "lr 0.006, beta_1 0.8728, beta_2 0.995, l2 0.0093, filters 60, kernel_sz 3,  dense_sz 147, activs tanh, padding 0\n",
      "Configuration 3, fold 1 trained 10 epochs in 5.627s.  Loss: tr - 0.703, val - 0.655.  Accuracy: tr - 0.525, val - 0.593.\n",
      "lr 0.0073, beta_1 0.9183, beta_2 0.9975, l2 0.0066, filters 57, kernel_sz 3,  dense_sz 26, activs relu, padding 1\n",
      "Configuration 4, fold 1 trained 6 epochs in 3.964s.  Loss: tr - 35.355, val - 43.181.  Accuracy: tr - 0.539, val - 0.568.\n",
      "lr 0.0012, beta_1 0.8523, beta_2 0.9921, l2 0.0142, filters 21, kernel_sz 4,  dense_sz 122, activs tanh, padding 0\n",
      "Configuration 5, fold 1 trained 10 epochs in 3.639s.  Loss: tr - 0.328, val - 0.284.  Accuracy: tr - 0.879, val - 0.91.\n",
      "lr 0.0028, beta_1 0.9334, beta_2 0.9969, l2 0.0032, filters 29, kernel_sz 4,  dense_sz 85, activs tanh, padding 0\n",
      "Configuration 6, fold 1 trained 10 epochs in 4.634s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.586.\n",
      "lr 0.009, beta_1 0.8995, beta_2 0.9927, l2 0.0175, filters 40, kernel_sz 2,  dense_sz 119, activs relu, padding 1\n",
      "Configuration 7, fold 1 trained 10 epochs in 4.21s.  Loss: tr - 0.617, val - 0.564.  Accuracy: tr - 0.657, val - 0.718.\n",
      "lr 0.0096, beta_1 0.9009, beta_2 0.9966, l2 0.0134, filters 35, kernel_sz 2,  dense_sz 36, activs relu, padding 1\n",
      "Configuration 8, fold 1 trained 10 epochs in 4.277s.  Loss: tr - 0.65, val - 0.639.  Accuracy: tr - 0.606, val - 0.641.\n",
      "lr 0.0034, beta_1 0.8662, beta_2 0.9936, l2 0.0069, filters 46, kernel_sz 4,  dense_sz 128, activs tanh, padding 0\n",
      "Configuration 9, fold 1 trained 10 epochs in 6.127s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.585, val - 0.592.\n",
      "lr 0.0005, beta_1 0.9473, beta_2 0.9984, l2 0.0154, filters 63, kernel_sz 5,  dense_sz 79, activs tanh, padding 0\n",
      "Configuration 10, fold 1 trained 10 epochs in 4.808s.  Loss: tr - 0.291, val - 0.268.  Accuracy: tr - 0.937, val - 0.952.\n",
      "lr 0.0067, beta_1 0.8821, beta_2 0.9918, l2 0.0048, filters 28, kernel_sz 3,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 11, fold 1 trained 6 epochs in 2.933s.  Loss: tr - 33.857, val - 41.349.  Accuracy: tr - 0.568, val - 0.587.\n",
      "lr 0.0054, beta_1 0.9276, beta_2 0.9942, l2 0.0008, filters 18, kernel_sz 2,  dense_sz 64, activs tanh, padding 0\n",
      "Configuration 12, fold 1 trained 10 epochs in 1.897s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.0016, beta_1 0.8928, beta_2 0.996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 13, fold 1 trained 10 epochs in 8.317s.  Loss: tr - 0.044, val - 0.038.  Accuracy: tr - 0.985, val - 0.989.\n",
      "lr 0.0048, beta_1 0.9241, beta_2 0.9912, l2 0.009, filters 44, kernel_sz 5,  dense_sz 45, activs relu, padding 1\n",
      "Configuration 14, fold 1 trained 6 epochs in 2.886s.  Loss: tr - 40.729, val - 49.593.  Accuracy: tr - 0.507, val - 0.504.\n",
      "lr 0.0085, beta_1 0.859, beta_2 0.9991, l2 0.0117, filters 31, kernel_sz 3,  dense_sz 141, activs tanh, padding 0\n",
      "Configuration 15, fold 1 trained 10 epochs in 4.536s.  Loss: tr - 0.673, val - 0.655.  Accuracy: tr - 0.554, val - 0.592.\n",
      "lr 0.0084, beta_1 0.9189, beta_2 0.9934, l2 0.0039, filters 59, kernel_sz 4,  dense_sz 95, activs relu, padding 0\n",
      "Configuration 16, fold 1 trained 6 epochs in 4.457s.  Loss: tr - 31.066, val - 40.789.  Accuracy: tr - 0.562, val - 0.592.\n",
      "lr 0.0047, beta_1 0.8607, beta_2 0.9963, l2 0.0164, filters 22, kernel_sz 2,  dense_sz 62, activs tanh, padding 1\n",
      "Configuration 17, fold 1 trained 10 epochs in 2.255s.  Loss: tr - 0.574, val - 0.48.  Accuracy: tr - 0.743, val - 0.749.\n",
      "lr 0.0017, beta_1 0.9294, beta_2 0.9915, l2 0.0054, filters 39, kernel_sz 3,  dense_sz 145, activs tanh, padding 1\n",
      "Configuration 18, fold 1 trained 10 epochs in 7.276s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.585, val - 0.592.\n",
      "lr 0.0054, beta_1 0.8875, beta_2 0.9981, l2 0.0149, filters 52, kernel_sz 5,  dense_sz 50, activs relu, padding 0\n",
      "Configuration 19, fold 1 trained 10 epochs in 4.348s.  Loss: tr - 0.676, val - 0.653.  Accuracy: tr - 0.573, val - 0.588.\n",
      "lr 0.0065, beta_1 0.9456, beta_2 0.9956, l2 0.0108, filters 42, kernel_sz 4,  dense_sz 123, activs tanh, padding 1\n",
      "Configuration 20, fold 1 trained 10 epochs in 6.775s.  Loss: tr - 0.66, val - 0.655.  Accuracy: tr - 0.59, val - 0.593.\n",
      "lr 0.0003, beta_1 0.8873, beta_2 0.994, l2 0.0099, filters 31, kernel_sz 2,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 21, fold 1 trained 10 epochs in 2.915s.  Loss: tr - 0.407, val - 0.374.  Accuracy: tr - 0.848, val - 0.853.\n",
      "lr 0.0036, beta_1 0.9062, beta_2 0.9987, l2 0.0185, filters 20, kernel_sz 3,  dense_sz 111, activs relu, padding 0\n",
      "Configuration 22, fold 1 trained 10 epochs in 1.845s.  Loss: tr - 0.085, val - 0.062.  Accuracy: tr - 0.969, val - 0.981.\n",
      "lr 0.0098, beta_1 0.8644, beta_2 0.9909, l2 0.0023, filters 55, kernel_sz 5,  dense_sz 83, activs tanh, padding 1\n",
      "Configuration 23, fold 1 trained 10 epochs in 5.838s.  Loss: tr - 0.659, val - 0.657.  Accuracy: tr - 0.584, val - 0.587.\n",
      "lr 0.0092, beta_1 0.9345, beta_2 0.9997, l2 0.0078, filters 26, kernel_sz 4,  dense_sz 133, activs tanh, padding 1\n",
      "Configuration 24, fold 1 trained 10 epochs in 5.056s.  Loss: tr - 0.665, val - 0.658.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.003, beta_1 0.8949, beta_2 0.9905, l2 0.0124, filters 62, kernel_sz 3,  dense_sz 37, activs relu, padding 0\n",
      "Configuration 25, fold 1 trained 10 epochs in 5.766s.  Loss: tr - 0.337, val - 0.396.  Accuracy: tr - 0.854, val - 0.8.\n",
      "lr 0.0009, beta_1 0.9138, beta_2 0.9953, l2 0.0015, filters 48, kernel_sz 2,  dense_sz 105, activs relu, padding 0\n",
      "Configuration 26, fold 1 trained 10 epochs in 3.983s.  Loss: tr - 0.124, val - 0.093.  Accuracy: tr - 0.958, val - 0.967.\n",
      "lr 0.0071, beta_1 0.8533, beta_2 0.9948, l2 0.0188, filters 36, kernel_sz 4,  dense_sz 73, activs tanh, padding 1\n",
      "Configuration 27, fold 1 trained 10 epochs in 6.365s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.585, val - 0.592.\n",
      "lr 0.006, beta_1 0.9113, beta_2 0.9924, l2 0.016, filters 33, kernel_sz 5,  dense_sz 100, activs relu, padding 0\n",
      "Configuration 28, fold 1 trained 10 epochs in 4.746s.  Loss: tr - 0.694, val - 0.679.  Accuracy: tr - 0.563, val - 0.583.\n",
      "lr 0.0023, beta_1 0.8718, beta_2 0.9978, l2 0.0047, filters 46, kernel_sz 3,  dense_sz 72, activs tanh, padding 1\n",
      "Configuration 29, fold 1 trained 10 epochs in 5.574s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.589, val - 0.592.\n",
      "Significant difference in validation losses across group, after 2 folds (0.01562960884774828)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 2 on list of 30 configurations.\n",
      "lr 0.0078, beta_1 0.9415, beta_2 0.9903, l2 0.0179, filters 50, kernel_sz 3,  dense_sz 59, activs tanh, padding 0\n",
      "Configuration 0, fold 2 trained 10 epochs in 5.021s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.591, val - 0.591.\n",
      "lr 0.0042, beta_1 0.8754, beta_2 0.9993, l2 0.0024, filters 38, kernel_sz 5,  dense_sz 88, activs relu, padding 1\n",
      "Configuration 1, fold 2 trained 10 epochs in 4.407s.  Loss: tr - 0.657, val - 0.653.  Accuracy: tr - 0.596, val - 0.6.\n",
      "lr 0.0022, beta_1 0.9068, beta_2 0.9946, l2 0.0109, filters 24, kernel_sz 4,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 2, fold 2 trained 10 epochs in 2.709s.  Loss: tr - 0.039, val - 0.048.  Accuracy: tr - 0.987, val - 0.987.\n",
      "lr 0.006, beta_1 0.8728, beta_2 0.995, l2 0.0093, filters 60, kernel_sz 3,  dense_sz 147, activs tanh, padding 0\n",
      "Configuration 3, fold 2 trained 10 epochs in 5.912s.  Loss: tr - 0.665, val - 0.657.  Accuracy: tr - 0.583, val - 0.591.\n",
      "lr 0.0073, beta_1 0.9183, beta_2 0.9975, l2 0.0066, filters 57, kernel_sz 3,  dense_sz 26, activs relu, padding 1\n",
      "Configuration 4, fold 2 trained 6 epochs in 3.956s.  Loss: tr - 34.141, val - 40.979.  Accuracy: tr - 0.574, val - 0.59.\n",
      "lr 0.0012, beta_1 0.8523, beta_2 0.9921, l2 0.0142, filters 21, kernel_sz 4,  dense_sz 122, activs tanh, padding 0\n",
      "Configuration 5, fold 2 trained 10 epochs in 3.624s.  Loss: tr - 0.579, val - 0.487.  Accuracy: tr - 0.687, val - 0.763.\n",
      "lr 0.0028, beta_1 0.9334, beta_2 0.9969, l2 0.0032, filters 29, kernel_sz 4,  dense_sz 85, activs tanh, padding 0\n",
      "Configuration 6, fold 2 trained 10 epochs in 4.645s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.593, val - 0.591.\n",
      "lr 0.009, beta_1 0.8995, beta_2 0.9927, l2 0.0175, filters 40, kernel_sz 2,  dense_sz 119, activs relu, padding 1\n",
      "Configuration 7, fold 2 trained 6 epochs in 2.534s.  Loss: tr - 31.719, val - 40.928.  Accuracy: tr - 0.561, val - 0.591.\n",
      "lr 0.0096, beta_1 0.9009, beta_2 0.9966, l2 0.0134, filters 35, kernel_sz 2,  dense_sz 36, activs relu, padding 1\n",
      "Configuration 8, fold 2 trained 10 epochs in 4.014s.  Loss: tr - 0.652, val - 0.636.  Accuracy: tr - 0.607, val - 0.614.\n",
      "lr 0.0034, beta_1 0.8662, beta_2 0.9936, l2 0.0069, filters 46, kernel_sz 4,  dense_sz 128, activs tanh, padding 0\n",
      "Configuration 9, fold 2 trained 10 epochs in 6.112s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0005, beta_1 0.9473, beta_2 0.9984, l2 0.0154, filters 63, kernel_sz 5,  dense_sz 79, activs tanh, padding 0\n",
      "Configuration 10, fold 2 trained 10 epochs in 5.117s.  Loss: tr - 0.341, val - 0.315.  Accuracy: tr - 0.887, val - 0.907.\n",
      "lr 0.0067, beta_1 0.8821, beta_2 0.9918, l2 0.0048, filters 28, kernel_sz 3,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 11, fold 2 trained 10 epochs in 4.848s.  Loss: tr - 0.602, val - 0.565.  Accuracy: tr - 0.685, val - 0.729.\n",
      "lr 0.0054, beta_1 0.9276, beta_2 0.9942, l2 0.0008, filters 18, kernel_sz 2,  dense_sz 64, activs tanh, padding 0\n",
      "Configuration 12, fold 2 trained 10 epochs in 1.912s.  Loss: tr - 0.154, val - 0.122.  Accuracy: tr - 0.961, val - 0.973.\n",
      "lr 0.0016, beta_1 0.8928, beta_2 0.996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 13, fold 2 trained 10 epochs in 8.293s.  Loss: tr - 0.069, val - 0.059.  Accuracy: tr - 0.976, val - 0.982.\n",
      "lr 0.0048, beta_1 0.9241, beta_2 0.9912, l2 0.009, filters 44, kernel_sz 5,  dense_sz 45, activs relu, padding 1\n",
      "Configuration 14, fold 2 trained 6 epochs in 2.897s.  Loss: tr - 35.067, val - 43.272.  Accuracy: tr - 0.555, val - 0.567.\n",
      "lr 0.0085, beta_1 0.859, beta_2 0.9991, l2 0.0117, filters 31, kernel_sz 3,  dense_sz 141, activs tanh, padding 0\n",
      "Configuration 15, fold 2 trained 10 epochs in 4.263s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.591, val - 0.59.\n",
      "lr 0.0084, beta_1 0.9189, beta_2 0.9934, l2 0.0039, filters 59, kernel_sz 4,  dense_sz 95, activs relu, padding 0\n",
      "Configuration 16, fold 2 trained 6 epochs in 4.446s.  Loss: tr - 47.973, val - 56.728.  Accuracy: tr - 0.449, val - 0.433.\n",
      "lr 0.0047, beta_1 0.8607, beta_2 0.9963, l2 0.0164, filters 22, kernel_sz 2,  dense_sz 62, activs tanh, padding 1\n",
      "Configuration 17, fold 2 trained 10 epochs in 2.542s.  Loss: tr - 0.607, val - 0.583.  Accuracy: tr - 0.689, val - 0.668.\n",
      "lr 0.0017, beta_1 0.9294, beta_2 0.9915, l2 0.0054, filters 39, kernel_sz 3,  dense_sz 145, activs tanh, padding 1\n",
      "Configuration 18, fold 2 trained 10 epochs in 7.234s.  Loss: tr - 0.077, val - 0.076.  Accuracy: tr - 0.991, val - 0.989.\n",
      "lr 0.0054, beta_1 0.8875, beta_2 0.9981, l2 0.0149, filters 52, kernel_sz 5,  dense_sz 50, activs relu, padding 0\n",
      "Configuration 19, fold 2 trained 6 epochs in 2.598s.  Loss: tr - 36.159, val - 46.381.  Accuracy: tr - 0.541, val - 0.536.\n",
      "lr 0.0065, beta_1 0.9456, beta_2 0.9956, l2 0.0108, filters 42, kernel_sz 4,  dense_sz 123, activs tanh, padding 1\n",
      "Configuration 20, fold 2 trained 10 epochs in 6.739s.  Loss: tr - 0.659, val - 0.654.  Accuracy: tr - 0.591, val - 0.59.\n",
      "lr 0.0003, beta_1 0.8873, beta_2 0.994, l2 0.0099, filters 31, kernel_sz 2,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 21, fold 2 trained 10 epochs in 2.709s.  Loss: tr - 0.391, val - 0.349.  Accuracy: tr - 0.872, val - 0.894.\n",
      "lr 0.0036, beta_1 0.9062, beta_2 0.9987, l2 0.0185, filters 20, kernel_sz 3,  dense_sz 111, activs relu, padding 0\n",
      "Configuration 22, fold 2 trained 10 epochs in 1.881s.  Loss: tr - 0.081, val - 0.073.  Accuracy: tr - 0.968, val - 0.977.\n",
      "lr 0.0098, beta_1 0.8644, beta_2 0.9909, l2 0.0023, filters 55, kernel_sz 5,  dense_sz 83, activs tanh, padding 1\n",
      "Configuration 23, fold 2 trained 10 epochs in 6.136s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.586, val - 0.583.\n",
      "lr 0.0092, beta_1 0.9345, beta_2 0.9997, l2 0.0078, filters 26, kernel_sz 4,  dense_sz 133, activs tanh, padding 1\n",
      "Configuration 24, fold 2 trained 10 epochs in 5.058s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.592, val - 0.59.\n",
      "lr 0.003, beta_1 0.8949, beta_2 0.9905, l2 0.0124, filters 62, kernel_sz 3,  dense_sz 37, activs relu, padding 0\n",
      "Configuration 25, fold 2 trained 10 epochs in 5.749s.  Loss: tr - 0.644, val - 0.632.  Accuracy: tr - 0.647, val - 0.604.\n",
      "lr 0.0009, beta_1 0.9138, beta_2 0.9953, l2 0.0015, filters 48, kernel_sz 2,  dense_sz 105, activs relu, padding 0\n",
      "Configuration 26, fold 2 trained 10 epochs in 3.981s.  Loss: tr - 0.124, val - 0.096.  Accuracy: tr - 0.959, val - 0.967.\n",
      "lr 0.0071, beta_1 0.8533, beta_2 0.9948, l2 0.0188, filters 36, kernel_sz 4,  dense_sz 73, activs tanh, padding 1\n",
      "Configuration 27, fold 2 trained 10 epochs in 6.055s.  Loss: tr - 0.661, val - 0.654.  Accuracy: tr - 0.594, val - 0.59.\n",
      "lr 0.006, beta_1 0.9113, beta_2 0.9924, l2 0.016, filters 33, kernel_sz 5,  dense_sz 100, activs relu, padding 0\n",
      "Configuration 28, fold 2 trained 10 epochs in 4.761s.  Loss: tr - 0.911, val - 0.663.  Accuracy: tr - 0.561, val - 0.567.\n",
      "lr 0.0023, beta_1 0.8718, beta_2 0.9978, l2 0.0047, filters 46, kernel_sz 3,  dense_sz 72, activs tanh, padding 1\n",
      "Configuration 29, fold 2 trained 10 epochs in 5.87s.  Loss: tr - 0.661, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "Significant difference in validation losses across group, after 3 folds (1.4482062671987782e-05)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 3 on list of 30 configurations.\n",
      "lr 0.0078, beta_1 0.9415, beta_2 0.9903, l2 0.0179, filters 50, kernel_sz 3,  dense_sz 59, activs tanh, padding 0\n",
      "Configuration 0, fold 3 trained 10 epochs in 5.085s.  Loss: tr - 0.661, val - 0.649.  Accuracy: tr - 0.592, val - 0.607.\n",
      "lr 0.0042, beta_1 0.8754, beta_2 0.9993, l2 0.0024, filters 38, kernel_sz 5,  dense_sz 88, activs relu, padding 1\n",
      "Configuration 1, fold 3 trained 6 epochs in 2.64s.  Loss: tr - 35.304, val - 41.157.  Accuracy: tr - 0.557, val - 0.588.\n",
      "lr 0.0022, beta_1 0.9068, beta_2 0.9946, l2 0.0109, filters 24, kernel_sz 4,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 2, fold 3 trained 10 epochs in 2.725s.  Loss: tr - 0.443, val - 0.389.  Accuracy: tr - 0.784, val - 0.812.\n",
      "lr 0.006, beta_1 0.8728, beta_2 0.995, l2 0.0093, filters 60, kernel_sz 3,  dense_sz 147, activs tanh, padding 0\n",
      "Configuration 3, fold 3 trained 10 epochs in 5.619s.  Loss: tr - 0.664, val - 0.651.  Accuracy: tr - 0.587, val - 0.593.\n",
      "lr 0.0073, beta_1 0.9183, beta_2 0.9975, l2 0.0066, filters 57, kernel_sz 3,  dense_sz 26, activs relu, padding 1\n",
      "Configuration 4, fold 3 trained 6 epochs in 3.98s.  Loss: tr - 39.215, val - 47.426.  Accuracy: tr - 0.518, val - 0.526.\n",
      "lr 0.0012, beta_1 0.8523, beta_2 0.9921, l2 0.0142, filters 21, kernel_sz 4,  dense_sz 122, activs tanh, padding 0\n",
      "Configuration 5, fold 3 trained 10 epochs in 3.639s.  Loss: tr - 0.177, val - 0.179.  Accuracy: tr - 0.959, val - 0.954.\n",
      "lr 0.0028, beta_1 0.9334, beta_2 0.9969, l2 0.0032, filters 29, kernel_sz 4,  dense_sz 85, activs tanh, padding 0\n",
      "Configuration 6, fold 3 trained 10 epochs in 4.925s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.009, beta_1 0.8995, beta_2 0.9927, l2 0.0175, filters 40, kernel_sz 2,  dense_sz 119, activs relu, padding 1\n",
      "Configuration 7, fold 3 trained 10 epochs in 4.226s.  Loss: tr - 0.648, val - 0.63.  Accuracy: tr - 0.614, val - 0.722.\n",
      "lr 0.0096, beta_1 0.9009, beta_2 0.9966, l2 0.0134, filters 35, kernel_sz 2,  dense_sz 36, activs relu, padding 1\n",
      "Configuration 8, fold 3 trained 10 epochs in 3.984s.  Loss: tr - 0.645, val - 0.632.  Accuracy: tr - 0.62, val - 0.619.\n",
      "lr 0.0034, beta_1 0.8662, beta_2 0.9936, l2 0.0069, filters 46, kernel_sz 4,  dense_sz 128, activs tanh, padding 0\n",
      "Configuration 9, fold 3 trained 10 epochs in 6.118s.  Loss: tr - 0.658, val - 0.651.  Accuracy: tr - 0.588, val - 0.599.\n",
      "lr 0.0005, beta_1 0.9473, beta_2 0.9984, l2 0.0154, filters 63, kernel_sz 5,  dense_sz 79, activs tanh, padding 0\n",
      "Configuration 10, fold 3 trained 10 epochs in 4.823s.  Loss: tr - 0.235, val - 0.229.  Accuracy: tr - 0.97, val - 0.974.\n",
      "lr 0.0067, beta_1 0.8821, beta_2 0.9918, l2 0.0048, filters 28, kernel_sz 3,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 11, fold 3 trained 10 epochs in 4.894s.  Loss: tr - 0.581, val - 0.513.  Accuracy: tr - 0.693, val - 0.779.\n",
      "lr 0.0054, beta_1 0.9276, beta_2 0.9942, l2 0.0008, filters 18, kernel_sz 2,  dense_sz 64, activs tanh, padding 0\n",
      "Configuration 12, fold 3 trained 10 epochs in 1.917s.  Loss: tr - 0.079, val - 0.074.  Accuracy: tr - 0.984, val - 0.984.\n",
      "lr 0.0016, beta_1 0.8928, beta_2 0.996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 13, fold 3 trained 10 epochs in 8.572s.  Loss: tr - 0.068, val - 0.051.  Accuracy: tr - 0.976, val - 0.983.\n",
      "lr 0.0048, beta_1 0.9241, beta_2 0.9912, l2 0.009, filters 44, kernel_sz 5,  dense_sz 45, activs relu, padding 1\n",
      "Configuration 14, fold 3 trained 6 epochs in 2.871s.  Loss: tr - 33.203, val - 39.271.  Accuracy: tr - 0.56, val - 0.607.\n",
      "lr 0.0085, beta_1 0.859, beta_2 0.9991, l2 0.0117, filters 31, kernel_sz 3,  dense_sz 141, activs tanh, padding 0\n",
      "Configuration 15, fold 3 trained 10 epochs in 4.242s.  Loss: tr - 0.661, val - 0.65.  Accuracy: tr - 0.583, val - 0.607.\n",
      "lr 0.0084, beta_1 0.9189, beta_2 0.9934, l2 0.0039, filters 59, kernel_sz 4,  dense_sz 95, activs relu, padding 0\n",
      "Configuration 16, fold 3 trained 6 epochs in 4.446s.  Loss: tr - 39.14, val - 46.662.  Accuracy: tr - 0.535, val - 0.533.\n",
      "lr 0.0047, beta_1 0.8607, beta_2 0.9963, l2 0.0164, filters 22, kernel_sz 2,  dense_sz 62, activs tanh, padding 1\n",
      "Configuration 17, fold 3 trained 10 epochs in 2.237s.  Loss: tr - 0.651, val - 0.638.  Accuracy: tr - 0.635, val - 0.646.\n",
      "lr 0.0017, beta_1 0.9294, beta_2 0.9915, l2 0.0054, filters 39, kernel_sz 3,  dense_sz 145, activs tanh, padding 1\n",
      "Configuration 18, fold 3 trained 10 epochs in 7.254s.  Loss: tr - 0.083, val - 0.082.  Accuracy: tr - 0.989, val - 0.987.\n",
      "lr 0.0054, beta_1 0.8875, beta_2 0.9981, l2 0.0149, filters 52, kernel_sz 5,  dense_sz 50, activs relu, padding 0\n",
      "Configuration 19, fold 3 trained 6 epochs in 2.586s.  Loss: tr - 42.735, val - 48.853.  Accuracy: tr - 0.492, val - 0.511.\n",
      "lr 0.0065, beta_1 0.9456, beta_2 0.9956, l2 0.0108, filters 42, kernel_sz 4,  dense_sz 123, activs tanh, padding 1\n",
      "Configuration 20, fold 3 trained 10 epochs in 7.036s.  Loss: tr - 0.66, val - 0.651.  Accuracy: tr - 0.586, val - 0.599.\n",
      "lr 0.0003, beta_1 0.8873, beta_2 0.994, l2 0.0099, filters 31, kernel_sz 2,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 21, fold 3 trained 10 epochs in 2.656s.  Loss: tr - 0.379, val - 0.33.  Accuracy: tr - 0.874, val - 0.897.\n",
      "lr 0.0036, beta_1 0.9062, beta_2 0.9987, l2 0.0185, filters 20, kernel_sz 3,  dense_sz 111, activs relu, padding 0\n",
      "Configuration 22, fold 3 trained 10 epochs in 1.868s.  Loss: tr - 0.123, val - 0.112.  Accuracy: tr - 0.957, val - 0.961.\n",
      "lr 0.0098, beta_1 0.8644, beta_2 0.9909, l2 0.0023, filters 55, kernel_sz 5,  dense_sz 83, activs tanh, padding 1\n",
      "Configuration 23, fold 3 trained 10 epochs in 5.818s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.584, val - 0.607.\n",
      "lr 0.0092, beta_1 0.9345, beta_2 0.9997, l2 0.0078, filters 26, kernel_sz 4,  dense_sz 133, activs tanh, padding 1\n",
      "Configuration 24, fold 3 trained 10 epochs in 5.042s.  Loss: tr - 0.664, val - 0.651.  Accuracy: tr - 0.578, val - 0.588.\n",
      "lr 0.003, beta_1 0.8949, beta_2 0.9905, l2 0.0124, filters 62, kernel_sz 3,  dense_sz 37, activs relu, padding 0\n",
      "Configuration 25, fold 3 trained 10 epochs in 5.781s.  Loss: tr - 0.677, val - 0.675.  Accuracy: tr - 0.575, val - 0.58.\n",
      "lr 0.0009, beta_1 0.9138, beta_2 0.9953, l2 0.0015, filters 48, kernel_sz 2,  dense_sz 105, activs relu, padding 0\n",
      "Configuration 26, fold 3 trained 10 epochs in 4.272s.  Loss: tr - 0.042, val - 0.042.  Accuracy: tr - 0.986, val - 0.987.\n",
      "lr 0.0071, beta_1 0.8533, beta_2 0.9948, l2 0.0188, filters 36, kernel_sz 4,  dense_sz 73, activs tanh, padding 1\n",
      "Configuration 27, fold 3 trained 10 epochs in 6.042s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.587, val - 0.599.\n",
      "lr 0.006, beta_1 0.9113, beta_2 0.9924, l2 0.016, filters 33, kernel_sz 5,  dense_sz 100, activs relu, padding 0\n",
      "Configuration 28, fold 3 trained 10 epochs in 4.752s.  Loss: tr - 0.661, val - 0.654.  Accuracy: tr - 0.591, val - 0.599.\n",
      "lr 0.0023, beta_1 0.8718, beta_2 0.9978, l2 0.0047, filters 46, kernel_sz 3,  dense_sz 72, activs tanh, padding 1\n",
      "Configuration 29, fold 3 trained 10 epochs in 5.583s.  Loss: tr - 0.664, val - 0.651.  Accuracy: tr - 0.586, val - 0.593.\n",
      "Significant difference in validation losses across group, after 4 folds (9.214600678142277e-09)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 4 on list of 30 configurations.\n",
      "lr 0.0078, beta_1 0.9415, beta_2 0.9903, l2 0.0179, filters 50, kernel_sz 3,  dense_sz 59, activs tanh, padding 0\n",
      "Configuration 0, fold 4 trained 10 epochs in 5.014s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.592.\n",
      "lr 0.0042, beta_1 0.8754, beta_2 0.9993, l2 0.0024, filters 38, kernel_sz 5,  dense_sz 88, activs relu, padding 1\n",
      "Configuration 1, fold 4 trained 10 epochs in 4.403s.  Loss: tr - 0.656, val - 0.651.  Accuracy: tr - 0.605, val - 0.621.\n",
      "lr 0.0022, beta_1 0.9068, beta_2 0.9946, l2 0.0109, filters 24, kernel_sz 4,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 2, fold 4 trained 10 epochs in 3.016s.  Loss: tr - 0.145, val - 0.095.  Accuracy: tr - 0.943, val - 0.965.\n",
      "lr 0.006, beta_1 0.8728, beta_2 0.995, l2 0.0093, filters 60, kernel_sz 3,  dense_sz 147, activs tanh, padding 0\n",
      "Configuration 3, fold 4 trained 10 epochs in 5.607s.  Loss: tr - 0.666, val - 0.655.  Accuracy: tr - 0.575, val - 0.593.\n",
      "lr 0.0073, beta_1 0.9183, beta_2 0.9975, l2 0.0066, filters 57, kernel_sz 3,  dense_sz 26, activs relu, padding 1\n",
      "Configuration 4, fold 4 trained 10 epochs in 6.582s.  Loss: tr - 0.632, val - 0.591.  Accuracy: tr - 0.627, val - 0.673.\n",
      "lr 0.0012, beta_1 0.8523, beta_2 0.9921, l2 0.0142, filters 21, kernel_sz 4,  dense_sz 122, activs tanh, padding 0\n",
      "Configuration 5, fold 4 trained 10 epochs in 3.63s.  Loss: tr - 0.196, val - 0.168.  Accuracy: tr - 0.96, val - 0.972.\n",
      "lr 0.0028, beta_1 0.9334, beta_2 0.9969, l2 0.0032, filters 29, kernel_sz 4,  dense_sz 85, activs tanh, padding 0\n",
      "Configuration 6, fold 4 trained 10 epochs in 4.638s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.009, beta_1 0.8995, beta_2 0.9927, l2 0.0175, filters 40, kernel_sz 2,  dense_sz 119, activs relu, padding 1\n",
      "Configuration 7, fold 4 trained 10 epochs in 4.244s.  Loss: tr - 0.628, val - 0.604.  Accuracy: tr - 0.677, val - 0.679.\n",
      "lr 0.0096, beta_1 0.9009, beta_2 0.9966, l2 0.0134, filters 35, kernel_sz 2,  dense_sz 36, activs relu, padding 1\n",
      "Configuration 8, fold 4 trained 10 epochs in 4.279s.  Loss: tr - 0.65, val - 0.645.  Accuracy: tr - 0.617, val - 0.618.\n",
      "lr 0.0034, beta_1 0.8662, beta_2 0.9936, l2 0.0069, filters 46, kernel_sz 4,  dense_sz 128, activs tanh, padding 0\n",
      "Configuration 9, fold 4 trained 10 epochs in 6.116s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.587.\n",
      "lr 0.0005, beta_1 0.9473, beta_2 0.9984, l2 0.0154, filters 63, kernel_sz 5,  dense_sz 79, activs tanh, padding 0\n",
      "Configuration 10, fold 4 trained 10 epochs in 4.803s.  Loss: tr - 0.246, val - 0.227.  Accuracy: tr - 0.961, val - 0.975.\n",
      "lr 0.0067, beta_1 0.8821, beta_2 0.9918, l2 0.0048, filters 28, kernel_sz 3,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 11, fold 4 trained 10 epochs in 4.872s.  Loss: tr - 0.653, val - 0.65.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.0054, beta_1 0.9276, beta_2 0.9942, l2 0.0008, filters 18, kernel_sz 2,  dense_sz 64, activs tanh, padding 0\n",
      "Configuration 12, fold 4 trained 10 epochs in 1.905s.  Loss: tr - 0.078, val - 0.069.  Accuracy: tr - 0.979, val - 0.984.\n",
      "lr 0.0016, beta_1 0.8928, beta_2 0.996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 13, fold 4 trained 10 epochs in 8.312s.  Loss: tr - 0.069, val - 0.053.  Accuracy: tr - 0.976, val - 0.985.\n",
      "lr 0.0048, beta_1 0.9241, beta_2 0.9912, l2 0.009, filters 44, kernel_sz 5,  dense_sz 45, activs relu, padding 1\n",
      "Configuration 14, fold 4 trained 6 epochs in 3.168s.  Loss: tr - 37.979, val - 46.356.  Accuracy: tr - 0.544, val - 0.536.\n",
      "lr 0.0085, beta_1 0.859, beta_2 0.9991, l2 0.0117, filters 31, kernel_sz 3,  dense_sz 141, activs tanh, padding 0\n",
      "Configuration 15, fold 4 trained 10 epochs in 4.255s.  Loss: tr - 0.671, val - 0.656.  Accuracy: tr - 0.583, val - 0.587.\n",
      "lr 0.0084, beta_1 0.9189, beta_2 0.9934, l2 0.0039, filters 59, kernel_sz 4,  dense_sz 95, activs relu, padding 0\n",
      "Configuration 16, fold 4 trained 6 epochs in 4.448s.  Loss: tr - 38.947, val - 48.344.  Accuracy: tr - 0.527, val - 0.517.\n",
      "lr 0.0047, beta_1 0.8607, beta_2 0.9963, l2 0.0164, filters 22, kernel_sz 2,  dense_sz 62, activs tanh, padding 1\n",
      "Configuration 17, fold 4 trained 10 epochs in 2.239s.  Loss: tr - 0.133, val - 0.124.  Accuracy: tr - 0.985, val - 0.984.\n",
      "lr 0.0017, beta_1 0.9294, beta_2 0.9915, l2 0.0054, filters 39, kernel_sz 3,  dense_sz 145, activs tanh, padding 1\n",
      "Configuration 18, fold 4 trained 10 epochs in 7.277s.  Loss: tr - 0.638, val - 0.615.  Accuracy: tr - 0.646, val - 0.703.\n",
      "lr 0.0054, beta_1 0.8875, beta_2 0.9981, l2 0.0149, filters 52, kernel_sz 5,  dense_sz 50, activs relu, padding 0\n",
      "Configuration 19, fold 4 trained 10 epochs in 4.335s.  Loss: tr - 0.676, val - 0.673.  Accuracy: tr - 0.591, val - 0.592.\n",
      "lr 0.0065, beta_1 0.9456, beta_2 0.9956, l2 0.0108, filters 42, kernel_sz 4,  dense_sz 123, activs tanh, padding 1\n",
      "Configuration 20, fold 4 trained 10 epochs in 6.766s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.588, val - 0.592.\n",
      "lr 0.0003, beta_1 0.8873, beta_2 0.994, l2 0.0099, filters 31, kernel_sz 2,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 21, fold 4 trained 10 epochs in 2.95s.  Loss: tr - 0.421, val - 0.416.  Accuracy: tr - 0.818, val - 0.802.\n",
      "lr 0.0036, beta_1 0.9062, beta_2 0.9987, l2 0.0185, filters 20, kernel_sz 3,  dense_sz 111, activs relu, padding 0\n",
      "Configuration 22, fold 4 trained 10 epochs in 1.888s.  Loss: tr - 0.115, val - 0.091.  Accuracy: tr - 0.956, val - 0.97.\n",
      "lr 0.0098, beta_1 0.8644, beta_2 0.9909, l2 0.0023, filters 55, kernel_sz 5,  dense_sz 83, activs tanh, padding 1\n",
      "Configuration 23, fold 4 trained 10 epochs in 5.832s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.59, val - 0.593.\n",
      "lr 0.0092, beta_1 0.9345, beta_2 0.9997, l2 0.0078, filters 26, kernel_sz 4,  dense_sz 133, activs tanh, padding 1\n",
      "Configuration 24, fold 4 trained 10 epochs in 5.051s.  Loss: tr - 0.662, val - 0.655.  Accuracy: tr - 0.586, val - 0.592.\n",
      "lr 0.003, beta_1 0.8949, beta_2 0.9905, l2 0.0124, filters 62, kernel_sz 3,  dense_sz 37, activs relu, padding 0\n",
      "Configuration 25, fold 4 trained 10 epochs in 5.751s.  Loss: tr - 0.328, val - 0.342.  Accuracy: tr - 0.861, val - 0.838.\n",
      "lr 0.0009, beta_1 0.9138, beta_2 0.9953, l2 0.0015, filters 48, kernel_sz 2,  dense_sz 105, activs relu, padding 0\n",
      "Configuration 26, fold 4 trained 10 epochs in 4.01s.  Loss: tr - 0.041, val - 0.038.  Accuracy: tr - 0.988, val - 0.987.\n",
      "lr 0.0071, beta_1 0.8533, beta_2 0.9948, l2 0.0188, filters 36, kernel_sz 4,  dense_sz 73, activs tanh, padding 1\n",
      "Configuration 27, fold 4 trained 10 epochs in 6.345s.  Loss: tr - 0.674, val - 0.656.  Accuracy: tr - 0.554, val - 0.593.\n",
      "lr 0.006, beta_1 0.9113, beta_2 0.9924, l2 0.016, filters 33, kernel_sz 5,  dense_sz 100, activs relu, padding 0\n",
      "Configuration 28, fold 4 trained 10 epochs in 4.741s.  Loss: tr - 0.671, val - 0.656.  Accuracy: tr - 0.578, val - 0.592.\n",
      "lr 0.0023, beta_1 0.8718, beta_2 0.9978, l2 0.0047, filters 46, kernel_sz 3,  dense_sz 72, activs tanh, padding 1\n",
      "Configuration 29, fold 4 trained 10 epochs in 5.576s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "Significant difference in validation losses across group, after 5 folds (2.0660501980234093e-12)\n",
      "Configuration 22 (mean vloss 0.08123181387782097) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 12 (mean vloss 0.2051544263958931) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 5 (mean vloss 0.27691709995269775) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 10 (mean vloss 0.2868556171655655) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 18 (mean vloss 0.31821323931217194) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 21 (mean vloss 0.372993940114975) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 17 (mean vloss 0.40768043249845504) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 25 (mean vloss 0.4553232043981552) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 8 (mean vloss 0.6360968708992004) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 6 (mean vloss 0.6537692308425903) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 0 (mean vloss 0.653993284702301) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 29 (mean vloss 0.6540236711502075) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 9 (mean vloss 0.6541872382164001) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 27 (mean vloss 0.654313063621521) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 20 (mean vloss 0.6545023202896119) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 23 (mean vloss 0.6545154452323914) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 15 (mean vloss 0.654673969745636) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 24 (mean vloss 0.6551540732383728) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 3 (mean vloss 0.6552164077758789) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 28 (mean vloss 0.6615207910537719) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 11 (mean vloss 8.731106793880462) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 1 (mean vloss 8.756017363071441) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 7 (mean vloss 17.216826808452605) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 19 (mean vloss 19.445399630069733) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 4 (mean vloss 36.31316162347794) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 14 (mean vloss 45.779815673828125) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 16 (mean vloss 46.87075653076172) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Paired tests completed\n",
      "Iteration completed in 688.47s. Remaining configurations 3.  Mean validation loss 0.103 and acc 0.956\n",
      "---------------------------------------------------------------------------\n",
      "####################################################################################################\n",
      "Iteration 2 of Iterated F-race.\n",
      "Training fold 0 on list of 30 configurations.\n",
      "Configuration 2, evaluated once before -- skipping.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9937, l2 0.0171, filters 46, kernel_sz 2,  dense_sz 49, activs relu, padding 0\n",
      "Configuration 30, fold 0 trained 10 epochs in 4.023s.  Loss: tr - 0.451, val - 0.422.  Accuracy: tr - 0.858, val - 0.857.\n",
      "lr 0.0001, beta_1 0.9449, beta_2 0.9907, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 1\n",
      "Configuration 31, fold 0 trained 10 epochs in 6.016s.  Loss: tr - 0.187, val - 0.151.  Accuracy: tr - 0.93, val - 0.946.\n",
      "lr 0.0022, beta_1 0.9198, beta_2 0.9932, l2 0.0108, filters 45, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 32, fold 0 trained 10 epochs in 4.814s.  Loss: tr - 0.239, val - 0.193.  Accuracy: tr - 0.896, val - 0.924.\n",
      "lr 0.0012, beta_1 0.9214, beta_2 0.9999, l2 0.0005, filters 40, kernel_sz 3,  dense_sz 69, activs relu, padding 1\n",
      "Configuration 33, fold 0 trained 10 epochs in 5.874s.  Loss: tr - 0.178, val - 0.133.  Accuracy: tr - 0.936, val - 0.952.\n",
      "lr 0.0075, beta_1 0.8625, beta_2 0.9973, l2 0.0031, filters 16, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 34, fold 0 trained 10 epochs in 2.102s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.0078, beta_1 0.9495, beta_2 0.9999, l2 0.0052, filters 25, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 35, fold 0 trained 10 epochs in 4.253s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.595.\n",
      "lr 0.0095, beta_1 0.8684, beta_2 0.9999, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 125, activs tanh, padding 0\n",
      "Configuration 36, fold 0 trained 10 epochs in 4.824s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.589, val - 0.596.\n",
      "lr 0.0001, beta_1 0.8968, beta_2 0.9941, l2 0.0084, filters 16, kernel_sz 4,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 37, fold 0 trained 10 epochs in 1.807s.  Loss: tr - 0.648, val - 0.644.  Accuracy: tr - 0.603, val - 0.614.\n",
      "lr 0.0001, beta_1 0.901, beta_2 0.9954, l2 0.0044, filters 64, kernel_sz 5,  dense_sz 46, activs relu, padding 0\n",
      "Configuration 38, fold 0 trained 10 epochs in 4.835s.  Loss: tr - 0.484, val - 0.462.  Accuracy: tr - 0.783, val - 0.798.\n",
      "lr 0.0001, beta_1 0.8895, beta_2 0.9958, l2 0.0005, filters 17, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 39, fold 0 trained 10 epochs in 1.953s.  Loss: tr - 0.641, val - 0.636.  Accuracy: tr - 0.657, val - 0.668.\n",
      "lr 0.0056, beta_1 0.95, beta_2 0.9963, l2 0.0005, filters 51, kernel_sz 5,  dense_sz 111, activs relu, padding 1\n",
      "Configuration 40, fold 0 trained 6 epochs in 3.341s.  Loss: tr - 42.323, val - 50.407.  Accuracy: tr - 0.504, val - 0.496.\n",
      "lr 0.0047, beta_1 0.85, beta_2 0.9988, l2 0.0175, filters 46, kernel_sz 2,  dense_sz 42, activs tanh, padding 0\n",
      "Configuration 41, fold 0 trained 10 epochs in 3.971s.  Loss: tr - 0.657, val - 0.656.  Accuracy: tr - 0.591, val - 0.586.\n",
      "lr 0.0057, beta_1 0.95, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 42, fold 0 trained 10 epochs in 1.743s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.0015, beta_1 0.9212, beta_2 0.9999, l2 0.0185, filters 64, kernel_sz 5,  dense_sz 65, activs relu, padding 1\n",
      "Configuration 43, fold 0 trained 10 epochs in 5.974s.  Loss: tr - 0.657, val - 0.653.  Accuracy: tr - 0.6, val - 0.617.\n",
      "lr 0.0018, beta_1 0.95, beta_2 0.9999, l2 0.02, filters 20, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 44, fold 0 trained 10 epochs in 2.111s.  Loss: tr - 0.626, val - 0.611.  Accuracy: tr - 0.62, val - 0.648.\n",
      "lr 0.0001, beta_1 0.851, beta_2 0.9917, l2 0.0185, filters 21, kernel_sz 2,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 45, fold 0 trained 10 epochs in 2.493s.  Loss: tr - 0.646, val - 0.644.  Accuracy: tr - 0.607, val - 0.62.\n",
      "lr 0.0001, beta_1 0.8788, beta_2 0.9999, l2 0.0005, filters 16, kernel_sz 2,  dense_sz 43, activs tanh, padding 1\n",
      "Configuration 46, fold 0 trained 10 epochs in 1.723s.  Loss: tr - 0.647, val - 0.644.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0036, beta_1 0.9441, beta_2 0.9999, l2 0.02, filters 46, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 47, fold 0 trained 10 epochs in 5.083s.  Loss: tr - 0.658, val - 0.656.  Accuracy: tr - 0.587, val - 0.588.\n",
      "lr 0.0001, beta_1 0.8736, beta_2 0.9978, l2 0.02, filters 44, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 48, fold 0 trained 10 epochs in 3.831s.  Loss: tr - 0.635, val - 0.631.  Accuracy: tr - 0.699, val - 0.713.\n",
      "lr 0.0014, beta_1 0.8528, beta_2 0.9924, l2 0.0071, filters 16, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 49, fold 0 trained 10 epochs in 2.047s.  Loss: tr - 0.062, val - 0.046.  Accuracy: tr - 0.98, val - 0.986.\n",
      "lr 0.0001, beta_1 0.9018, beta_2 0.9931, l2 0.02, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 50, fold 0 trained 10 epochs in 4.325s.  Loss: tr - 0.656, val - 0.654.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 47, activs relu, padding 1\n",
      "Configuration 51, fold 0 trained 10 epochs in 2.367s.  Loss: tr - 0.663, val - 0.658.  Accuracy: tr - 0.573, val - 0.596.\n",
      "lr 0.0025, beta_1 0.8928, beta_2 0.992, l2 0.0005, filters 64, kernel_sz 2,  dense_sz 72, activs relu, padding 0\n",
      "Configuration 52, fold 0 trained 10 epochs in 4.107s.  Loss: tr - 0.117, val - 0.093.  Accuracy: tr - 0.957, val - 0.965.\n",
      "lr 0.0087, beta_1 0.95, beta_2 0.9938, l2 0.02, filters 16, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 53, fold 0 trained 10 epochs in 1.471s.  Loss: tr - 0.368, val - 0.312.  Accuracy: tr - 0.829, val - 0.857.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9908, l2 0.0005, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 54, fold 0 trained 10 epochs in 4.284s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.0024, beta_1 0.9349, beta_2 0.99, l2 0.02, filters 16, kernel_sz 4,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 55, fold 0 trained 10 epochs in 2.206s.  Loss: tr - 0.409, val - 0.383.  Accuracy: tr - 0.892, val - 0.925.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.99, l2 0.0005, filters 38, kernel_sz 3,  dense_sz 50, activs tanh, padding 0\n",
      "Configuration 56, fold 0 trained 10 epochs in 6.007s.  Loss: tr - 0.351, val - 0.332.  Accuracy: tr - 0.931, val - 0.951.\n",
      "lr 0.0001, beta_1 0.9134, beta_2 0.9936, l2 0.0184, filters 16, kernel_sz 2,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 57, fold 0 trained 10 epochs in 2.05s.  Loss: tr - 0.507, val - 0.492.  Accuracy: tr - 0.843, val - 0.822.\n",
      "Not enough evidence to support a difference across group, after 1 folds (0.46506624123787865)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 1 on list of 30 configurations.\n",
      "Configuration 2, evaluated once before -- skipping.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9937, l2 0.0171, filters 46, kernel_sz 2,  dense_sz 49, activs relu, padding 0\n",
      "Configuration 30, fold 1 trained 10 epochs in 4.047s.  Loss: tr - 0.552, val - 0.532.  Accuracy: tr - 0.762, val - 0.789.\n",
      "lr 0.0001, beta_1 0.9449, beta_2 0.9907, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 1\n",
      "Configuration 31, fold 1 trained 10 epochs in 6.018s.  Loss: tr - 0.311, val - 0.261.  Accuracy: tr - 0.885, val - 0.908.\n",
      "lr 0.0022, beta_1 0.9198, beta_2 0.9932, l2 0.0108, filters 45, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 32, fold 1 trained 10 epochs in 4.794s.  Loss: tr - 0.146, val - 0.124.  Accuracy: tr - 0.945, val - 0.953.\n",
      "lr 0.0012, beta_1 0.9214, beta_2 0.9999, l2 0.0005, filters 40, kernel_sz 3,  dense_sz 69, activs relu, padding 1\n",
      "Configuration 33, fold 1 trained 10 epochs in 5.574s.  Loss: tr - 0.056, val - 0.047.  Accuracy: tr - 0.98, val - 0.988.\n",
      "lr 0.0075, beta_1 0.8625, beta_2 0.9973, l2 0.0031, filters 16, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 34, fold 1 trained 10 epochs in 2.107s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.59, val - 0.592.\n",
      "lr 0.0078, beta_1 0.9495, beta_2 0.9999, l2 0.0052, filters 25, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 35, fold 1 trained 10 epochs in 4.556s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "lr 0.0095, beta_1 0.8684, beta_2 0.9999, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 125, activs tanh, padding 0\n",
      "Configuration 36, fold 1 trained 10 epochs in 4.835s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.587, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8968, beta_2 0.9941, l2 0.0084, filters 16, kernel_sz 4,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 37, fold 1 trained 10 epochs in 1.802s.  Loss: tr - 0.649, val - 0.644.  Accuracy: tr - 0.626, val - 0.62.\n",
      "lr 0.0001, beta_1 0.901, beta_2 0.9954, l2 0.0044, filters 64, kernel_sz 5,  dense_sz 46, activs relu, padding 0\n",
      "Configuration 38, fold 1 trained 10 epochs in 4.845s.  Loss: tr - 0.358, val - 0.324.  Accuracy: tr - 0.88, val - 0.906.\n",
      "lr 0.0001, beta_1 0.8895, beta_2 0.9958, l2 0.0005, filters 17, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 39, fold 1 trained 10 epochs in 1.657s.  Loss: tr - 0.637, val - 0.633.  Accuracy: tr - 0.667, val - 0.669.\n",
      "lr 0.0056, beta_1 0.95, beta_2 0.9963, l2 0.0005, filters 51, kernel_sz 5,  dense_sz 111, activs relu, padding 1\n",
      "Configuration 40, fold 1 trained 6 epochs in 3.328s.  Loss: tr - 35.332, val - 42.468.  Accuracy: tr - 0.56, val - 0.575.\n",
      "lr 0.0047, beta_1 0.85, beta_2 0.9988, l2 0.0175, filters 46, kernel_sz 2,  dense_sz 42, activs tanh, padding 0\n",
      "Configuration 41, fold 1 trained 10 epochs in 3.963s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.59, val - 0.593.\n",
      "lr 0.0057, beta_1 0.95, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 42, fold 1 trained 10 epochs in 2.037s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "lr 0.0015, beta_1 0.9212, beta_2 0.9999, l2 0.0185, filters 64, kernel_sz 5,  dense_sz 65, activs relu, padding 1\n",
      "Configuration 43, fold 1 trained 10 epochs in 5.99s.  Loss: tr - 0.454, val - 0.387.  Accuracy: tr - 0.788, val - 0.811.\n",
      "lr 0.0018, beta_1 0.95, beta_2 0.9999, l2 0.02, filters 20, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 44, fold 1 trained 10 epochs in 2.136s.  Loss: tr - 0.391, val - 0.366.  Accuracy: tr - 0.824, val - 0.838.\n",
      "lr 0.0001, beta_1 0.851, beta_2 0.9917, l2 0.0185, filters 21, kernel_sz 2,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 45, fold 1 trained 10 epochs in 2.211s.  Loss: tr - 0.654, val - 0.652.  Accuracy: tr - 0.598, val - 0.599.\n",
      "lr 0.0001, beta_1 0.8788, beta_2 0.9999, l2 0.0005, filters 16, kernel_sz 2,  dense_sz 43, activs tanh, padding 1\n",
      "Configuration 46, fold 1 trained 10 epochs in 1.754s.  Loss: tr - 0.656, val - 0.654.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.0036, beta_1 0.9441, beta_2 0.9999, l2 0.02, filters 46, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 47, fold 1 trained 10 epochs in 5.099s.  Loss: tr - 0.658, val - 0.654.  Accuracy: tr - 0.592, val - 0.587.\n",
      "lr 0.0001, beta_1 0.8736, beta_2 0.9978, l2 0.02, filters 44, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 48, fold 1 trained 10 epochs in 4.123s.  Loss: tr - 0.646, val - 0.643.  Accuracy: tr - 0.643, val - 0.647.\n",
      "lr 0.0014, beta_1 0.8528, beta_2 0.9924, l2 0.0071, filters 16, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 49, fold 1 trained 10 epochs in 1.996s.  Loss: tr - 0.244, val - 0.22.  Accuracy: tr - 0.901, val - 0.914.\n",
      "lr 0.0001, beta_1 0.9018, beta_2 0.9931, l2 0.02, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 50, fold 1 trained 10 epochs in 4.292s.  Loss: tr - 0.656, val - 0.654.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 47, activs relu, padding 1\n",
      "Configuration 51, fold 1 trained 10 epochs in 2.073s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.0025, beta_1 0.8928, beta_2 0.992, l2 0.0005, filters 64, kernel_sz 2,  dense_sz 72, activs relu, padding 0\n",
      "Configuration 52, fold 1 trained 10 epochs in 4.11s.  Loss: tr - 0.042, val - 0.032.  Accuracy: tr - 0.987, val - 0.993.\n",
      "lr 0.0087, beta_1 0.95, beta_2 0.9938, l2 0.02, filters 16, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 53, fold 1 trained 10 epochs in 1.482s.  Loss: tr - 0.498, val - 0.419.  Accuracy: tr - 0.762, val - 0.822.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9908, l2 0.0005, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 54, fold 1 trained 10 epochs in 4.586s.  Loss: tr - 0.65, val - 0.648.  Accuracy: tr - 0.649, val - 0.653.\n",
      "lr 0.0024, beta_1 0.9349, beta_2 0.99, l2 0.02, filters 16, kernel_sz 4,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 55, fold 1 trained 10 epochs in 2.237s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.99, l2 0.0005, filters 38, kernel_sz 3,  dense_sz 50, activs tanh, padding 0\n",
      "Configuration 56, fold 1 trained 10 epochs in 6.012s.  Loss: tr - 0.408, val - 0.377.  Accuracy: tr - 0.914, val - 0.939.\n",
      "lr 0.0001, beta_1 0.9134, beta_2 0.9936, l2 0.0184, filters 16, kernel_sz 2,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 57, fold 1 trained 10 epochs in 1.727s.  Loss: tr - 0.591, val - 0.578.  Accuracy: tr - 0.754, val - 0.783.\n",
      "Significant difference in validation losses across group, after 2 folds (0.003993898921824977)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 2 on list of 30 configurations.\n",
      "Configuration 2, evaluated once before -- skipping.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9937, l2 0.0171, filters 46, kernel_sz 2,  dense_sz 49, activs relu, padding 0\n",
      "Configuration 30, fold 2 trained 10 epochs in 4.037s.  Loss: tr - 0.635, val - 0.628.  Accuracy: tr - 0.62, val - 0.619.\n",
      "lr 0.0001, beta_1 0.9449, beta_2 0.9907, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 1\n",
      "Configuration 31, fold 2 trained 10 epochs in 6.008s.  Loss: tr - 0.11, val - 0.089.  Accuracy: tr - 0.961, val - 0.965.\n",
      "lr 0.0022, beta_1 0.9198, beta_2 0.9932, l2 0.0108, filters 45, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 32, fold 2 trained 10 epochs in 5.089s.  Loss: tr - 0.486, val - 0.468.  Accuracy: tr - 0.75, val - 0.768.\n",
      "lr 0.0012, beta_1 0.9214, beta_2 0.9999, l2 0.0005, filters 40, kernel_sz 3,  dense_sz 69, activs relu, padding 1\n",
      "Configuration 33, fold 2 trained 10 epochs in 5.567s.  Loss: tr - 0.036, val - 0.038.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0075, beta_1 0.8625, beta_2 0.9973, l2 0.0031, filters 16, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 34, fold 2 trained 10 epochs in 2.12s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0078, beta_1 0.9495, beta_2 0.9999, l2 0.0052, filters 25, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 35, fold 2 trained 10 epochs in 4.266s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.591.\n",
      "lr 0.0095, beta_1 0.8684, beta_2 0.9999, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 125, activs tanh, padding 0\n",
      "Configuration 36, fold 2 trained 10 epochs in 4.819s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.59, val - 0.591.\n",
      "lr 0.0001, beta_1 0.8968, beta_2 0.9941, l2 0.0084, filters 16, kernel_sz 4,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 37, fold 2 trained 10 epochs in 1.797s.  Loss: tr - 0.629, val - 0.618.  Accuracy: tr - 0.699, val - 0.724.\n",
      "lr 0.0001, beta_1 0.901, beta_2 0.9954, l2 0.0044, filters 64, kernel_sz 5,  dense_sz 46, activs relu, padding 0\n",
      "Configuration 38, fold 2 trained 10 epochs in 5.108s.  Loss: tr - 0.42, val - 0.38.  Accuracy: tr - 0.836, val - 0.863.\n",
      "lr 0.0001, beta_1 0.8895, beta_2 0.9958, l2 0.0005, filters 17, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 39, fold 2 trained 10 epochs in 1.648s.  Loss: tr - 0.656, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0056, beta_1 0.95, beta_2 0.9963, l2 0.0005, filters 51, kernel_sz 5,  dense_sz 111, activs relu, padding 1\n",
      "Configuration 40, fold 2 trained 6 epochs in 3.323s.  Loss: tr - 39.358, val - 47.248.  Accuracy: tr - 0.537, val - 0.528.\n",
      "lr 0.0047, beta_1 0.85, beta_2 0.9988, l2 0.0175, filters 46, kernel_sz 2,  dense_sz 42, activs tanh, padding 0\n",
      "Configuration 41, fold 2 trained 10 epochs in 3.978s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.595, val - 0.591.\n",
      "lr 0.0057, beta_1 0.95, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 42, fold 2 trained 10 epochs in 1.745s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.59.\n",
      "lr 0.0015, beta_1 0.9212, beta_2 0.9999, l2 0.0185, filters 64, kernel_sz 5,  dense_sz 65, activs relu, padding 1\n",
      "Configuration 43, fold 2 trained 10 epochs in 5.987s.  Loss: tr - 0.33, val - 0.201.  Accuracy: tr - 0.854, val - 0.923.\n",
      "lr 0.0018, beta_1 0.95, beta_2 0.9999, l2 0.02, filters 20, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 44, fold 2 trained 10 epochs in 2.435s.  Loss: tr - 0.156, val - 0.117.  Accuracy: tr - 0.949, val - 0.965.\n",
      "lr 0.0001, beta_1 0.851, beta_2 0.9917, l2 0.0185, filters 21, kernel_sz 2,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 45, fold 2 trained 10 epochs in 2.207s.  Loss: tr - 0.655, val - 0.652.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0001, beta_1 0.8788, beta_2 0.9999, l2 0.0005, filters 16, kernel_sz 2,  dense_sz 43, activs tanh, padding 1\n",
      "Configuration 46, fold 2 trained 10 epochs in 1.722s.  Loss: tr - 0.642, val - 0.637.  Accuracy: tr - 0.65, val - 0.65.\n",
      "lr 0.0036, beta_1 0.9441, beta_2 0.9999, l2 0.02, filters 46, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 47, fold 2 trained 10 epochs in 5.083s.  Loss: tr - 0.669, val - 0.654.  Accuracy: tr - 0.581, val - 0.591.\n",
      "lr 0.0001, beta_1 0.8736, beta_2 0.9978, l2 0.02, filters 44, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 48, fold 2 trained 10 epochs in 3.819s.  Loss: tr - 0.651, val - 0.648.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0014, beta_1 0.8528, beta_2 0.9924, l2 0.0071, filters 16, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 49, fold 2 trained 10 epochs in 2.019s.  Loss: tr - 0.086, val - 0.063.  Accuracy: tr - 0.969, val - 0.983.\n",
      "lr 0.0001, beta_1 0.9018, beta_2 0.9931, l2 0.02, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 50, fold 2 trained 10 epochs in 4.586s.  Loss: tr - 0.656, val - 0.654.  Accuracy: tr - 0.601, val - 0.591.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 47, activs relu, padding 1\n",
      "Configuration 51, fold 2 trained 10 epochs in 2.083s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0025, beta_1 0.8928, beta_2 0.992, l2 0.0005, filters 64, kernel_sz 2,  dense_sz 72, activs relu, padding 0\n",
      "Configuration 52, fold 2 trained 10 epochs in 4.112s.  Loss: tr - 0.112, val - 0.101.  Accuracy: tr - 0.959, val - 0.964.\n",
      "lr 0.0087, beta_1 0.95, beta_2 0.9938, l2 0.02, filters 16, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 53, fold 2 trained 10 epochs in 1.487s.  Loss: tr - 0.097, val - 0.1.  Accuracy: tr - 0.964, val - 0.977.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9908, l2 0.0005, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 54, fold 2 trained 10 epochs in 4.296s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.593, val - 0.591.\n",
      "lr 0.0024, beta_1 0.9349, beta_2 0.99, l2 0.02, filters 16, kernel_sz 4,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 55, fold 2 trained 10 epochs in 2.231s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.593, val - 0.591.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.99, l2 0.0005, filters 38, kernel_sz 3,  dense_sz 50, activs tanh, padding 0\n",
      "Configuration 56, fold 2 trained 10 epochs in 5.999s.  Loss: tr - 0.457, val - 0.414.  Accuracy: tr - 0.895, val - 0.923.\n",
      "lr 0.0001, beta_1 0.9134, beta_2 0.9936, l2 0.0184, filters 16, kernel_sz 2,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 57, fold 2 trained 10 epochs in 2.016s.  Loss: tr - 0.564, val - 0.548.  Accuracy: tr - 0.794, val - 0.819.\n",
      "Significant difference in validation losses across group, after 3 folds (1.4910787074640965e-06)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 3 on list of 30 configurations.\n",
      "Configuration 2, evaluated once before -- skipping.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9937, l2 0.0171, filters 46, kernel_sz 2,  dense_sz 49, activs relu, padding 0\n",
      "Configuration 30, fold 3 trained 10 epochs in 4.059s.  Loss: tr - 0.527, val - 0.508.  Accuracy: tr - 0.824, val - 0.837.\n",
      "lr 0.0001, beta_1 0.9449, beta_2 0.9907, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 1\n",
      "Configuration 31, fold 3 trained 10 epochs in 6.02s.  Loss: tr - 0.305, val - 0.272.  Accuracy: tr - 0.898, val - 0.907.\n",
      "lr 0.0022, beta_1 0.9198, beta_2 0.9932, l2 0.0108, filters 45, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 32, fold 3 trained 10 epochs in 4.817s.  Loss: tr - 0.184, val - 0.18.  Accuracy: tr - 0.928, val - 0.92.\n",
      "lr 0.0012, beta_1 0.9214, beta_2 0.9999, l2 0.0005, filters 40, kernel_sz 3,  dense_sz 69, activs relu, padding 1\n",
      "Configuration 33, fold 3 trained 10 epochs in 5.573s.  Loss: tr - 0.043, val - 0.038.  Accuracy: tr - 0.987, val - 0.986.\n",
      "lr 0.0075, beta_1 0.8625, beta_2 0.9973, l2 0.0031, filters 16, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 34, fold 3 trained 10 epochs in 2.395s.  Loss: tr - 0.668, val - 0.65.  Accuracy: tr - 0.587, val - 0.607.\n",
      "lr 0.0078, beta_1 0.9495, beta_2 0.9999, l2 0.0052, filters 25, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 35, fold 3 trained 10 epochs in 4.254s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.593, val - 0.607.\n",
      "lr 0.0095, beta_1 0.8684, beta_2 0.9999, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 125, activs tanh, padding 0\n",
      "Configuration 36, fold 3 trained 10 epochs in 4.818s.  Loss: tr - 0.659, val - 0.651.  Accuracy: tr - 0.588, val - 0.607.\n",
      "lr 0.0001, beta_1 0.8968, beta_2 0.9941, l2 0.0084, filters 16, kernel_sz 4,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 37, fold 3 trained 10 epochs in 1.794s.  Loss: tr - 0.635, val - 0.627.  Accuracy: tr - 0.676, val - 0.684.\n",
      "lr 0.0001, beta_1 0.901, beta_2 0.9954, l2 0.0044, filters 64, kernel_sz 5,  dense_sz 46, activs relu, padding 0\n",
      "Configuration 38, fold 3 trained 10 epochs in 4.841s.  Loss: tr - 0.392, val - 0.39.  Accuracy: tr - 0.846, val - 0.837.\n",
      "lr 0.0001, beta_1 0.8895, beta_2 0.9958, l2 0.0005, filters 17, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 39, fold 3 trained 10 epochs in 1.666s.  Loss: tr - 0.655, val - 0.649.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0056, beta_1 0.95, beta_2 0.9963, l2 0.0005, filters 51, kernel_sz 5,  dense_sz 111, activs relu, padding 1\n",
      "Configuration 40, fold 3 trained 6 epochs in 3.345s.  Loss: tr - 32.696, val - 40.698.  Accuracy: tr - 0.558, val - 0.593.\n",
      "lr 0.0047, beta_1 0.85, beta_2 0.9988, l2 0.0175, filters 46, kernel_sz 2,  dense_sz 42, activs tanh, padding 0\n",
      "Configuration 41, fold 3 trained 10 epochs in 4.288s.  Loss: tr - 0.658, val - 0.652.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0057, beta_1 0.95, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 42, fold 3 trained 10 epochs in 1.747s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0015, beta_1 0.9212, beta_2 0.9999, l2 0.0185, filters 64, kernel_sz 5,  dense_sz 65, activs relu, padding 1\n",
      "Configuration 43, fold 3 trained 10 epochs in 5.977s.  Loss: tr - 0.654, val - 0.648.  Accuracy: tr - 0.596, val - 0.632.\n",
      "lr 0.0018, beta_1 0.95, beta_2 0.9999, l2 0.02, filters 20, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 44, fold 3 trained 10 epochs in 2.119s.  Loss: tr - 0.166, val - 0.154.  Accuracy: tr - 0.939, val - 0.944.\n",
      "lr 0.0001, beta_1 0.851, beta_2 0.9917, l2 0.0185, filters 21, kernel_sz 2,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 45, fold 3 trained 10 epochs in 2.201s.  Loss: tr - 0.654, val - 0.649.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0001, beta_1 0.8788, beta_2 0.9999, l2 0.0005, filters 16, kernel_sz 2,  dense_sz 43, activs tanh, padding 1\n",
      "Configuration 46, fold 3 trained 10 epochs in 1.721s.  Loss: tr - 0.645, val - 0.638.  Accuracy: tr - 0.604, val - 0.624.\n",
      "lr 0.0036, beta_1 0.9441, beta_2 0.9999, l2 0.02, filters 46, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 47, fold 3 trained 10 epochs in 5.381s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.588, val - 0.599.\n",
      "lr 0.0001, beta_1 0.8736, beta_2 0.9978, l2 0.02, filters 44, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 48, fold 3 trained 10 epochs in 3.827s.  Loss: tr - 0.653, val - 0.647.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0014, beta_1 0.8528, beta_2 0.9924, l2 0.0071, filters 16, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 49, fold 3 trained 10 epochs in 2.04s.  Loss: tr - 0.624, val - 0.596.  Accuracy: tr - 0.668, val - 0.663.\n",
      "lr 0.0001, beta_1 0.9018, beta_2 0.9931, l2 0.02, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 50, fold 3 trained 10 epochs in 4.3s.  Loss: tr - 0.656, val - 0.65.  Accuracy: tr - 0.651, val - 0.661.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 47, activs relu, padding 1\n",
      "Configuration 51, fold 3 trained 10 epochs in 2.088s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0025, beta_1 0.8928, beta_2 0.992, l2 0.0005, filters 64, kernel_sz 2,  dense_sz 72, activs relu, padding 0\n",
      "Configuration 52, fold 3 trained 10 epochs in 4.068s.  Loss: tr - 0.077, val - 0.066.  Accuracy: tr - 0.973, val - 0.976.\n",
      "lr 0.0087, beta_1 0.95, beta_2 0.9938, l2 0.02, filters 16, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 53, fold 3 trained 10 epochs in 1.776s.  Loss: tr - 0.058, val - 0.051.  Accuracy: tr - 0.984, val - 0.985.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9908, l2 0.0005, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 54, fold 3 trained 10 epochs in 4.269s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0024, beta_1 0.9349, beta_2 0.99, l2 0.02, filters 16, kernel_sz 4,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 55, fold 3 trained 10 epochs in 2.235s.  Loss: tr - 0.344, val - 0.337.  Accuracy: tr - 0.932, val - 0.942.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.99, l2 0.0005, filters 38, kernel_sz 3,  dense_sz 50, activs tanh, padding 0\n",
      "Configuration 56, fold 3 trained 10 epochs in 6.016s.  Loss: tr - 0.438, val - 0.411.  Accuracy: tr - 0.897, val - 0.9.\n",
      "lr 0.0001, beta_1 0.9134, beta_2 0.9936, l2 0.0184, filters 16, kernel_sz 2,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 57, fold 3 trained 10 epochs in 1.73s.  Loss: tr - 0.551, val - 0.546.  Accuracy: tr - 0.788, val - 0.737.\n",
      "Significant difference in validation losses across group, after 4 folds (2.2893548658135565e-10)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 4 on list of 30 configurations.\n",
      "Configuration 2, evaluated once before -- skipping.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9937, l2 0.0171, filters 46, kernel_sz 2,  dense_sz 49, activs relu, padding 0\n",
      "Configuration 30, fold 4 trained 10 epochs in 4.063s.  Loss: tr - 0.519, val - 0.499.  Accuracy: tr - 0.838, val - 0.85.\n",
      "lr 0.0001, beta_1 0.9449, beta_2 0.9907, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 1\n",
      "Configuration 31, fold 4 trained 10 epochs in 6.323s.  Loss: tr - 0.115, val - 0.096.  Accuracy: tr - 0.961, val - 0.966.\n",
      "lr 0.0022, beta_1 0.9198, beta_2 0.9932, l2 0.0108, filters 45, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 32, fold 4 trained 10 epochs in 4.808s.  Loss: tr - 0.285, val - 0.216.  Accuracy: tr - 0.885, val - 0.913.\n",
      "lr 0.0012, beta_1 0.9214, beta_2 0.9999, l2 0.0005, filters 40, kernel_sz 3,  dense_sz 69, activs relu, padding 1\n",
      "Configuration 33, fold 4 trained 10 epochs in 5.557s.  Loss: tr - 0.045, val - 0.037.  Accuracy: tr - 0.985, val - 0.988.\n",
      "lr 0.0075, beta_1 0.8625, beta_2 0.9973, l2 0.0031, filters 16, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 34, fold 4 trained 10 epochs in 2.097s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.0078, beta_1 0.9495, beta_2 0.9999, l2 0.0052, filters 25, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 35, fold 4 trained 10 epochs in 4.267s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.593.\n",
      "lr 0.0095, beta_1 0.8684, beta_2 0.9999, l2 0.0005, filters 64, kernel_sz 5,  dense_sz 125, activs tanh, padding 0\n",
      "Configuration 36, fold 4 trained 10 epochs in 4.826s.  Loss: tr - 0.658, val - 0.656.  Accuracy: tr - 0.589, val - 0.586.\n",
      "lr 0.0001, beta_1 0.8968, beta_2 0.9941, l2 0.0084, filters 16, kernel_sz 4,  dense_sz 31, activs relu, padding 0\n",
      "Configuration 37, fold 4 trained 10 epochs in 2.134s.  Loss: tr - 0.574, val - 0.556.  Accuracy: tr - 0.766, val - 0.769.\n",
      "lr 0.0001, beta_1 0.901, beta_2 0.9954, l2 0.0044, filters 64, kernel_sz 5,  dense_sz 46, activs relu, padding 0\n",
      "Configuration 38, fold 4 trained 10 epochs in 4.861s.  Loss: tr - 0.379, val - 0.362.  Accuracy: tr - 0.834, val - 0.845.\n",
      "lr 0.0001, beta_1 0.8895, beta_2 0.9958, l2 0.0005, filters 17, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 39, fold 4 trained 10 epochs in 1.669s.  Loss: tr - 0.653, val - 0.651.  Accuracy: tr - 0.599, val - 0.598.\n",
      "lr 0.0056, beta_1 0.95, beta_2 0.9963, l2 0.0005, filters 51, kernel_sz 5,  dense_sz 111, activs relu, padding 1\n",
      "Configuration 40, fold 4 trained 6 epochs in 3.321s.  Loss: tr - 40.446, val - 48.344.  Accuracy: tr - 0.529, val - 0.517.\n",
      "lr 0.0047, beta_1 0.85, beta_2 0.9988, l2 0.0175, filters 46, kernel_sz 2,  dense_sz 42, activs tanh, padding 0\n",
      "Configuration 41, fold 4 trained 10 epochs in 3.976s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.0057, beta_1 0.95, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 42, fold 4 trained 10 epochs in 1.75s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.0015, beta_1 0.9212, beta_2 0.9999, l2 0.0185, filters 64, kernel_sz 5,  dense_sz 65, activs relu, padding 1\n",
      "Configuration 43, fold 4 trained 10 epochs in 5.978s.  Loss: tr - 0.656, val - 0.654.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.0018, beta_1 0.95, beta_2 0.9999, l2 0.02, filters 20, kernel_sz 2,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 44, fold 4 trained 10 epochs in 2.426s.  Loss: tr - 0.632, val - 0.618.  Accuracy: tr - 0.613, val - 0.672.\n",
      "lr 0.0001, beta_1 0.851, beta_2 0.9917, l2 0.0185, filters 21, kernel_sz 2,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 45, fold 4 trained 10 epochs in 2.214s.  Loss: tr - 0.645, val - 0.642.  Accuracy: tr - 0.621, val - 0.635.\n",
      "lr 0.0001, beta_1 0.8788, beta_2 0.9999, l2 0.0005, filters 16, kernel_sz 2,  dense_sz 43, activs tanh, padding 1\n",
      "Configuration 46, fold 4 trained 10 epochs in 1.722s.  Loss: tr - 0.647, val - 0.644.  Accuracy: tr - 0.641, val - 0.644.\n",
      "lr 0.0036, beta_1 0.9441, beta_2 0.9999, l2 0.02, filters 46, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 47, fold 4 trained 10 epochs in 5.103s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.591, val - 0.591.\n",
      "lr 0.0001, beta_1 0.8736, beta_2 0.9978, l2 0.02, filters 44, kernel_sz 2,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 48, fold 4 trained 10 epochs in 3.826s.  Loss: tr - 0.648, val - 0.645.  Accuracy: tr - 0.643, val - 0.656.\n",
      "lr 0.0014, beta_1 0.8528, beta_2 0.9924, l2 0.0071, filters 16, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 49, fold 4 trained 10 epochs in 2.008s.  Loss: tr - 0.124, val - 0.099.  Accuracy: tr - 0.953, val - 0.965.\n",
      "lr 0.0001, beta_1 0.9018, beta_2 0.9931, l2 0.02, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 50, fold 4 trained 10 epochs in 4.578s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.591, val - 0.592.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.9914, l2 0.0005, filters 16, kernel_sz 5,  dense_sz 47, activs relu, padding 1\n",
      "Configuration 51, fold 4 trained 10 epochs in 2.077s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.593.\n",
      "lr 0.0025, beta_1 0.8928, beta_2 0.992, l2 0.0005, filters 64, kernel_sz 2,  dense_sz 72, activs relu, padding 0\n",
      "Configuration 52, fold 4 trained 10 epochs in 4.071s.  Loss: tr - 0.041, val - 0.038.  Accuracy: tr - 0.987, val - 0.988.\n",
      "lr 0.0087, beta_1 0.95, beta_2 0.9938, l2 0.02, filters 16, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 53, fold 4 trained 10 epochs in 1.48s.  Loss: tr - 0.475, val - 0.363.  Accuracy: tr - 0.81, val - 0.857.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9908, l2 0.0005, filters 26, kernel_sz 4,  dense_sz 25, activs tanh, padding 0\n",
      "Configuration 54, fold 4 trained 10 epochs in 4.295s.  Loss: tr - 0.654, val - 0.652.  Accuracy: tr - 0.593, val - 0.625.\n",
      "lr 0.0024, beta_1 0.9349, beta_2 0.99, l2 0.02, filters 16, kernel_sz 4,  dense_sz 25, activs tanh, padding 1\n",
      "Configuration 55, fold 4 trained 10 epochs in 2.243s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.592, val - 0.593.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.99, l2 0.0005, filters 38, kernel_sz 3,  dense_sz 50, activs tanh, padding 0\n",
      "Configuration 56, fold 4 trained 10 epochs in 6.332s.  Loss: tr - 0.443, val - 0.412.  Accuracy: tr - 0.884, val - 0.899.\n",
      "lr 0.0001, beta_1 0.9134, beta_2 0.9936, l2 0.0184, filters 16, kernel_sz 2,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 57, fold 4 trained 10 epochs in 1.736s.  Loss: tr - 0.59, val - 0.578.  Accuracy: tr - 0.771, val - 0.757.\n",
      "Significant difference in validation losses across group, after 5 folds (8.02826112249395e-15)\n",
      "Configuration 31 (mean vloss 0.17392174899578094) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 49 (mean vloss 0.20467647388577462) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 32 (mean vloss 0.23628346472978592) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 44 (mean vloss 0.37316652834415437) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 38 (mean vloss 0.3835291266441345) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 56 (mean vloss 0.3891628861427307) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 43 (mean vloss 0.5084839880466461) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 30 (mean vloss 0.5177666485309601) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 55 (mean vloss 0.5369020402431488) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 57 (mean vloss 0.5483188033103943) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 37 (mean vloss 0.6179484009742737) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 48 (mean vloss 0.6428687930107116) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 46 (mean vloss 0.6434419751167297) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 39 (mean vloss 0.6446253180503845) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 45 (mean vloss 0.6476930379867554) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 54 (mean vloss 0.6516920804977417) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 50 (mean vloss 0.6535247325897217) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 34 (mean vloss 0.6538048028945923) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 47 (mean vloss 0.6538374900817872) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 42 (mean vloss 0.6539103984832764) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 35 (mean vloss 0.6540109753608704) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 36 (mean vloss 0.6542793035507202) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 41 (mean vloss 0.654311454296112) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 51 (mean vloss 0.6543972015380859) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Configuration 40 (mean vloss 45.83296203613281) dropped with pvalue 0.03125 against configuration 13 (mean vloss 0.048762496560811996)\n",
      "Paired tests completed\n",
      "Iteration completed in 497.862s. Remaining configurations 5.  Mean validation loss 0.123 and acc 0.95\n",
      "---------------------------------------------------------------------------\n",
      "####################################################################################################\n",
      "Iteration 3 of Iterated F-race.\n",
      "Training fold 0 on list of 30 configurations.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.999, l2 0.0033, filters 50, kernel_sz 5,  dense_sz 39, activs relu, padding 0\n",
      "Configuration 58, fold 0 trained 6 epochs in 2.563s.  Loss: tr - 43.032, val - 51.705.  Accuracy: tr - 0.49, val - 0.483.\n",
      "lr 0.0071, beta_1 0.9319, beta_2 0.9947, l2 0.02, filters 56, kernel_sz 5,  dense_sz 29, activs relu, padding 1\n",
      "Configuration 59, fold 0 trained 6 epochs in 3.443s.  Loss: tr - 37.367, val - 43.537.  Accuracy: tr - 0.534, val - 0.565.\n",
      "lr 0.0002, beta_1 0.85, beta_2 0.9955, l2 0.02, filters 50, kernel_sz 2,  dense_sz 137, activs tanh, padding 1\n",
      "Configuration 60, fold 0 trained 10 epochs in 5.608s.  Loss: tr - 0.251, val - 0.225.  Accuracy: tr - 0.961, val - 0.972.\n",
      "lr 0.0078, beta_1 0.935, beta_2 0.9942, l2 0.02, filters 53, kernel_sz 4,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 61, fold 0 trained 6 epochs in 4.97s.  Loss: tr - 43.493, val - 51.527.  Accuracy: tr - 0.485, val - 0.485.\n",
      "lr 0.0088, beta_1 0.8541, beta_2 0.9956, l2 0.02, filters 51, kernel_sz 2,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 62, fold 0 trained 10 epochs in 4.565s.  Loss: tr - 0.663, val - 0.658.  Accuracy: tr - 0.58, val - 0.596.\n",
      "lr 0.0001, beta_1 0.9173, beta_2 0.9997, l2 0.0173, filters 54, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 63, fold 0 trained 10 epochs in 6.654s.  Loss: tr - 0.324, val - 0.293.  Accuracy: tr - 0.866, val - 0.886.\n",
      "lr 0.0092, beta_1 0.9222, beta_2 0.9916, l2 0.02, filters 16, kernel_sz 4,  dense_sz 39, activs relu, padding 1\n",
      "Configuration 64, fold 0 trained 10 epochs in 2.215s.  Loss: tr - 0.673, val - 0.658.  Accuracy: tr - 0.568, val - 0.575.\n",
      "lr 0.0081, beta_1 0.905, beta_2 0.9999, l2 0.0061, filters 64, kernel_sz 5,  dense_sz 45, activs tanh, padding 0\n",
      "Configuration 65, fold 0 trained 10 epochs in 4.821s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.59, val - 0.596.\n",
      "lr 0.0016, beta_1 0.8781, beta_2 0.9996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 66, fold 0 trained 10 epochs in 7.103s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.0001, beta_1 0.9344, beta_2 0.999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 67, fold 0 trained 10 epochs in 6.802s.  Loss: tr - 0.259, val - 0.227.  Accuracy: tr - 0.951, val - 0.965.\n",
      "lr 0.0024, beta_1 0.85, beta_2 0.9979, l2 0.0183, filters 56, kernel_sz 3,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 68, fold 0 trained 10 epochs in 6.456s.  Loss: tr - 0.046, val - 0.037.  Accuracy: tr - 0.986, val - 0.989.\n",
      "lr 0.0051, beta_1 0.8946, beta_2 0.9968, l2 0.02, filters 64, kernel_sz 5,  dense_sz 102, activs relu, padding 1\n",
      "Configuration 69, fold 0 trained 8 epochs in 5.096s.  Loss: tr - 45.351, val - 41.374.  Accuracy: tr - 0.532, val - 0.586.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 5,  dense_sz 111, activs tanh, padding 1\n",
      "Configuration 70, fold 0 trained 10 epochs in 6.016s.  Loss: tr - 0.203, val - 0.198.  Accuracy: tr - 0.977, val - 0.978.\n",
      "lr 0.0017, beta_1 0.8601, beta_2 0.9905, l2 0.0117, filters 35, kernel_sz 5,  dense_sz 85, activs tanh, padding 1\n",
      "Configuration 71, fold 0 trained 10 epochs in 3.923s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.596.\n",
      "lr 0.0007, beta_1 0.85, beta_2 0.9985, l2 0.02, filters 64, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 72, fold 0 trained 10 epochs in 5.995s.  Loss: tr - 0.077, val - 0.061.  Accuracy: tr - 0.973, val - 0.98.\n",
      "lr 0.0057, beta_1 0.8899, beta_2 0.9999, l2 0.0075, filters 16, kernel_sz 5,  dense_sz 26, activs tanh, padding 1\n",
      "Configuration 73, fold 0 trained 10 epochs in 2.09s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.594, val - 0.596.\n",
      "lr 0.0074, beta_1 0.8681, beta_2 0.9999, l2 0.0124, filters 27, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 74, fold 0 trained 6 epochs in 2.754s.  Loss: tr - 33.097, val - 41.832.  Accuracy: tr - 0.554, val - 0.582.\n",
      "lr 0.0004, beta_1 0.95, beta_2 0.9954, l2 0.02, filters 33, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 75, fold 0 trained 10 epochs in 3.172s.  Loss: tr - 0.524, val - 0.495.  Accuracy: tr - 0.799, val - 0.822.\n",
      "lr 0.0096, beta_1 0.9138, beta_2 0.995, l2 0.02, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 0\n",
      "Configuration 76, fold 0 trained 7 epochs in 3.679s.  Loss: tr - 42.509, val - 40.534.  Accuracy: tr - 0.575, val - 0.595.\n",
      "lr 0.0001, beta_1 0.854, beta_2 0.9961, l2 0.0195, filters 27, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 77, fold 0 trained 10 epochs in 3.982s.  Loss: tr - 0.358, val - 0.318.  Accuracy: tr - 0.908, val - 0.913.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 0 trained 10 epochs in 8.082s.  Loss: tr - 0.03, val - 0.024.  Accuracy: tr - 0.991, val - 0.991.\n",
      "lr 0.003, beta_1 0.9064, beta_2 0.9962, l2 0.011, filters 64, kernel_sz 5,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 79, fold 0 trained 10 epochs in 5.968s.  Loss: tr - 0.656, val - 0.631.  Accuracy: tr - 0.616, val - 0.655.\n",
      "lr 0.0051, beta_1 0.8541, beta_2 0.9906, l2 0.02, filters 41, kernel_sz 3,  dense_sz 135, activs tanh, padding 1\n",
      "Configuration 80, fold 0 trained 10 epochs in 5.189s.  Loss: tr - 0.659, val - 0.656.  Accuracy: tr - 0.588, val - 0.596.\n",
      "lr 0.0032, beta_1 0.8769, beta_2 0.9947, l2 0.0131, filters 55, kernel_sz 4,  dense_sz 113, activs relu, padding 1\n",
      "Configuration 81, fold 0 trained 10 epochs in 8.307s.  Loss: tr - 0.643, val - 0.622.  Accuracy: tr - 0.663, val - 0.688.\n",
      "lr 0.0011, beta_1 0.95, beta_2 0.9999, l2 0.0119, filters 35, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 82, fold 0 trained 10 epochs in 4.221s.  Loss: tr - 0.175, val - 0.146.  Accuracy: tr - 0.935, val - 0.949.\n",
      "lr 0.0073, beta_1 0.9184, beta_2 0.9975, l2 0.02, filters 63, kernel_sz 3,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 83, fold 0 trained 6 epochs in 4.284s.  Loss: tr - 34.65, val - 41.374.  Accuracy: tr - 0.558, val - 0.586.\n",
      "lr 0.0001, beta_1 0.8717, beta_2 0.9999, l2 0.0181, filters 41, kernel_sz 2,  dense_sz 126, activs relu, padding 0\n",
      "Configuration 84, fold 0 trained 10 epochs in 3.73s.  Loss: tr - 0.391, val - 0.358.  Accuracy: tr - 0.897, val - 0.899.\n",
      "lr 0.0032, beta_1 0.9345, beta_2 0.9904, l2 0.02, filters 51, kernel_sz 5,  dense_sz 56, activs relu, padding 1\n",
      "Configuration 85, fold 0 trained 10 epochs in 5.548s.  Loss: tr - 0.661, val - 0.662.  Accuracy: tr - 0.59, val - 0.586.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9949, l2 0.02, filters 63, kernel_sz 5,  dense_sz 29, activs tanh, padding 0\n",
      "Configuration 86, fold 0 trained 10 epochs in 4.81s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.594, val - 0.595.\n",
      "Not enough evidence to support a difference across group, after 1 folds (0.4650662412378788)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 1 on list of 30 configurations.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.999, l2 0.0033, filters 50, kernel_sz 5,  dense_sz 39, activs relu, padding 0\n",
      "Configuration 58, fold 1 trained 6 epochs in 2.526s.  Loss: tr - 35.042, val - 41.756.  Accuracy: tr - 0.573, val - 0.582.\n",
      "lr 0.0071, beta_1 0.9319, beta_2 0.9947, l2 0.02, filters 56, kernel_sz 5,  dense_sz 29, activs relu, padding 1\n",
      "Configuration 59, fold 1 trained 6 epochs in 3.457s.  Loss: tr - 44.767, val - 59.288.  Accuracy: tr - 0.484, val - 0.407.\n",
      "lr 0.0002, beta_1 0.85, beta_2 0.9955, l2 0.02, filters 50, kernel_sz 2,  dense_sz 137, activs tanh, padding 1\n",
      "Configuration 60, fold 1 trained 10 epochs in 5.614s.  Loss: tr - 0.195, val - 0.182.  Accuracy: tr - 0.981, val - 0.982.\n",
      "lr 0.0078, beta_1 0.935, beta_2 0.9942, l2 0.02, filters 53, kernel_sz 4,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 61, fold 1 trained 6 epochs in 5.26s.  Loss: tr - 47.487, val - 58.244.  Accuracy: tr - 0.45, val - 0.418.\n",
      "lr 0.0088, beta_1 0.8541, beta_2 0.9956, l2 0.02, filters 51, kernel_sz 2,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 62, fold 1 trained 10 epochs in 4.565s.  Loss: tr - 0.673, val - 0.655.  Accuracy: tr - 0.564, val - 0.592.\n",
      "lr 0.0001, beta_1 0.9173, beta_2 0.9997, l2 0.0173, filters 54, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 63, fold 1 trained 10 epochs in 6.332s.  Loss: tr - 0.622, val - 0.616.  Accuracy: tr - 0.64, val - 0.644.\n",
      "lr 0.0092, beta_1 0.9222, beta_2 0.9916, l2 0.02, filters 16, kernel_sz 4,  dense_sz 39, activs relu, padding 1\n",
      "Configuration 64, fold 1 trained 10 epochs in 2.2s.  Loss: tr - 0.67, val - 0.669.  Accuracy: tr - 0.592, val - 0.586.\n",
      "lr 0.0081, beta_1 0.905, beta_2 0.9999, l2 0.0061, filters 64, kernel_sz 5,  dense_sz 45, activs tanh, padding 0\n",
      "Configuration 65, fold 1 trained 10 epochs in 4.846s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.59, val - 0.593.\n",
      "lr 0.0016, beta_1 0.8781, beta_2 0.9996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 66, fold 1 trained 10 epochs in 7.104s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.595, val - 0.592.\n",
      "lr 0.0001, beta_1 0.9344, beta_2 0.999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 67, fold 1 trained 10 epochs in 7.092s.  Loss: tr - 0.248, val - 0.221.  Accuracy: tr - 0.958, val - 0.971.\n",
      "lr 0.0024, beta_1 0.85, beta_2 0.9979, l2 0.0183, filters 56, kernel_sz 3,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 68, fold 1 trained 10 epochs in 6.433s.  Loss: tr - 0.421, val - 0.419.  Accuracy: tr - 0.805, val - 0.8.\n",
      "lr 0.0051, beta_1 0.8946, beta_2 0.9968, l2 0.02, filters 64, kernel_sz 5,  dense_sz 102, activs relu, padding 1\n",
      "Configuration 69, fold 1 trained 10 epochs in 5.996s.  Loss: tr - 0.813, val - 0.676.  Accuracy: tr - 0.573, val - 0.557.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 5,  dense_sz 111, activs tanh, padding 1\n",
      "Configuration 70, fold 1 trained 10 epochs in 6.025s.  Loss: tr - 0.215, val - 0.203.  Accuracy: tr - 0.976, val - 0.983.\n",
      "lr 0.0017, beta_1 0.8601, beta_2 0.9905, l2 0.0117, filters 35, kernel_sz 5,  dense_sz 85, activs tanh, padding 1\n",
      "Configuration 71, fold 1 trained 10 epochs in 3.928s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "lr 0.0007, beta_1 0.85, beta_2 0.9985, l2 0.02, filters 64, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 72, fold 1 trained 10 epochs in 5.985s.  Loss: tr - 0.068, val - 0.056.  Accuracy: tr - 0.975, val - 0.979.\n",
      "lr 0.0057, beta_1 0.8899, beta_2 0.9999, l2 0.0075, filters 16, kernel_sz 5,  dense_sz 26, activs tanh, padding 1\n",
      "Configuration 73, fold 1 trained 10 epochs in 2.38s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "lr 0.0074, beta_1 0.8681, beta_2 0.9999, l2 0.0124, filters 27, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 74, fold 1 trained 6 epochs in 2.768s.  Loss: tr - 35.793, val - 42.392.  Accuracy: tr - 0.571, val - 0.576.\n",
      "lr 0.0004, beta_1 0.95, beta_2 0.9954, l2 0.02, filters 33, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 75, fold 1 trained 10 epochs in 3.14s.  Loss: tr - 0.477, val - 0.462.  Accuracy: tr - 0.733, val - 0.758.\n",
      "lr 0.0096, beta_1 0.9138, beta_2 0.995, l2 0.02, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 0\n",
      "Configuration 76, fold 1 trained 8 epochs in 3.851s.  Loss: tr - 41.899, val - 42.392.  Accuracy: tr - 0.581, val - 0.576.\n",
      "lr 0.0001, beta_1 0.854, beta_2 0.9961, l2 0.0195, filters 27, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 77, fold 1 trained 10 epochs in 4.014s.  Loss: tr - 0.439, val - 0.398.  Accuracy: tr - 0.886, val - 0.912.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 1 trained 10 epochs in 8.096s.  Loss: tr - 0.024, val - 0.021.  Accuracy: tr - 0.992, val - 0.994.\n",
      "lr 0.003, beta_1 0.9064, beta_2 0.9962, l2 0.011, filters 64, kernel_sz 5,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 79, fold 1 trained 8 epochs in 4.788s.  Loss: tr - 42.063, val - 41.756.  Accuracy: tr - 0.572, val - 0.582.\n",
      "lr 0.0051, beta_1 0.8541, beta_2 0.9906, l2 0.02, filters 41, kernel_sz 3,  dense_sz 135, activs tanh, padding 1\n",
      "Configuration 80, fold 1 trained 10 epochs in 5.479s.  Loss: tr - 0.663, val - 0.657.  Accuracy: tr - 0.584, val - 0.586.\n",
      "lr 0.0032, beta_1 0.8769, beta_2 0.9947, l2 0.0131, filters 55, kernel_sz 4,  dense_sz 113, activs relu, padding 1\n",
      "Configuration 81, fold 1 trained 10 epochs in 8.305s.  Loss: tr - 0.609, val - 0.562.  Accuracy: tr - 0.688, val - 0.709.\n",
      "lr 0.0011, beta_1 0.95, beta_2 0.9999, l2 0.0119, filters 35, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 82, fold 1 trained 10 epochs in 3.961s.  Loss: tr - 0.073, val - 0.054.  Accuracy: tr - 0.973, val - 0.983.\n",
      "lr 0.0073, beta_1 0.9184, beta_2 0.9975, l2 0.02, filters 63, kernel_sz 3,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 83, fold 1 trained 6 epochs in 4.287s.  Loss: tr - 34.985, val - 41.349.  Accuracy: tr - 0.568, val - 0.587.\n",
      "lr 0.0001, beta_1 0.8717, beta_2 0.9999, l2 0.0181, filters 41, kernel_sz 2,  dense_sz 126, activs relu, padding 0\n",
      "Configuration 84, fold 1 trained 10 epochs in 3.736s.  Loss: tr - 0.379, val - 0.342.  Accuracy: tr - 0.908, val - 0.93.\n",
      "lr 0.0032, beta_1 0.9345, beta_2 0.9904, l2 0.02, filters 51, kernel_sz 5,  dense_sz 56, activs relu, padding 1\n",
      "Configuration 85, fold 1 trained 8 epochs in 4.43s.  Loss: tr - 47.265, val - 42.392.  Accuracy: tr - 0.527, val - 0.576.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9949, l2 0.02, filters 63, kernel_sz 5,  dense_sz 29, activs tanh, padding 0\n",
      "Configuration 86, fold 1 trained 10 epochs in 5.109s.  Loss: tr - 0.646, val - 0.641.  Accuracy: tr - 0.661, val - 0.673.\n",
      "Significant difference in validation losses across group, after 2 folds (0.001897535675257289)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 2 on list of 30 configurations.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.999, l2 0.0033, filters 50, kernel_sz 5,  dense_sz 39, activs relu, padding 0\n",
      "Configuration 58, fold 2 trained 6 epochs in 2.538s.  Loss: tr - 35.873, val - 43.323.  Accuracy: tr - 0.55, val - 0.567.\n",
      "lr 0.0071, beta_1 0.9319, beta_2 0.9947, l2 0.02, filters 56, kernel_sz 5,  dense_sz 29, activs relu, padding 1\n",
      "Configuration 59, fold 2 trained 6 epochs in 3.431s.  Loss: tr - 34.981, val - 41.743.  Accuracy: tr - 0.55, val - 0.583.\n",
      "lr 0.0002, beta_1 0.85, beta_2 0.9955, l2 0.02, filters 50, kernel_sz 2,  dense_sz 137, activs tanh, padding 1\n",
      "Configuration 60, fold 2 trained 10 epochs in 5.628s.  Loss: tr - 0.171, val - 0.16.  Accuracy: tr - 0.985, val - 0.988.\n",
      "lr 0.0078, beta_1 0.935, beta_2 0.9942, l2 0.02, filters 53, kernel_sz 4,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 61, fold 2 trained 6 epochs in 4.968s.  Loss: tr - 34.895, val - 41.794.  Accuracy: tr - 0.569, val - 0.582.\n",
      "lr 0.0088, beta_1 0.8541, beta_2 0.9956, l2 0.02, filters 51, kernel_sz 2,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 62, fold 2 trained 10 epochs in 4.567s.  Loss: tr - 0.673, val - 0.656.  Accuracy: tr - 0.569, val - 0.59.\n",
      "lr 0.0001, beta_1 0.9173, beta_2 0.9997, l2 0.0173, filters 54, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 63, fold 2 trained 10 epochs in 6.329s.  Loss: tr - 0.503, val - 0.474.  Accuracy: tr - 0.78, val - 0.795.\n",
      "lr 0.0092, beta_1 0.9222, beta_2 0.9916, l2 0.02, filters 16, kernel_sz 4,  dense_sz 39, activs relu, padding 1\n",
      "Configuration 64, fold 2 trained 10 epochs in 2.23s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0081, beta_1 0.905, beta_2 0.9999, l2 0.0061, filters 64, kernel_sz 5,  dense_sz 45, activs tanh, padding 0\n",
      "Configuration 65, fold 2 trained 10 epochs in 5.134s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.592, val - 0.591.\n",
      "lr 0.0016, beta_1 0.8781, beta_2 0.9996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 66, fold 2 trained 8 epochs in 5.701s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.592, val - 0.591.\n",
      "lr 0.0001, beta_1 0.9344, beta_2 0.999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 67, fold 2 trained 10 epochs in 6.805s.  Loss: tr - 0.303, val - 0.254.  Accuracy: tr - 0.939, val - 0.96.\n",
      "lr 0.0024, beta_1 0.85, beta_2 0.9979, l2 0.0183, filters 56, kernel_sz 3,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 68, fold 2 trained 10 epochs in 6.413s.  Loss: tr - 0.052, val - 0.042.  Accuracy: tr - 0.982, val - 0.986.\n",
      "lr 0.0051, beta_1 0.8946, beta_2 0.9968, l2 0.02, filters 64, kernel_sz 5,  dense_sz 102, activs relu, padding 1\n",
      "Configuration 69, fold 2 trained 6 epochs in 3.591s.  Loss: tr - 33.872, val - 40.928.  Accuracy: tr - 0.587, val - 0.591.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 5,  dense_sz 111, activs tanh, padding 1\n",
      "Configuration 70, fold 2 trained 10 epochs in 6.031s.  Loss: tr - 0.246, val - 0.225.  Accuracy: tr - 0.963, val - 0.974.\n",
      "lr 0.0017, beta_1 0.8601, beta_2 0.9905, l2 0.0117, filters 35, kernel_sz 5,  dense_sz 85, activs tanh, padding 1\n",
      "Configuration 71, fold 2 trained 10 epochs in 4.221s.  Loss: tr - 0.659, val - 0.654.  Accuracy: tr - 0.582, val - 0.59.\n",
      "lr 0.0007, beta_1 0.85, beta_2 0.9985, l2 0.02, filters 64, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 72, fold 2 trained 10 epochs in 6.034s.  Loss: tr - 0.051, val - 0.046.  Accuracy: tr - 0.983, val - 0.985.\n",
      "lr 0.0057, beta_1 0.8899, beta_2 0.9999, l2 0.0075, filters 16, kernel_sz 5,  dense_sz 26, activs tanh, padding 1\n",
      "Configuration 73, fold 2 trained 10 epochs in 2.102s.  Loss: tr - 0.657, val - 0.654.  Accuracy: tr - 0.594, val - 0.591.\n",
      "lr 0.0074, beta_1 0.8681, beta_2 0.9999, l2 0.0124, filters 27, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 74, fold 2 trained 6 epochs in 2.751s.  Loss: tr - 39.551, val - 46.432.  Accuracy: tr - 0.532, val - 0.536.\n",
      "lr 0.0004, beta_1 0.95, beta_2 0.9954, l2 0.02, filters 33, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 75, fold 2 trained 10 epochs in 3.159s.  Loss: tr - 0.535, val - 0.513.  Accuracy: tr - 0.721, val - 0.73.\n",
      "lr 0.0096, beta_1 0.9138, beta_2 0.995, l2 0.02, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 0\n",
      "Configuration 76, fold 2 trained 6 epochs in 2.887s.  Loss: tr - 42.316, val - 51.223.  Accuracy: tr - 0.495, val - 0.488.\n",
      "lr 0.0001, beta_1 0.854, beta_2 0.9961, l2 0.0195, filters 27, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 77, fold 2 trained 10 epochs in 3.98s.  Loss: tr - 0.498, val - 0.463.  Accuracy: tr - 0.84, val - 0.878.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 2 trained 10 epochs in 8.346s.  Loss: tr - 0.067, val - 0.051.  Accuracy: tr - 0.979, val - 0.986.\n",
      "lr 0.003, beta_1 0.9064, beta_2 0.9962, l2 0.011, filters 64, kernel_sz 5,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 79, fold 2 trained 10 epochs in 5.986s.  Loss: tr - 54.87, val - 49.592.  Accuracy: tr - 0.451, val - 0.504.\n",
      "lr 0.0051, beta_1 0.8541, beta_2 0.9906, l2 0.02, filters 41, kernel_sz 3,  dense_sz 135, activs tanh, padding 1\n",
      "Configuration 80, fold 2 trained 10 epochs in 5.184s.  Loss: tr - 0.664, val - 0.654.  Accuracy: tr - 0.581, val - 0.591.\n",
      "lr 0.0032, beta_1 0.8769, beta_2 0.9947, l2 0.0131, filters 55, kernel_sz 4,  dense_sz 113, activs relu, padding 1\n",
      "Configuration 81, fold 2 trained 10 epochs in 8.284s.  Loss: tr - 0.633, val - 0.578.  Accuracy: tr - 0.685, val - 0.701.\n",
      "lr 0.0011, beta_1 0.95, beta_2 0.9999, l2 0.0119, filters 35, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 82, fold 2 trained 10 epochs in 3.921s.  Loss: tr - 0.073, val - 0.07.  Accuracy: tr - 0.973, val - 0.977.\n",
      "lr 0.0073, beta_1 0.9184, beta_2 0.9975, l2 0.02, filters 63, kernel_sz 3,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 83, fold 2 trained 6 epochs in 4.285s.  Loss: tr - 36.295, val - 44.088.  Accuracy: tr - 0.549, val - 0.559.\n",
      "lr 0.0001, beta_1 0.8717, beta_2 0.9999, l2 0.0181, filters 41, kernel_sz 2,  dense_sz 126, activs relu, padding 0\n",
      "Configuration 84, fold 2 trained 10 epochs in 4.024s.  Loss: tr - 0.414, val - 0.383.  Accuracy: tr - 0.895, val - 0.915.\n",
      "lr 0.0032, beta_1 0.9345, beta_2 0.9904, l2 0.02, filters 51, kernel_sz 5,  dense_sz 56, activs relu, padding 1\n",
      "Configuration 85, fold 2 trained 6 epochs in 3.334s.  Loss: tr - 32.942, val - 40.928.  Accuracy: tr - 0.566, val - 0.591.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9949, l2 0.02, filters 63, kernel_sz 5,  dense_sz 29, activs tanh, padding 0\n",
      "Configuration 86, fold 2 trained 10 epochs in 4.821s.  Loss: tr - 0.546, val - 0.529.  Accuracy: tr - 0.756, val - 0.774.\n",
      "Significant difference in validation losses across group, after 3 folds (4.4656651244499147e-07)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 3 on list of 30 configurations.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.999, l2 0.0033, filters 50, kernel_sz 5,  dense_sz 39, activs relu, padding 0\n",
      "Configuration 58, fold 3 trained 6 epochs in 2.522s.  Loss: tr - 35.09, val - 40.596.  Accuracy: tr - 0.574, val - 0.594.\n",
      "lr 0.0071, beta_1 0.9319, beta_2 0.9947, l2 0.02, filters 56, kernel_sz 5,  dense_sz 29, activs relu, padding 1\n",
      "Configuration 59, fold 3 trained 6 epochs in 3.436s.  Loss: tr - 32.402, val - 39.832.  Accuracy: tr - 0.558, val - 0.602.\n",
      "lr 0.0002, beta_1 0.85, beta_2 0.9955, l2 0.02, filters 50, kernel_sz 2,  dense_sz 137, activs tanh, padding 1\n",
      "Configuration 60, fold 3 trained 10 epochs in 5.607s.  Loss: tr - 0.281, val - 0.25.  Accuracy: tr - 0.951, val - 0.958.\n",
      "lr 0.0078, beta_1 0.935, beta_2 0.9942, l2 0.02, filters 53, kernel_sz 4,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 61, fold 3 trained 6 epochs in 4.966s.  Loss: tr - 36.404, val - 44.623.  Accuracy: tr - 0.549, val - 0.554.\n",
      "lr 0.0088, beta_1 0.8541, beta_2 0.9956, l2 0.02, filters 51, kernel_sz 2,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 62, fold 3 trained 10 epochs in 4.567s.  Loss: tr - 0.663, val - 0.65.  Accuracy: tr - 0.584, val - 0.607.\n",
      "lr 0.0001, beta_1 0.9173, beta_2 0.9997, l2 0.0173, filters 54, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 63, fold 3 trained 10 epochs in 6.644s.  Loss: tr - 0.561, val - 0.539.  Accuracy: tr - 0.683, val - 0.706.\n",
      "lr 0.0092, beta_1 0.9222, beta_2 0.9916, l2 0.02, filters 16, kernel_sz 4,  dense_sz 39, activs relu, padding 1\n",
      "Configuration 64, fold 3 trained 10 epochs in 2.232s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0081, beta_1 0.905, beta_2 0.9999, l2 0.0061, filters 64, kernel_sz 5,  dense_sz 45, activs tanh, padding 0\n",
      "Configuration 65, fold 3 trained 10 epochs in 4.82s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.59, val - 0.599.\n",
      "lr 0.0016, beta_1 0.8781, beta_2 0.9996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 66, fold 3 trained 9 epochs in 6.39s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.588, val - 0.607.\n",
      "lr 0.0001, beta_1 0.9344, beta_2 0.999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 67, fold 3 trained 10 epochs in 6.8s.  Loss: tr - 0.286, val - 0.251.  Accuracy: tr - 0.94, val - 0.947.\n",
      "lr 0.0024, beta_1 0.85, beta_2 0.9979, l2 0.0183, filters 56, kernel_sz 3,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 68, fold 3 trained 10 epochs in 6.43s.  Loss: tr - 0.05, val - 0.051.  Accuracy: tr - 0.983, val - 0.982.\n",
      "lr 0.0051, beta_1 0.8946, beta_2 0.9968, l2 0.02, filters 64, kernel_sz 5,  dense_sz 102, activs relu, padding 1\n",
      "Configuration 69, fold 3 trained 6 epochs in 3.607s.  Loss: tr - 35.102, val - 40.596.  Accuracy: tr - 0.563, val - 0.594.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 5,  dense_sz 111, activs tanh, padding 1\n",
      "Configuration 70, fold 3 trained 10 epochs in 6.32s.  Loss: tr - 0.224, val - 0.219.  Accuracy: tr - 0.972, val - 0.968.\n",
      "lr 0.0017, beta_1 0.8601, beta_2 0.9905, l2 0.0117, filters 35, kernel_sz 5,  dense_sz 85, activs tanh, padding 1\n",
      "Configuration 71, fold 3 trained 9 epochs in 3.535s.  Loss: tr - 0.659, val - 0.65.  Accuracy: tr - 0.573, val - 0.607.\n",
      "lr 0.0007, beta_1 0.85, beta_2 0.9985, l2 0.02, filters 64, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 72, fold 3 trained 10 epochs in 5.992s.  Loss: tr - 0.032, val - 0.038.  Accuracy: tr - 0.99, val - 0.988.\n",
      "lr 0.0057, beta_1 0.8899, beta_2 0.9999, l2 0.0075, filters 16, kernel_sz 5,  dense_sz 26, activs tanh, padding 1\n",
      "Configuration 73, fold 3 trained 10 epochs in 2.097s.  Loss: tr - 0.658, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0074, beta_1 0.8681, beta_2 0.9999, l2 0.0124, filters 27, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 74, fold 3 trained 6 epochs in 2.758s.  Loss: tr - 34.475, val - 40.698.  Accuracy: tr - 0.575, val - 0.593.\n",
      "lr 0.0004, beta_1 0.95, beta_2 0.9954, l2 0.02, filters 33, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 75, fold 3 trained 10 epochs in 3.167s.  Loss: tr - 0.542, val - 0.522.  Accuracy: tr - 0.762, val - 0.768.\n",
      "lr 0.0096, beta_1 0.9138, beta_2 0.995, l2 0.02, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 0\n",
      "Configuration 76, fold 3 trained 6 epochs in 3.207s.  Loss: tr - 33.964, val - 40.138.  Accuracy: tr - 0.564, val - 0.599.\n",
      "lr 0.0001, beta_1 0.854, beta_2 0.9961, l2 0.0195, filters 27, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 77, fold 3 trained 10 epochs in 3.994s.  Loss: tr - 0.507, val - 0.481.  Accuracy: tr - 0.804, val - 0.83.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 3 trained 10 epochs in 8.05s.  Loss: tr - 0.023, val - 0.027.  Accuracy: tr - 0.993, val - 0.99.\n",
      "lr 0.003, beta_1 0.9064, beta_2 0.9962, l2 0.011, filters 64, kernel_sz 5,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 79, fold 3 trained 6 epochs in 3.582s.  Loss: tr - 40.124, val - 50.28.  Accuracy: tr - 0.522, val - 0.497.\n",
      "lr 0.0051, beta_1 0.8541, beta_2 0.9906, l2 0.02, filters 41, kernel_sz 3,  dense_sz 135, activs tanh, padding 1\n",
      "Configuration 80, fold 3 trained 10 epochs in 5.194s.  Loss: tr - 0.66, val - 0.652.  Accuracy: tr - 0.588, val - 0.599.\n",
      "lr 0.0032, beta_1 0.8769, beta_2 0.9947, l2 0.0131, filters 55, kernel_sz 4,  dense_sz 113, activs relu, padding 1\n",
      "Configuration 81, fold 3 trained 6 epochs in 4.985s.  Loss: tr - 35.83, val - 41.157.  Accuracy: tr - 0.554, val - 0.588.\n",
      "lr 0.0011, beta_1 0.95, beta_2 0.9999, l2 0.0119, filters 35, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 82, fold 3 trained 10 epochs in 3.948s.  Loss: tr - 0.118, val - 0.114.  Accuracy: tr - 0.953, val - 0.956.\n",
      "lr 0.0073, beta_1 0.9184, beta_2 0.9975, l2 0.02, filters 63, kernel_sz 3,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 83, fold 3 trained 6 epochs in 4.284s.  Loss: tr - 32.107, val - 39.847.  Accuracy: tr - 0.54, val - 0.593.\n",
      "lr 0.0001, beta_1 0.8717, beta_2 0.9999, l2 0.0181, filters 41, kernel_sz 2,  dense_sz 126, activs relu, padding 0\n",
      "Configuration 84, fold 3 trained 10 epochs in 4.033s.  Loss: tr - 0.415, val - 0.391.  Accuracy: tr - 0.914, val - 0.908.\n",
      "lr 0.0032, beta_1 0.9345, beta_2 0.9904, l2 0.02, filters 51, kernel_sz 5,  dense_sz 56, activs relu, padding 1\n",
      "Configuration 85, fold 3 trained 6 epochs in 3.313s.  Loss: tr - 34.126, val - 39.832.  Accuracy: tr - 0.565, val - 0.602.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9949, l2 0.02, filters 63, kernel_sz 5,  dense_sz 29, activs tanh, padding 0\n",
      "Configuration 86, fold 3 trained 10 epochs in 4.803s.  Loss: tr - 0.647, val - 0.64.  Accuracy: tr - 0.654, val - 0.666.\n",
      "Significant difference in validation losses across group, after 4 folds (4.5962436849168637e-11)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 4 on list of 30 configurations.\n",
      "Configuration 13, evaluated once before -- skipping.\n",
      "lr 0.01, beta_1 0.85, beta_2 0.999, l2 0.0033, filters 50, kernel_sz 5,  dense_sz 39, activs relu, padding 0\n",
      "Configuration 58, fold 4 trained 6 epochs in 2.52s.  Loss: tr - 35.267, val - 42.278.  Accuracy: tr - 0.556, val - 0.577.\n",
      "lr 0.0071, beta_1 0.9319, beta_2 0.9947, l2 0.02, filters 56, kernel_sz 5,  dense_sz 29, activs relu, padding 1\n",
      "Configuration 59, fold 4 trained 6 epochs in 3.441s.  Loss: tr - 34.944, val - 41.412.  Accuracy: tr - 0.569, val - 0.586.\n",
      "lr 0.0002, beta_1 0.85, beta_2 0.9955, l2 0.02, filters 50, kernel_sz 2,  dense_sz 137, activs tanh, padding 1\n",
      "Configuration 60, fold 4 trained 10 epochs in 5.626s.  Loss: tr - 0.228, val - 0.207.  Accuracy: tr - 0.97, val - 0.979.\n",
      "lr 0.0078, beta_1 0.935, beta_2 0.9942, l2 0.02, filters 53, kernel_sz 4,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 61, fold 4 trained 6 epochs in 4.971s.  Loss: tr - 48.465, val - 58.588.  Accuracy: tr - 0.445, val - 0.414.\n",
      "lr 0.0088, beta_1 0.8541, beta_2 0.9956, l2 0.02, filters 51, kernel_sz 2,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 62, fold 4 trained 10 epochs in 4.859s.  Loss: tr - 0.663, val - 0.656.  Accuracy: tr - 0.586, val - 0.572.\n",
      "lr 0.0001, beta_1 0.9173, beta_2 0.9997, l2 0.0173, filters 54, kernel_sz 3,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 63, fold 4 trained 10 epochs in 6.341s.  Loss: tr - 0.38, val - 0.343.  Accuracy: tr - 0.867, val - 0.883.\n",
      "lr 0.0092, beta_1 0.9222, beta_2 0.9916, l2 0.02, filters 16, kernel_sz 4,  dense_sz 39, activs relu, padding 1\n",
      "Configuration 64, fold 4 trained 10 epochs in 2.25s.  Loss: tr - 1.172, val - 0.665.  Accuracy: tr - 0.576, val - 0.592.\n",
      "lr 0.0081, beta_1 0.905, beta_2 0.9999, l2 0.0061, filters 64, kernel_sz 5,  dense_sz 45, activs tanh, padding 0\n",
      "Configuration 65, fold 4 trained 10 epochs in 4.827s.  Loss: tr - 0.656, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "lr 0.0016, beta_1 0.8781, beta_2 0.9996, l2 0.02, filters 55, kernel_sz 4,  dense_sz 150, activs tanh, padding 0\n",
      "Configuration 66, fold 4 trained 10 epochs in 7.107s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.593.\n",
      "lr 0.0001, beta_1 0.9344, beta_2 0.999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 67, fold 4 trained 10 epochs in 6.797s.  Loss: tr - 0.242, val - 0.214.  Accuracy: tr - 0.961, val - 0.975.\n",
      "lr 0.0024, beta_1 0.85, beta_2 0.9979, l2 0.0183, filters 56, kernel_sz 3,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 68, fold 4 trained 10 epochs in 6.416s.  Loss: tr - 0.275, val - 0.217.  Accuracy: tr - 0.892, val - 0.917.\n",
      "lr 0.0051, beta_1 0.8946, beta_2 0.9968, l2 0.02, filters 64, kernel_sz 5,  dense_sz 102, activs relu, padding 1\n",
      "Configuration 69, fold 4 trained 6 epochs in 3.885s.  Loss: tr - 36.491, val - 42.686.  Accuracy: tr - 0.543, val - 0.573.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 5,  dense_sz 111, activs tanh, padding 1\n",
      "Configuration 70, fold 4 trained 10 epochs in 6.031s.  Loss: tr - 0.218, val - 0.208.  Accuracy: tr - 0.973, val - 0.977.\n",
      "lr 0.0017, beta_1 0.8601, beta_2 0.9905, l2 0.0117, filters 35, kernel_sz 5,  dense_sz 85, activs tanh, padding 1\n",
      "Configuration 71, fold 4 trained 10 epochs in 3.936s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.593, val - 0.592.\n",
      "lr 0.0007, beta_1 0.85, beta_2 0.9985, l2 0.02, filters 64, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 72, fold 4 trained 10 epochs in 6.027s.  Loss: tr - 0.041, val - 0.039.  Accuracy: tr - 0.987, val - 0.989.\n",
      "lr 0.0057, beta_1 0.8899, beta_2 0.9999, l2 0.0075, filters 16, kernel_sz 5,  dense_sz 26, activs tanh, padding 1\n",
      "Configuration 73, fold 4 trained 10 epochs in 2.111s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.592, val - 0.593.\n",
      "lr 0.0074, beta_1 0.8681, beta_2 0.9999, l2 0.0124, filters 27, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 74, fold 4 trained 6 epochs in 2.766s.  Loss: tr - 32.226, val - 42.992.  Accuracy: tr - 0.543, val - 0.57.\n",
      "lr 0.0004, beta_1 0.95, beta_2 0.9954, l2 0.02, filters 33, kernel_sz 2,  dense_sz 25, activs relu, padding 0\n",
      "Configuration 75, fold 4 trained 10 epochs in 3.445s.  Loss: tr - 0.428, val - 0.403.  Accuracy: tr - 0.826, val - 0.835.\n",
      "lr 0.0096, beta_1 0.9138, beta_2 0.995, l2 0.02, filters 64, kernel_sz 5,  dense_sz 118, activs relu, padding 0\n",
      "Configuration 76, fold 4 trained 10 epochs in 4.81s.  Loss: tr - 46.21, val - 46.445.  Accuracy: tr - 0.538, val - 0.535.\n",
      "lr 0.0001, beta_1 0.854, beta_2 0.9961, l2 0.0195, filters 27, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 77, fold 4 trained 10 epochs in 3.991s.  Loss: tr - 0.399, val - 0.354.  Accuracy: tr - 0.902, val - 0.91.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 4 trained 10 epochs in 8.062s.  Loss: tr - 0.052, val - 0.04.  Accuracy: tr - 0.983, val - 0.989.\n",
      "lr 0.003, beta_1 0.9064, beta_2 0.9962, l2 0.011, filters 64, kernel_sz 5,  dense_sz 25, activs relu, padding 1\n",
      "Configuration 79, fold 4 trained 10 epochs in 5.98s.  Loss: tr - 0.656, val - 0.653.  Accuracy: tr - 0.594, val - 0.595.\n",
      "lr 0.0051, beta_1 0.8541, beta_2 0.9906, l2 0.02, filters 41, kernel_sz 3,  dense_sz 135, activs tanh, padding 1\n",
      "Configuration 80, fold 4 trained 10 epochs in 5.182s.  Loss: tr - 0.667, val - 0.655.  Accuracy: tr - 0.584, val - 0.587.\n",
      "lr 0.0032, beta_1 0.8769, beta_2 0.9947, l2 0.0131, filters 55, kernel_sz 4,  dense_sz 113, activs relu, padding 1\n",
      "Configuration 81, fold 4 trained 10 epochs in 8.28s.  Loss: tr - 0.664, val - 0.656.  Accuracy: tr - 0.578, val - 0.594.\n",
      "lr 0.0011, beta_1 0.95, beta_2 0.9999, l2 0.0119, filters 35, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 82, fold 4 trained 10 epochs in 4.214s.  Loss: tr - 0.074, val - 0.06.  Accuracy: tr - 0.972, val - 0.978.\n",
      "lr 0.0073, beta_1 0.9184, beta_2 0.9975, l2 0.02, filters 63, kernel_sz 3,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 83, fold 4 trained 6 epochs in 4.274s.  Loss: tr - 32.531, val - 40.8.  Accuracy: tr - 0.571, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8717, beta_2 0.9999, l2 0.0181, filters 41, kernel_sz 2,  dense_sz 126, activs relu, padding 0\n",
      "Configuration 84, fold 4 trained 10 epochs in 3.742s.  Loss: tr - 0.321, val - 0.281.  Accuracy: tr - 0.928, val - 0.937.\n",
      "lr 0.0032, beta_1 0.9345, beta_2 0.9904, l2 0.02, filters 51, kernel_sz 5,  dense_sz 56, activs relu, padding 1\n",
      "Configuration 85, fold 4 trained 10 epochs in 5.542s.  Loss: tr - 0.628, val - 0.612.  Accuracy: tr - 0.706, val - 0.701.\n",
      "lr 0.0001, beta_1 0.95, beta_2 0.9949, l2 0.02, filters 63, kernel_sz 5,  dense_sz 29, activs tanh, padding 0\n",
      "Configuration 86, fold 4 trained 10 epochs in 4.811s.  Loss: tr - 0.653, val - 0.651.  Accuracy: tr - 0.594, val - 0.593.\n",
      "Significant difference in validation losses across group, after 5 folds (1.8363705410003993e-15)\n",
      "Configuration 13 (mean vloss 0.048762496560811996) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 82 (mean vloss 0.08904142379760742) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 60 (mean vloss 0.20483945906162263) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 70 (mean vloss 0.21063591539859772) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 67 (mean vloss 0.23328933715820313) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 84 (mean vloss 0.3508871555328369) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 77 (mean vloss 0.40242921710014345) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 63 (mean vloss 0.4531353533267975) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 75 (mean vloss 0.47913891077041626) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 86 (mean vloss 0.623078465461731) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 71 (mean vloss 0.6536356806755066) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 73 (mean vloss 0.6537678241729736) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 65 (mean vloss 0.6538389205932618) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 66 (mean vloss 0.6541123747825622) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 80 (mean vloss 0.6548165678977966) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 62 (mean vloss 0.6549193859100342) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 64 (mean vloss 0.6593885660171509) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 81 (mean vloss 8.715226948261261) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 85 (mean vloss 24.88502320051193) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 79 (mean vloss 28.582439851760864) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 69 (mean vloss 33.25205295085907) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 83 (mean vloss 41.49153442382813) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 74 (mean vloss 42.86924743652344) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 58 (mean vloss 43.93166275024414) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 76 (mean vloss 44.14638061523438) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 59 (mean vloss 45.162234497070315) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 61 (mean vloss 50.95521697998047) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Paired tests completed\n",
      "Iteration completed in 694.112s. Remaining configurations 3.  Mean validation loss 0.078 and acc 0.97\n",
      "---------------------------------------------------------------------------\n",
      "####################################################################################################\n",
      "Iteration 4 of Iterated F-race.\n",
      "Training fold 0 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0075, beta_1 0.9001, beta_2 0.9999, l2 0.008, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 87, fold 0 trained 6 epochs in 2.915s.  Loss: tr - 37.278, val - 44.835.  Accuracy: tr - 0.536, val - 0.552.\n",
      "lr 0.0001, beta_1 0.858, beta_2 0.9968, l2 0.02, filters 50, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 88, fold 0 trained 10 epochs in 8.12s.  Loss: tr - 0.081, val - 0.065.  Accuracy: tr - 0.978, val - 0.985.\n",
      "lr 0.004, beta_1 0.8738, beta_2 0.9971, l2 0.02, filters 16, kernel_sz 4,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 89, fold 0 trained 10 epochs in 2.207s.  Loss: tr - 0.116, val - 0.083.  Accuracy: tr - 0.959, val - 0.968.\n",
      "lr 0.0001, beta_1 0.8591, beta_2 0.9989, l2 0.0111, filters 64, kernel_sz 4,  dense_sz 99, activs relu, padding 1\n",
      "Configuration 90, fold 0 trained 10 epochs in 9.021s.  Loss: tr - 0.086, val - 0.069.  Accuracy: tr - 0.975, val - 0.981.\n",
      "lr 0.0018, beta_1 0.85, beta_2 0.9992, l2 0.02, filters 38, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 91, fold 0 trained 10 epochs in 6.308s.  Loss: tr - 0.122, val - 0.097.  Accuracy: tr - 0.955, val - 0.964.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9985, l2 0.0189, filters 52, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 92, fold 0 trained 10 epochs in 6.205s.  Loss: tr - 0.171, val - 0.139.  Accuracy: tr - 0.96, val - 0.964.\n",
      "lr 0.0023, beta_1 0.85, beta_2 0.9991, l2 0.0095, filters 44, kernel_sz 5,  dense_sz 149, activs relu, padding 1\n",
      "Configuration 93, fold 0 trained 10 epochs in 4.81s.  Loss: tr - 0.656, val - 0.651.  Accuracy: tr - 0.593, val - 0.608.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 0 trained 10 epochs in 2.505s.  Loss: tr - 0.037, val - 0.031.  Accuracy: tr - 0.989, val - 0.991.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 0 trained 10 epochs in 5.547s.  Loss: tr - 0.143, val - 0.105.  Accuracy: tr - 0.949, val - 0.963.\n",
      "lr 0.0039, beta_1 0.9007, beta_2 0.9999, l2 0.0075, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 96, fold 0 trained 6 epochs in 5.413s.  Loss: tr - 35.953, val - 42.519.  Accuracy: tr - 0.556, val - 0.575.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 0 trained 10 epochs in 8.994s.  Loss: tr - 0.09, val - 0.06.  Accuracy: tr - 0.97, val - 0.98.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.0186, filters 48, kernel_sz 5,  dense_sz 127, activs relu, padding 1\n",
      "Configuration 98, fold 0 trained 10 epochs in 5.001s.  Loss: tr - 0.189, val - 0.156.  Accuracy: tr - 0.939, val - 0.954.\n",
      "lr 0.0055, beta_1 0.9174, beta_2 0.998, l2 0.0158, filters 51, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 99, fold 0 trained 6 epochs in 4.84s.  Loss: tr - 31.447, val - 41.374.  Accuracy: tr - 0.563, val - 0.586.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 0 trained 10 epochs in 9.038s.  Loss: tr - 0.073, val - 0.06.  Accuracy: tr - 0.977, val - 0.984.\n",
      "lr 0.0009, beta_1 0.8822, beta_2 0.9997, l2 0.0179, filters 46, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 101, fold 0 trained 10 epochs in 5.903s.  Loss: tr - 0.084, val - 0.081.  Accuracy: tr - 0.988, val - 0.989.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 0 trained 10 epochs in 5.094s.  Loss: tr - 0.042, val - 0.033.  Accuracy: tr - 0.985, val - 0.989.\n",
      "lr 0.0001, beta_1 0.9078, beta_2 0.9988, l2 0.02, filters 64, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 103, fold 0 trained 10 epochs in 9.014s.  Loss: tr - 0.064, val - 0.053.  Accuracy: tr - 0.982, val - 0.984.\n",
      "lr 0.0001, beta_1 0.8996, beta_2 0.9999, l2 0.0172, filters 64, kernel_sz 5,  dense_sz 139, activs relu, padding 0\n",
      "Configuration 104, fold 0 trained 10 epochs in 4.833s.  Loss: tr - 0.284, val - 0.262.  Accuracy: tr - 0.893, val - 0.906.\n",
      "lr 0.0009, beta_1 0.8939, beta_2 0.9941, l2 0.02, filters 55, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 105, fold 0 trained 10 epochs in 8.337s.  Loss: tr - 0.042, val - 0.036.  Accuracy: tr - 0.986, val - 0.989.\n",
      "lr 0.0001, beta_1 0.9181, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 106, fold 0 trained 10 epochs in 9.028s.  Loss: tr - 0.239, val - 0.189.  Accuracy: tr - 0.937, val - 0.946.\n",
      "lr 0.0067, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 107, fold 0 trained 6 epochs in 4.34s.  Loss: tr - 41.711, val - 50.433.  Accuracy: tr - 0.501, val - 0.496.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 0 trained 10 epochs in 6.745s.  Loss: tr - 0.083, val - 0.064.  Accuracy: tr - 0.972, val - 0.98.\n",
      "lr 0.0036, beta_1 0.8581, beta_2 0.9999, l2 0.0161, filters 57, kernel_sz 2,  dense_sz 122, activs relu, padding 1\n",
      "Configuration 109, fold 0 trained 6 epochs in 4.515s.  Loss: tr - 36.438, val - 42.672.  Accuracy: tr - 0.549, val - 0.573.\n",
      "lr 0.0027, beta_1 0.9029, beta_2 0.9999, l2 0.02, filters 55, kernel_sz 2,  dense_sz 92, activs relu, padding 1\n",
      "Configuration 110, fold 0 trained 10 epochs in 7.235s.  Loss: tr - 0.549, val - 0.422.  Accuracy: tr - 0.738, val - 0.847.\n",
      "lr 0.0007, beta_1 0.8873, beta_2 0.9999, l2 0.02, filters 51, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 111, fold 0 trained 10 epochs in 8.123s.  Loss: tr - 0.102, val - 0.099.  Accuracy: tr - 0.986, val - 0.987.\n",
      "lr 0.0025, beta_1 0.8949, beta_2 0.9935, l2 0.02, filters 42, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 112, fold 0 trained 10 epochs in 4.296s.  Loss: tr - 0.045, val - 0.037.  Accuracy: tr - 0.986, val - 0.99.\n",
      "lr 0.0001, beta_1 0.8683, beta_2 0.9949, l2 0.02, filters 29, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 113, fold 0 trained 10 epochs in 5.467s.  Loss: tr - 0.217, val - 0.183.  Accuracy: tr - 0.945, val - 0.949.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 0 trained 10 epochs in 5.936s.  Loss: tr - 0.068, val - 0.052.  Accuracy: tr - 0.976, val - 0.98.\n",
      "lr 0.002, beta_1 0.8552, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 133, activs relu, padding 1\n",
      "Configuration 115, fold 0 trained 10 epochs in 8.983s.  Loss: tr - 0.375, val - 0.239.  Accuracy: tr - 0.848, val - 0.915.\n",
      "Not enough evidence to support a difference across group, after 1 folds (0.46506624123787865)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 1 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0075, beta_1 0.9001, beta_2 0.9999, l2 0.008, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 87, fold 1 trained 6 epochs in 2.929s.  Loss: tr - 43.514, val - 53.588.  Accuracy: tr - 0.484, val - 0.464.\n",
      "lr 0.0001, beta_1 0.858, beta_2 0.9968, l2 0.02, filters 50, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 88, fold 1 trained 10 epochs in 7.81s.  Loss: tr - 0.102, val - 0.087.  Accuracy: tr - 0.972, val - 0.978.\n",
      "lr 0.004, beta_1 0.8738, beta_2 0.9971, l2 0.02, filters 16, kernel_sz 4,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 89, fold 1 trained 10 epochs in 2.21s.  Loss: tr - 0.111, val - 0.065.  Accuracy: tr - 0.959, val - 0.977.\n",
      "lr 0.0001, beta_1 0.8591, beta_2 0.9989, l2 0.0111, filters 64, kernel_sz 4,  dense_sz 99, activs relu, padding 1\n",
      "Configuration 90, fold 1 trained 10 epochs in 9.024s.  Loss: tr - 0.176, val - 0.142.  Accuracy: tr - 0.949, val - 0.957.\n",
      "lr 0.0018, beta_1 0.85, beta_2 0.9992, l2 0.02, filters 38, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 91, fold 1 trained 10 epochs in 6.615s.  Loss: tr - 0.08, val - 0.06.  Accuracy: tr - 0.97, val - 0.979.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9985, l2 0.0189, filters 52, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 92, fold 1 trained 10 epochs in 6.217s.  Loss: tr - 0.252, val - 0.208.  Accuracy: tr - 0.931, val - 0.954.\n",
      "lr 0.0023, beta_1 0.85, beta_2 0.9991, l2 0.0095, filters 44, kernel_sz 5,  dense_sz 149, activs relu, padding 1\n",
      "Configuration 93, fold 1 trained 10 epochs in 4.815s.  Loss: tr - 0.65, val - 0.622.  Accuracy: tr - 0.601, val - 0.64.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 1 trained 10 epochs in 2.208s.  Loss: tr - 0.042, val - 0.037.  Accuracy: tr - 0.987, val - 0.988.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 1 trained 10 epochs in 5.545s.  Loss: tr - 0.131, val - 0.105.  Accuracy: tr - 0.948, val - 0.961.\n",
      "lr 0.0039, beta_1 0.9007, beta_2 0.9999, l2 0.0075, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 96, fold 1 trained 6 epochs in 5.414s.  Loss: tr - 44.236, val - 55.14.  Accuracy: tr - 0.479, val - 0.449.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 1 trained 10 epochs in 8.988s.  Loss: tr - 0.048, val - 0.036.  Accuracy: tr - 0.984, val - 0.989.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.0186, filters 48, kernel_sz 5,  dense_sz 127, activs relu, padding 1\n",
      "Configuration 98, fold 1 trained 10 epochs in 5.292s.  Loss: tr - 0.128, val - 0.106.  Accuracy: tr - 0.96, val - 0.973.\n",
      "lr 0.0055, beta_1 0.9174, beta_2 0.998, l2 0.0158, filters 51, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 99, fold 1 trained 6 epochs in 4.841s.  Loss: tr - 35.632, val - 43.257.  Accuracy: tr - 0.565, val - 0.567.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 1 trained 10 epochs in 9.036s.  Loss: tr - 0.144, val - 0.113.  Accuracy: tr - 0.954, val - 0.966.\n",
      "lr 0.0009, beta_1 0.8822, beta_2 0.9997, l2 0.0179, filters 46, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 101, fold 1 trained 10 epochs in 5.613s.  Loss: tr - 0.095, val - 0.087.  Accuracy: tr - 0.988, val - 0.989.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 1 trained 10 epochs in 5.086s.  Loss: tr - 0.055, val - 0.049.  Accuracy: tr - 0.981, val - 0.984.\n",
      "lr 0.0001, beta_1 0.9078, beta_2 0.9988, l2 0.02, filters 64, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 103, fold 1 trained 10 epochs in 9.01s.  Loss: tr - 0.125, val - 0.102.  Accuracy: tr - 0.961, val - 0.971.\n",
      "lr 0.0001, beta_1 0.8996, beta_2 0.9999, l2 0.0172, filters 64, kernel_sz 5,  dense_sz 139, activs relu, padding 0\n",
      "Configuration 104, fold 1 trained 10 epochs in 5.141s.  Loss: tr - 0.288, val - 0.254.  Accuracy: tr - 0.902, val - 0.923.\n",
      "lr 0.0009, beta_1 0.8939, beta_2 0.9941, l2 0.02, filters 55, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 105, fold 1 trained 10 epochs in 8.327s.  Loss: tr - 0.049, val - 0.038.  Accuracy: tr - 0.983, val - 0.987.\n",
      "lr 0.0001, beta_1 0.9181, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 106, fold 1 trained 10 epochs in 9.023s.  Loss: tr - 0.081, val - 0.069.  Accuracy: tr - 0.973, val - 0.978.\n",
      "lr 0.0067, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 107, fold 1 trained 6 epochs in 4.05s.  Loss: tr - 35.811, val - 42.392.  Accuracy: tr - 0.562, val - 0.576.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 1 trained 10 epochs in 6.736s.  Loss: tr - 0.079, val - 0.055.  Accuracy: tr - 0.971, val - 0.979.\n",
      "lr 0.0036, beta_1 0.8581, beta_2 0.9999, l2 0.0161, filters 57, kernel_sz 2,  dense_sz 122, activs relu, padding 1\n",
      "Configuration 109, fold 1 trained 10 epochs in 7.507s.  Loss: tr - 0.573, val - 0.455.  Accuracy: tr - 0.738, val - 0.883.\n",
      "lr 0.0027, beta_1 0.9029, beta_2 0.9999, l2 0.02, filters 55, kernel_sz 2,  dense_sz 92, activs relu, padding 1\n",
      "Configuration 110, fold 1 trained 10 epochs in 7.231s.  Loss: tr - 0.586, val - 0.56.  Accuracy: tr - 0.669, val - 0.754.\n",
      "lr 0.0007, beta_1 0.8873, beta_2 0.9999, l2 0.02, filters 51, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 111, fold 1 trained 10 epochs in 8.418s.  Loss: tr - 0.101, val - 0.089.  Accuracy: tr - 0.986, val - 0.991.\n",
      "lr 0.0025, beta_1 0.8949, beta_2 0.9935, l2 0.02, filters 42, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 112, fold 1 trained 10 epochs in 4.312s.  Loss: tr - 0.045, val - 0.037.  Accuracy: tr - 0.985, val - 0.989.\n",
      "lr 0.0001, beta_1 0.8683, beta_2 0.9949, l2 0.02, filters 29, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 113, fold 1 trained 10 epochs in 5.476s.  Loss: tr - 0.113, val - 0.091.  Accuracy: tr - 0.969, val - 0.971.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 1 trained 10 epochs in 5.662s.  Loss: tr - 0.042, val - 0.032.  Accuracy: tr - 0.985, val - 0.989.\n",
      "lr 0.002, beta_1 0.8552, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 133, activs relu, padding 1\n",
      "Configuration 115, fold 1 trained 10 epochs in 9.005s.  Loss: tr - 0.031, val - 0.027.  Accuracy: tr - 0.991, val - 0.992.\n",
      "Significant difference in validation losses across group, after 2 folds (0.005785507197353977)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 2 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0075, beta_1 0.9001, beta_2 0.9999, l2 0.008, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 87, fold 2 trained 6 epochs in 2.97s.  Loss: tr - 35.849, val - 41.794.  Accuracy: tr - 0.573, val - 0.582.\n",
      "lr 0.0001, beta_1 0.858, beta_2 0.9968, l2 0.02, filters 50, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 88, fold 2 trained 10 epochs in 8.074s.  Loss: tr - 0.093, val - 0.077.  Accuracy: tr - 0.974, val - 0.981.\n",
      "lr 0.004, beta_1 0.8738, beta_2 0.9971, l2 0.02, filters 16, kernel_sz 4,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 89, fold 2 trained 10 epochs in 2.249s.  Loss: tr - 0.625, val - 0.547.  Accuracy: tr - 0.694, val - 0.777.\n",
      "lr 0.0001, beta_1 0.8591, beta_2 0.9989, l2 0.0111, filters 64, kernel_sz 4,  dense_sz 99, activs relu, padding 1\n",
      "Configuration 90, fold 2 trained 10 epochs in 9.01s.  Loss: tr - 0.113, val - 0.085.  Accuracy: tr - 0.967, val - 0.978.\n",
      "lr 0.0018, beta_1 0.85, beta_2 0.9992, l2 0.02, filters 38, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 91, fold 2 trained 10 epochs in 6.299s.  Loss: tr - 0.331, val - 0.238.  Accuracy: tr - 0.848, val - 0.901.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9985, l2 0.0189, filters 52, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 92, fold 2 trained 10 epochs in 6.199s.  Loss: tr - 0.179, val - 0.148.  Accuracy: tr - 0.951, val - 0.958.\n",
      "lr 0.0023, beta_1 0.85, beta_2 0.9991, l2 0.0095, filters 44, kernel_sz 5,  dense_sz 149, activs relu, padding 1\n",
      "Configuration 93, fold 2 trained 10 epochs in 4.832s.  Loss: tr - 0.48, val - 0.397.  Accuracy: tr - 0.807, val - 0.846.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 2 trained 10 epochs in 2.589s.  Loss: tr - 0.035, val - 0.039.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 2 trained 10 epochs in 5.553s.  Loss: tr - 0.027, val - 0.035.  Accuracy: tr - 0.991, val - 0.993.\n",
      "lr 0.0039, beta_1 0.9007, beta_2 0.9999, l2 0.0075, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 96, fold 2 trained 6 epochs in 5.404s.  Loss: tr - 35.874, val - 43.323.  Accuracy: tr - 0.548, val - 0.567.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 2 trained 10 epochs in 8.971s.  Loss: tr - 0.124, val - 0.086.  Accuracy: tr - 0.954, val - 0.974.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.0186, filters 48, kernel_sz 5,  dense_sz 127, activs relu, padding 1\n",
      "Configuration 98, fold 2 trained 10 epochs in 4.999s.  Loss: tr - 0.166, val - 0.129.  Accuracy: tr - 0.949, val - 0.964.\n",
      "lr 0.0055, beta_1 0.9174, beta_2 0.998, l2 0.0158, filters 51, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 99, fold 2 trained 6 epochs in 4.838s.  Loss: tr - 34.955, val - 41.794.  Accuracy: tr - 0.555, val - 0.582.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 2 trained 10 epochs in 9.028s.  Loss: tr - 0.053, val - 0.048.  Accuracy: tr - 0.985, val - 0.987.\n",
      "lr 0.0009, beta_1 0.8822, beta_2 0.9997, l2 0.0179, filters 46, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 101, fold 2 trained 10 epochs in 5.9s.  Loss: tr - 0.091, val - 0.088.  Accuracy: tr - 0.987, val - 0.99.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 2 trained 10 epochs in 5.094s.  Loss: tr - 0.053, val - 0.046.  Accuracy: tr - 0.981, val - 0.986.\n",
      "lr 0.0001, beta_1 0.9078, beta_2 0.9988, l2 0.02, filters 64, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 103, fold 2 trained 10 epochs in 9.001s.  Loss: tr - 0.104, val - 0.087.  Accuracy: tr - 0.965, val - 0.972.\n",
      "lr 0.0001, beta_1 0.8996, beta_2 0.9999, l2 0.0172, filters 64, kernel_sz 5,  dense_sz 139, activs relu, padding 0\n",
      "Configuration 104, fold 2 trained 10 epochs in 4.84s.  Loss: tr - 0.403, val - 0.381.  Accuracy: tr - 0.831, val - 0.852.\n",
      "lr 0.0009, beta_1 0.8939, beta_2 0.9941, l2 0.02, filters 55, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 105, fold 2 trained 10 epochs in 8.328s.  Loss: tr - 0.101, val - 0.072.  Accuracy: tr - 0.961, val - 0.976.\n",
      "lr 0.0001, beta_1 0.9181, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 106, fold 2 trained 10 epochs in 9.02s.  Loss: tr - 0.155, val - 0.118.  Accuracy: tr - 0.953, val - 0.964.\n",
      "lr 0.0067, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 107, fold 2 trained 6 epochs in 4.331s.  Loss: tr - 39.073, val - 47.197.  Accuracy: tr - 0.536, val - 0.528.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 2 trained 10 epochs in 6.733s.  Loss: tr - 0.275, val - 0.239.  Accuracy: tr - 0.89, val - 0.894.\n",
      "lr 0.0036, beta_1 0.8581, beta_2 0.9999, l2 0.0161, filters 57, kernel_sz 2,  dense_sz 122, activs relu, padding 1\n",
      "Configuration 109, fold 2 trained 6 epochs in 4.513s.  Loss: tr - 32.607, val - 48.709.  Accuracy: tr - 0.561, val - 0.513.\n",
      "lr 0.0027, beta_1 0.9029, beta_2 0.9999, l2 0.02, filters 55, kernel_sz 2,  dense_sz 92, activs relu, padding 1\n",
      "Configuration 110, fold 2 trained 10 epochs in 7.237s.  Loss: tr - 0.114, val - 0.09.  Accuracy: tr - 0.962, val - 0.965.\n",
      "lr 0.0007, beta_1 0.8873, beta_2 0.9999, l2 0.02, filters 51, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 111, fold 2 trained 10 epochs in 8.112s.  Loss: tr - 0.106, val - 0.101.  Accuracy: tr - 0.987, val - 0.986.\n",
      "lr 0.0025, beta_1 0.8949, beta_2 0.9935, l2 0.02, filters 42, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 112, fold 2 trained 10 epochs in 4.3s.  Loss: tr - 0.371, val - 0.237.  Accuracy: tr - 0.872, val - 0.9.\n",
      "lr 0.0001, beta_1 0.8683, beta_2 0.9949, l2 0.02, filters 29, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 113, fold 2 trained 10 epochs in 5.48s.  Loss: tr - 0.104, val - 0.087.  Accuracy: tr - 0.972, val - 0.979.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 2 trained 10 epochs in 5.941s.  Loss: tr - 0.03, val - 0.039.  Accuracy: tr - 0.99, val - 0.99.\n",
      "lr 0.002, beta_1 0.8552, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 133, activs relu, padding 1\n",
      "Configuration 115, fold 2 trained 10 epochs in 8.984s.  Loss: tr - 0.576, val - 0.43.  Accuracy: tr - 0.701, val - 0.837.\n",
      "Significant difference in validation losses across group, after 3 folds (3.998117481301989e-05)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 3 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0075, beta_1 0.9001, beta_2 0.9999, l2 0.008, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 87, fold 3 trained 6 epochs in 2.892s.  Loss: tr - 39.56, val - 46.56.  Accuracy: tr - 0.518, val - 0.534.\n",
      "lr 0.0001, beta_1 0.858, beta_2 0.9968, l2 0.02, filters 50, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 88, fold 3 trained 10 epochs in 7.79s.  Loss: tr - 0.138, val - 0.121.  Accuracy: tr - 0.962, val - 0.965.\n",
      "lr 0.004, beta_1 0.8738, beta_2 0.9971, l2 0.02, filters 16, kernel_sz 4,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 89, fold 3 trained 10 epochs in 2.258s.  Loss: tr - 0.096, val - 0.077.  Accuracy: tr - 0.969, val - 0.972.\n",
      "lr 0.0001, beta_1 0.8591, beta_2 0.9989, l2 0.0111, filters 64, kernel_sz 4,  dense_sz 99, activs relu, padding 1\n",
      "Configuration 90, fold 3 trained 10 epochs in 9.013s.  Loss: tr - 0.294, val - 0.294.  Accuracy: tr - 0.904, val - 0.874.\n",
      "lr 0.0018, beta_1 0.85, beta_2 0.9992, l2 0.02, filters 38, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 91, fold 3 trained 10 epochs in 6.579s.  Loss: tr - 0.047, val - 0.045.  Accuracy: tr - 0.985, val - 0.984.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9985, l2 0.0189, filters 52, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 92, fold 3 trained 10 epochs in 6.192s.  Loss: tr - 0.332, val - 0.305.  Accuracy: tr - 0.902, val - 0.908.\n",
      "lr 0.0023, beta_1 0.85, beta_2 0.9991, l2 0.0095, filters 44, kernel_sz 5,  dense_sz 149, activs relu, padding 1\n",
      "Configuration 93, fold 3 trained 10 epochs in 4.806s.  Loss: tr - 0.542, val - 0.456.  Accuracy: tr - 0.76, val - 0.812.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 3 trained 10 epochs in 2.249s.  Loss: tr - 0.036, val - 0.038.  Accuracy: tr - 0.989, val - 0.986.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 3 trained 10 epochs in 5.545s.  Loss: tr - 0.035, val - 0.036.  Accuracy: tr - 0.989, val - 0.988.\n",
      "lr 0.0039, beta_1 0.9007, beta_2 0.9999, l2 0.0075, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 96, fold 3 trained 6 epochs in 5.409s.  Loss: tr - 33.923, val - 40.138.  Accuracy: tr - 0.586, val - 0.599.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 3 trained 10 epochs in 9.274s.  Loss: tr - 0.052, val - 0.046.  Accuracy: tr - 0.982, val - 0.985.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.0186, filters 48, kernel_sz 5,  dense_sz 127, activs relu, padding 1\n",
      "Configuration 98, fold 3 trained 10 epochs in 5.003s.  Loss: tr - 0.101, val - 0.101.  Accuracy: tr - 0.967, val - 0.96.\n",
      "lr 0.0055, beta_1 0.9174, beta_2 0.998, l2 0.0158, filters 51, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 99, fold 3 trained 6 epochs in 4.839s.  Loss: tr - 34.056, val - 39.271.  Accuracy: tr - 0.587, val - 0.607.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 3 trained 10 epochs in 9.028s.  Loss: tr - 0.093, val - 0.09.  Accuracy: tr - 0.974, val - 0.971.\n",
      "lr 0.0009, beta_1 0.8822, beta_2 0.9997, l2 0.0179, filters 46, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 101, fold 3 trained 10 epochs in 5.623s.  Loss: tr - 0.094, val - 0.089.  Accuracy: tr - 0.987, val - 0.989.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 3 trained 10 epochs in 5.094s.  Loss: tr - 0.031, val - 0.029.  Accuracy: tr - 0.991, val - 0.989.\n",
      "lr 0.0001, beta_1 0.9078, beta_2 0.9988, l2 0.02, filters 64, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 103, fold 3 trained 10 epochs in 9.003s.  Loss: tr - 0.168, val - 0.15.  Accuracy: tr - 0.948, val - 0.945.\n",
      "lr 0.0001, beta_1 0.8996, beta_2 0.9999, l2 0.0172, filters 64, kernel_sz 5,  dense_sz 139, activs relu, padding 0\n",
      "Configuration 104, fold 3 trained 10 epochs in 5.144s.  Loss: tr - 0.482, val - 0.467.  Accuracy: tr - 0.795, val - 0.812.\n",
      "lr 0.0009, beta_1 0.8939, beta_2 0.9941, l2 0.02, filters 55, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 105, fold 3 trained 10 epochs in 8.321s.  Loss: tr - 0.041, val - 0.039.  Accuracy: tr - 0.986, val - 0.988.\n",
      "lr 0.0001, beta_1 0.9181, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 106, fold 3 trained 10 epochs in 9.027s.  Loss: tr - 0.094, val - 0.086.  Accuracy: tr - 0.97, val - 0.973.\n",
      "lr 0.0067, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 107, fold 3 trained 6 epochs in 4.047s.  Loss: tr - 39.2, val - 47.426.  Accuracy: tr - 0.523, val - 0.526.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 3 trained 10 epochs in 6.738s.  Loss: tr - 0.095, val - 0.077.  Accuracy: tr - 0.965, val - 0.971.\n",
      "lr 0.0036, beta_1 0.8581, beta_2 0.9999, l2 0.0161, filters 57, kernel_sz 2,  dense_sz 122, activs relu, padding 1\n",
      "Configuration 109, fold 3 trained 10 epochs in 7.509s.  Loss: tr - 0.633, val - 0.621.  Accuracy: tr - 0.629, val - 0.644.\n",
      "lr 0.0027, beta_1 0.9029, beta_2 0.9999, l2 0.02, filters 55, kernel_sz 2,  dense_sz 92, activs relu, padding 1\n",
      "Configuration 110, fold 3 trained 10 epochs in 7.55s.  Loss: tr - 0.15, val - 0.121.  Accuracy: tr - 0.954, val - 0.951.\n",
      "lr 0.0007, beta_1 0.8873, beta_2 0.9999, l2 0.02, filters 51, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 111, fold 3 trained 10 epochs in 8.111s.  Loss: tr - 0.123, val - 0.111.  Accuracy: tr - 0.98, val - 0.978.\n",
      "lr 0.0025, beta_1 0.8949, beta_2 0.9935, l2 0.02, filters 42, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 112, fold 3 trained 10 epochs in 4.305s.  Loss: tr - 0.196, val - 0.154.  Accuracy: tr - 0.93, val - 0.938.\n",
      "lr 0.0001, beta_1 0.8683, beta_2 0.9949, l2 0.02, filters 29, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 113, fold 3 trained 10 epochs in 5.464s.  Loss: tr - 0.082, val - 0.078.  Accuracy: tr - 0.978, val - 0.978.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 3 trained 10 epochs in 5.668s.  Loss: tr - 0.034, val - 0.035.  Accuracy: tr - 0.989, val - 0.988.\n",
      "lr 0.002, beta_1 0.8552, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 133, activs relu, padding 1\n",
      "Configuration 115, fold 3 trained 6 epochs in 5.395s.  Loss: tr - 34.727, val - 44.317.  Accuracy: tr - 0.552, val - 0.557.\n",
      "Significant difference in validation losses across group, after 4 folds (2.2696741151155483e-08)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 4 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0075, beta_1 0.9001, beta_2 0.9999, l2 0.008, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 87, fold 4 trained 6 epochs in 2.894s.  Loss: tr - 34.964, val - 41.412.  Accuracy: tr - 0.578, val - 0.586.\n",
      "lr 0.0001, beta_1 0.858, beta_2 0.9968, l2 0.02, filters 50, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 88, fold 4 trained 10 epochs in 8.071s.  Loss: tr - 0.072, val - 0.057.  Accuracy: tr - 0.981, val - 0.984.\n",
      "lr 0.004, beta_1 0.8738, beta_2 0.9971, l2 0.02, filters 16, kernel_sz 4,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 89, fold 4 trained 10 epochs in 2.236s.  Loss: tr - 0.05, val - 0.043.  Accuracy: tr - 0.984, val - 0.983.\n",
      "lr 0.0001, beta_1 0.8591, beta_2 0.9989, l2 0.0111, filters 64, kernel_sz 4,  dense_sz 99, activs relu, padding 1\n",
      "Configuration 90, fold 4 trained 10 epochs in 9.006s.  Loss: tr - 0.063, val - 0.053.  Accuracy: tr - 0.982, val - 0.984.\n",
      "lr 0.0018, beta_1 0.85, beta_2 0.9992, l2 0.02, filters 38, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 91, fold 4 trained 10 epochs in 6.308s.  Loss: tr - 0.265, val - 0.184.  Accuracy: tr - 0.901, val - 0.932.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9985, l2 0.0189, filters 52, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 92, fold 4 trained 10 epochs in 6.213s.  Loss: tr - 0.277, val - 0.236.  Accuracy: tr - 0.93, val - 0.936.\n",
      "lr 0.0023, beta_1 0.85, beta_2 0.9991, l2 0.0095, filters 44, kernel_sz 5,  dense_sz 149, activs relu, padding 1\n",
      "Configuration 93, fold 4 trained 10 epochs in 4.819s.  Loss: tr - 0.353, val - 0.227.  Accuracy: tr - 0.844, val - 0.91.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 4 trained 10 epochs in 2.539s.  Loss: tr - 0.032, val - 0.032.  Accuracy: tr - 0.99, val - 0.989.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 4 trained 10 epochs in 5.554s.  Loss: tr - 0.041, val - 0.03.  Accuracy: tr - 0.985, val - 0.99.\n",
      "lr 0.0039, beta_1 0.9007, beta_2 0.9999, l2 0.0075, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 96, fold 4 trained 6 epochs in 5.409s.  Loss: tr - 35.505, val - 42.89.  Accuracy: tr - 0.551, val - 0.571.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 4 trained 10 epochs in 8.986s.  Loss: tr - 0.042, val - 0.031.  Accuracy: tr - 0.987, val - 0.992.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.0186, filters 48, kernel_sz 5,  dense_sz 127, activs relu, padding 1\n",
      "Configuration 98, fold 4 trained 10 epochs in 5.01s.  Loss: tr - 0.056, val - 0.051.  Accuracy: tr - 0.984, val - 0.985.\n",
      "lr 0.0055, beta_1 0.9174, beta_2 0.998, l2 0.0158, filters 51, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 99, fold 4 trained 6 epochs in 4.84s.  Loss: tr - 40.178, val - 47.732.  Accuracy: tr - 0.519, val - 0.523.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 4 trained 10 epochs in 9.024s.  Loss: tr - 0.053, val - 0.045.  Accuracy: tr - 0.984, val - 0.988.\n",
      "lr 0.0009, beta_1 0.8822, beta_2 0.9997, l2 0.0179, filters 46, kernel_sz 3,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 101, fold 4 trained 10 epochs in 5.915s.  Loss: tr - 0.082, val - 0.078.  Accuracy: tr - 0.99, val - 0.99.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 4 trained 10 epochs in 5.094s.  Loss: tr - 0.07, val - 0.056.  Accuracy: tr - 0.974, val - 0.981.\n",
      "lr 0.0001, beta_1 0.9078, beta_2 0.9988, l2 0.02, filters 64, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 103, fold 4 trained 10 epochs in 8.992s.  Loss: tr - 0.092, val - 0.072.  Accuracy: tr - 0.969, val - 0.976.\n",
      "lr 0.0001, beta_1 0.8996, beta_2 0.9999, l2 0.0172, filters 64, kernel_sz 5,  dense_sz 139, activs relu, padding 0\n",
      "Configuration 104, fold 4 trained 10 epochs in 4.85s.  Loss: tr - 0.394, val - 0.361.  Accuracy: tr - 0.848, val - 0.854.\n",
      "lr 0.0009, beta_1 0.8939, beta_2 0.9941, l2 0.02, filters 55, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 105, fold 4 trained 10 epochs in 8.308s.  Loss: tr - 0.101, val - 0.08.  Accuracy: tr - 0.961, val - 0.968.\n",
      "lr 0.0001, beta_1 0.9181, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 106, fold 4 trained 10 epochs in 9.028s.  Loss: tr - 0.097, val - 0.078.  Accuracy: tr - 0.97, val - 0.977.\n",
      "lr 0.0067, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 107, fold 4 trained 6 epochs in 4.334s.  Loss: tr - 39.712, val - 49.975.  Accuracy: tr - 0.517, val - 0.5.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 4 trained 10 epochs in 6.727s.  Loss: tr - 0.044, val - 0.034.  Accuracy: tr - 0.986, val - 0.991.\n",
      "lr 0.0036, beta_1 0.8581, beta_2 0.9999, l2 0.0161, filters 57, kernel_sz 2,  dense_sz 122, activs relu, padding 1\n",
      "Configuration 109, fold 4 trained 10 epochs in 7.505s.  Loss: tr - 0.655, val - 0.652.  Accuracy: tr - 0.598, val - 0.646.\n",
      "lr 0.0027, beta_1 0.9029, beta_2 0.9999, l2 0.02, filters 55, kernel_sz 2,  dense_sz 92, activs relu, padding 1\n",
      "Configuration 110, fold 4 trained 10 epochs in 7.247s.  Loss: tr - 0.111, val - 0.1.  Accuracy: tr - 0.962, val - 0.961.\n",
      "lr 0.0007, beta_1 0.8873, beta_2 0.9999, l2 0.02, filters 51, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 111, fold 4 trained 10 epochs in 8.111s.  Loss: tr - 0.095, val - 0.088.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0025, beta_1 0.8949, beta_2 0.9935, l2 0.02, filters 42, kernel_sz 3,  dense_sz 150, activs relu, padding 0\n",
      "Configuration 112, fold 4 trained 10 epochs in 4.305s.  Loss: tr - 0.054, val - 0.045.  Accuracy: tr - 0.984, val - 0.988.\n",
      "lr 0.0001, beta_1 0.8683, beta_2 0.9949, l2 0.02, filters 29, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 113, fold 4 trained 10 epochs in 5.765s.  Loss: tr - 0.192, val - 0.153.  Accuracy: tr - 0.959, val - 0.967.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 4 trained 10 epochs in 5.652s.  Loss: tr - 0.054, val - 0.047.  Accuracy: tr - 0.982, val - 0.985.\n",
      "lr 0.002, beta_1 0.8552, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 133, activs relu, padding 1\n",
      "Configuration 115, fold 4 trained 10 epochs in 8.983s.  Loss: tr - 0.643, val - 0.624.  Accuracy: tr - 0.638, val - 0.603.\n",
      "Significant difference in validation losses across group, after 5 folds (9.203036377611862e-12)\n",
      "Configuration 105 (mean vloss 0.0528201699256897) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 88 (mean vloss 0.08150166422128677) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 101 (mean vloss 0.08452248424291611) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 103 (mean vloss 0.09280752390623093) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 111 (mean vloss 0.097679103910923) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 112 (mean vloss 0.10211258232593537) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 106 (mean vloss 0.1080396220088005) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 98 (mean vloss 0.1087218962609768) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 113 (mean vloss 0.1183554470539093) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 91 (mean vloss 0.12485718131065368) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 90 (mean vloss 0.12849924266338347) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 89 (mean vloss 0.16279453188180923) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 92 (mean vloss 0.20712625086307526) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 110 (mean vloss 0.25852580964565275) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 104 (mean vloss 0.3449796259403229) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 93 (mean vloss 0.47057836353778837) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 115 (mean vloss 9.127496201172471) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 109 (mean vloss 18.621783220767973) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 99 (mean vloss 42.68563613891602) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 96 (mean vloss 44.80193862915039) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 87 (mean vloss 45.637586975097655) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 107 (mean vloss 47.484354400634764) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 5 on list of 8 configurations.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 5 trained 10 epochs in 8.063s.  Loss: tr - 0.035, val - 0.038.  Accuracy: tr - 0.989, val - 0.991.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 5 trained 10 epochs in 2.238s.  Loss: tr - 0.031, val - 0.035.  Accuracy: tr - 0.991, val - 0.99.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 5 trained 10 epochs in 5.539s.  Loss: tr - 0.592, val - 0.529.  Accuracy: tr - 0.689, val - 0.818.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 5 trained 10 epochs in 9.267s.  Loss: tr - 0.029, val - 0.035.  Accuracy: tr - 0.99, val - 0.99.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 5 trained 10 epochs in 9.02s.  Loss: tr - 0.141, val - 0.118.  Accuracy: tr - 0.957, val - 0.963.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 5 trained 10 epochs in 5.085s.  Loss: tr - 0.05, val - 0.048.  Accuracy: tr - 0.982, val - 0.984.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 5 trained 10 epochs in 6.728s.  Loss: tr - 0.041, val - 0.045.  Accuracy: tr - 0.986, val - 0.988.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 5 trained 10 epochs in 5.662s.  Loss: tr - 0.06, val - 0.051.  Accuracy: tr - 0.979, val - 0.983.\n",
      "Not enough evidence to support a difference across group, after 6 folds (0.08685819274566248)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 6 on list of 8 configurations.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 6 trained 10 epochs in 8.121s.  Loss: tr - 0.036, val - 0.035.  Accuracy: tr - 0.989, val - 0.99.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 6 trained 10 epochs in 2.527s.  Loss: tr - 0.042, val - 0.045.  Accuracy: tr - 0.987, val - 0.986.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 6 trained 10 epochs in 5.542s.  Loss: tr - 0.049, val - 0.05.  Accuracy: tr - 0.983, val - 0.984.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 6 trained 10 epochs in 8.971s.  Loss: tr - 0.064, val - 0.059.  Accuracy: tr - 0.976, val - 0.977.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 6 trained 10 epochs in 9.04s.  Loss: tr - 0.058, val - 0.054.  Accuracy: tr - 0.984, val - 0.985.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 6 trained 10 epochs in 5.091s.  Loss: tr - 0.033, val - 0.034.  Accuracy: tr - 0.99, val - 0.99.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 6 trained 10 epochs in 6.728s.  Loss: tr - 0.039, val - 0.035.  Accuracy: tr - 0.988, val - 0.988.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 6 trained 10 epochs in 5.642s.  Loss: tr - 0.054, val - 0.054.  Accuracy: tr - 0.982, val - 0.983.\n",
      "Not enough evidence to support a difference across group, after 7 folds (0.10866566871135838)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 7 on list of 8 configurations.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 7 trained 10 epochs in 8.356s.  Loss: tr - 0.057, val - 0.051.  Accuracy: tr - 0.981, val - 0.982.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 7 trained 10 epochs in 2.252s.  Loss: tr - 0.029, val - 0.03.  Accuracy: tr - 0.991, val - 0.991.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 7 trained 10 epochs in 5.56s.  Loss: tr - 0.039, val - 0.037.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 7 trained 10 epochs in 8.975s.  Loss: tr - 0.038, val - 0.039.  Accuracy: tr - 0.987, val - 0.988.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 7 trained 10 epochs in 9.026s.  Loss: tr - 0.046, val - 0.045.  Accuracy: tr - 0.988, val - 0.986.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 7 trained 10 epochs in 5.113s.  Loss: tr - 0.04, val - 0.041.  Accuracy: tr - 0.987, val - 0.984.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 7 trained 10 epochs in 7.034s.  Loss: tr - 0.065, val - 0.05.  Accuracy: tr - 0.978, val - 0.983.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 7 trained 10 epochs in 5.644s.  Loss: tr - 0.032, val - 0.029.  Accuracy: tr - 0.99, val - 0.99.\n",
      "Not enough evidence to support a difference across group, after 8 folds (0.10908582571512082)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 8 on list of 8 configurations.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 8 trained 10 epochs in 8.096s.  Loss: tr - 0.029, val - 0.027.  Accuracy: tr - 0.991, val - 0.99.\n",
      "lr 0.0011, beta_1 0.8629, beta_2 0.9996, l2 0.0173, filters 16, kernel_sz 4,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 94, fold 8 trained 10 epochs in 2.247s.  Loss: tr - 0.028, val - 0.027.  Accuracy: tr - 0.992, val - 0.99.\n",
      "lr 0.0026, beta_1 0.8983, beta_2 0.9979, l2 0.0061, filters 48, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 95, fold 8 trained 10 epochs in 5.553s.  Loss: tr - 0.152, val - 0.072.  Accuracy: tr - 0.944, val - 0.975.\n",
      "lr 0.0013, beta_1 0.85, beta_2 0.9999, l2 0.0138, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 97, fold 8 trained 10 epochs in 8.98s.  Loss: tr - 0.173, val - 0.112.  Accuracy: tr - 0.939, val - 0.967.\n",
      "lr 0.0001, beta_1 0.85, beta_2 0.9999, l2 0.02, filters 64, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 100, fold 8 trained 10 epochs in 9.302s.  Loss: tr - 0.076, val - 0.057.  Accuracy: tr - 0.98, val - 0.985.\n",
      "lr 0.0005, beta_1 0.8948, beta_2 0.9999, l2 0.02, filters 47, kernel_sz 5,  dense_sz 139, activs relu, padding 1\n",
      "Configuration 102, fold 8 trained 10 epochs in 5.106s.  Loss: tr - 0.055, val - 0.047.  Accuracy: tr - 0.981, val - 0.983.\n",
      "lr 0.0015, beta_1 0.8966, beta_2 0.9991, l2 0.02, filters 64, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 108, fold 8 trained 10 epochs in 6.744s.  Loss: tr - 0.389, val - 0.437.  Accuracy: tr - 0.825, val - 0.774.\n",
      "lr 0.0012, beta_1 0.9198, beta_2 0.9994, l2 0.02, filters 47, kernel_sz 3,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 114, fold 8 trained 10 epochs in 5.647s.  Loss: tr - 0.053, val - 0.038.  Accuracy: tr - 0.98, val - 0.987.\n",
      "Significant difference in validation losses across group, after 9 folds (0.034601207777420644)\n",
      "Configuration 97 (mean vloss 0.055875185049242444) dropped with pvalue 0.02734375 against configuration 78 (mean vloss 0.03493209472960896)\n",
      "Configuration 100 (mean vloss 0.06988539050022761) dropped with pvalue 0.013671875 against configuration 78 (mean vloss 0.03493209472960896)\n",
      "Configuration 108 (mean vloss 0.11505540501740244) dropped with pvalue 0.02734375 against configuration 78 (mean vloss 0.03493209472960896)\n",
      "Paired tests completed\n",
      "Iteration completed in 1105.849s. Remaining configurations 5.  Mean validation loss 0.053 and acc 0.982\n",
      "---------------------------------------------------------------------------\n",
      "####################################################################################################\n",
      "Iteration 5 of Iterated F-race.\n",
      "Training fold 0 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 102, evaluated once before -- skipping.\n",
      "lr 0.0053, beta_1 0.8832, beta_2 0.9988, l2 0.0182, filters 34, kernel_sz 5,  dense_sz 124, activs relu, padding 1\n",
      "Configuration 116, fold 0 trained 10 epochs in 3.969s.  Loss: tr - 0.663, val - 0.655.  Accuracy: tr - 0.574, val - 0.595.\n",
      "lr 0.0021, beta_1 0.8841, beta_2 0.9999, l2 0.02, filters 49, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 117, fold 0 trained 10 epochs in 5.444s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.591, val - 0.596.\n",
      "lr 0.0033, beta_1 0.8752, beta_2 0.9984, l2 0.0163, filters 57, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 118, fold 0 trained 6 epochs in 3.94s.  Loss: tr - 42.329, val - 50.407.  Accuracy: tr - 0.495, val - 0.496.\n",
      "lr 0.0005, beta_1 0.95, beta_2 0.9992, l2 0.0151, filters 55, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 119, fold 0 trained 10 epochs in 5.846s.  Loss: tr - 0.109, val - 0.084.  Accuracy: tr - 0.959, val - 0.967.\n",
      "lr 0.0001, beta_1 0.907, beta_2 0.9999, l2 0.02, filters 38, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 120, fold 0 trained 10 epochs in 4.43s.  Loss: tr - 0.321, val - 0.289.  Accuracy: tr - 0.89, val - 0.896.\n",
      "lr 0.0001, beta_1 0.8999, beta_2 0.9999, l2 0.02, filters 56, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 121, fold 0 trained 10 epochs in 5.798s.  Loss: tr - 0.118, val - 0.089.  Accuracy: tr - 0.963, val - 0.971.\n",
      "lr 0.0049, beta_1 0.934, beta_2 0.9997, l2 0.02, filters 44, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 122, fold 0 trained 10 epochs in 6.929s.  Loss: tr - 0.788, val - 0.636.  Accuracy: tr - 0.582, val - 0.65.\n",
      "lr 0.0003, beta_1 0.9139, beta_2 0.9999, l2 0.02, filters 58, kernel_sz 5,  dense_sz 126, activs relu, padding 1\n",
      "Configuration 123, fold 0 trained 10 epochs in 6.187s.  Loss: tr - 0.066, val - 0.053.  Accuracy: tr - 0.976, val - 0.983.\n",
      "lr 0.0013, beta_1 0.8947, beta_2 0.9999, l2 0.0123, filters 33, kernel_sz 5,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 124, fold 0 trained 10 epochs in 5.975s.  Loss: tr - 0.035, val - 0.026.  Accuracy: tr - 0.989, val - 0.991.\n",
      "lr 0.0007, beta_1 0.8903, beta_2 0.9986, l2 0.02, filters 40, kernel_sz 5,  dense_sz 120, activs relu, padding 1\n",
      "Configuration 125, fold 0 trained 10 epochs in 4.43s.  Loss: tr - 0.041, val - 0.032.  Accuracy: tr - 0.987, val - 0.99.\n",
      "lr 0.0053, beta_1 0.85, beta_2 0.998, l2 0.0149, filters 53, kernel_sz 5,  dense_sz 94, activs relu, padding 1\n",
      "Configuration 126, fold 0 trained 6 epochs in 3.434s.  Loss: tr - 36.5, val - 43.359.  Accuracy: tr - 0.545, val - 0.566.\n",
      "lr 0.0001, beta_1 0.8811, beta_2 0.9999, l2 0.02, filters 37, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 127, fold 0 trained 10 epochs in 4.33s.  Loss: tr - 0.134, val - 0.117.  Accuracy: tr - 0.953, val - 0.96.\n",
      "lr 0.0003, beta_1 0.9275, beta_2 0.9985, l2 0.02, filters 43, kernel_sz 5,  dense_sz 132, activs relu, padding 1\n",
      "Configuration 128, fold 0 trained 10 epochs in 4.795s.  Loss: tr - 0.097, val - 0.082.  Accuracy: tr - 0.964, val - 0.972.\n",
      "lr 0.0001, beta_1 0.8762, beta_2 0.9982, l2 0.0193, filters 59, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 129, fold 0 trained 10 epochs in 6.227s.  Loss: tr - 0.101, val - 0.085.  Accuracy: tr - 0.966, val - 0.973.\n",
      "lr 0.003, beta_1 0.9164, beta_2 0.9985, l2 0.02, filters 42, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 130, fold 0 trained 10 epochs in 7.059s.  Loss: tr - 0.631, val - 0.594.  Accuracy: tr - 0.662, val - 0.705.\n",
      "lr 0.0001, beta_1 0.8964, beta_2 0.9984, l2 0.02, filters 57, kernel_sz 5,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 131, fold 0 trained 10 epochs in 6.105s.  Loss: tr - 0.097, val - 0.087.  Accuracy: tr - 0.966, val - 0.97.\n",
      "lr 0.0086, beta_1 0.9143, beta_2 0.9987, l2 0.0176, filters 45, kernel_sz 5,  dense_sz 129, activs relu, padding 0\n",
      "Configuration 132, fold 0 trained 6 epochs in 2.244s.  Loss: tr - 42.523, val - 50.865.  Accuracy: tr - 0.493, val - 0.491.\n",
      "lr 0.0044, beta_1 0.8904, beta_2 0.9999, l2 0.0156, filters 43, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 133, fold 0 trained 6 epochs in 2.873s.  Loss: tr - 36.111, val - 47.15.  Accuracy: tr - 0.532, val - 0.528.\n",
      "lr 0.0004, beta_1 0.8767, beta_2 0.9999, l2 0.0096, filters 50, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 134, fold 0 trained 10 epochs in 5.551s.  Loss: tr - 0.154, val - 0.125.  Accuracy: tr - 0.946, val - 0.955.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 0 trained 10 epochs in 4.811s.  Loss: tr - 0.034, val - 0.025.  Accuracy: tr - 0.989, val - 0.992.\n",
      "lr 0.0001, beta_1 0.8959, beta_2 0.9999, l2 0.0178, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 136, fold 0 trained 10 epochs in 4.795s.  Loss: tr - 0.219, val - 0.186.  Accuracy: tr - 0.932, val - 0.938.\n",
      "lr 0.0001, beta_1 0.8709, beta_2 0.9996, l2 0.02, filters 29, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 137, fold 0 trained 10 epochs in 5.07s.  Loss: tr - 0.116, val - 0.097.  Accuracy: tr - 0.961, val - 0.968.\n",
      "lr 0.0001, beta_1 0.8972, beta_2 0.9982, l2 0.0119, filters 32, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 138, fold 0 trained 10 epochs in 3.846s.  Loss: tr - 0.177, val - 0.14.  Accuracy: tr - 0.949, val - 0.957.\n",
      "lr 0.0001, beta_1 0.8657, beta_2 0.9999, l2 0.02, filters 54, kernel_sz 4,  dense_sz 131, activs relu, padding 1\n",
      "Configuration 139, fold 0 trained 10 epochs in 8.11s.  Loss: tr - 0.13, val - 0.112.  Accuracy: tr - 0.962, val - 0.967.\n",
      "lr 0.0001, beta_1 0.8766, beta_2 0.999, l2 0.0169, filters 50, kernel_sz 5,  dense_sz 138, activs relu, padding 0\n",
      "Configuration 140, fold 0 trained 10 epochs in 4.119s.  Loss: tr - 0.437, val - 0.412.  Accuracy: tr - 0.827, val - 0.828.\n",
      "lr 0.0033, beta_1 0.8937, beta_2 0.9999, l2 0.02, filters 57, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 141, fold 0 trained 10 epochs in 8.596s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.584, val - 0.595.\n",
      "lr 0.0001, beta_1 0.8919, beta_2 0.9985, l2 0.0169, filters 43, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 142, fold 0 trained 10 epochs in 4.8s.  Loss: tr - 0.065, val - 0.052.  Accuracy: tr - 0.98, val - 0.986.\n",
      "lr 0.0034, beta_1 0.8826, beta_2 0.9981, l2 0.0198, filters 46, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 143, fold 0 trained 10 epochs in 5.393s.  Loss: tr - 0.675, val - 0.658.  Accuracy: tr - 0.582, val - 0.596.\n",
      "Not enough evidence to support a difference across group, after 1 folds (0.46506624123787865)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 1 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 102, evaluated once before -- skipping.\n",
      "lr 0.0053, beta_1 0.8832, beta_2 0.9988, l2 0.0182, filters 34, kernel_sz 5,  dense_sz 124, activs relu, padding 1\n",
      "Configuration 116, fold 1 trained 10 epochs in 4.017s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.586, val - 0.586.\n",
      "lr 0.0021, beta_1 0.8841, beta_2 0.9999, l2 0.02, filters 49, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 117, fold 1 trained 10 epochs in 5.447s.  Loss: tr - 0.659, val - 0.655.  Accuracy: tr - 0.588, val - 0.592.\n",
      "lr 0.0033, beta_1 0.8752, beta_2 0.9984, l2 0.0163, filters 57, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 118, fold 1 trained 6 epochs in 3.64s.  Loss: tr - 34.305, val - 43.181.  Accuracy: tr - 0.562, val - 0.568.\n",
      "lr 0.0005, beta_1 0.95, beta_2 0.9992, l2 0.0151, filters 55, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 119, fold 1 trained 10 epochs in 5.833s.  Loss: tr - 0.047, val - 0.04.  Accuracy: tr - 0.984, val - 0.986.\n",
      "lr 0.0001, beta_1 0.907, beta_2 0.9999, l2 0.02, filters 38, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 120, fold 1 trained 10 epochs in 4.422s.  Loss: tr - 0.184, val - 0.159.  Accuracy: tr - 0.938, val - 0.948.\n",
      "lr 0.0001, beta_1 0.8999, beta_2 0.9999, l2 0.02, filters 56, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 121, fold 1 trained 10 epochs in 6.096s.  Loss: tr - 0.075, val - 0.066.  Accuracy: tr - 0.976, val - 0.977.\n",
      "lr 0.0049, beta_1 0.934, beta_2 0.9997, l2 0.02, filters 44, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 122, fold 1 trained 10 epochs in 6.95s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.589, val - 0.592.\n",
      "lr 0.0003, beta_1 0.9139, beta_2 0.9999, l2 0.02, filters 58, kernel_sz 5,  dense_sz 126, activs relu, padding 1\n",
      "Configuration 123, fold 1 trained 10 epochs in 6.198s.  Loss: tr - 0.065, val - 0.055.  Accuracy: tr - 0.976, val - 0.98.\n",
      "lr 0.0013, beta_1 0.8947, beta_2 0.9999, l2 0.0123, filters 33, kernel_sz 5,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 124, fold 1 trained 10 epochs in 5.672s.  Loss: tr - 0.035, val - 0.023.  Accuracy: tr - 0.99, val - 0.994.\n",
      "lr 0.0007, beta_1 0.8903, beta_2 0.9986, l2 0.02, filters 40, kernel_sz 5,  dense_sz 120, activs relu, padding 1\n",
      "Configuration 125, fold 1 trained 10 epochs in 4.426s.  Loss: tr - 0.138, val - 0.11.  Accuracy: tr - 0.947, val - 0.961.\n",
      "lr 0.0053, beta_1 0.85, beta_2 0.998, l2 0.0149, filters 53, kernel_sz 5,  dense_sz 94, activs relu, padding 1\n",
      "Configuration 126, fold 1 trained 6 epochs in 3.419s.  Loss: tr - 34.868, val - 41.425.  Accuracy: tr - 0.553, val - 0.586.\n",
      "lr 0.0001, beta_1 0.8811, beta_2 0.9999, l2 0.02, filters 37, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 127, fold 1 trained 10 epochs in 4.344s.  Loss: tr - 0.138, val - 0.114.  Accuracy: tr - 0.959, val - 0.967.\n",
      "lr 0.0003, beta_1 0.9275, beta_2 0.9985, l2 0.02, filters 43, kernel_sz 5,  dense_sz 132, activs relu, padding 1\n",
      "Configuration 128, fold 1 trained 10 epochs in 5.095s.  Loss: tr - 0.045, val - 0.037.  Accuracy: tr - 0.984, val - 0.988.\n",
      "lr 0.0001, beta_1 0.8762, beta_2 0.9982, l2 0.0193, filters 59, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 129, fold 1 trained 10 epochs in 6.236s.  Loss: tr - 0.058, val - 0.048.  Accuracy: tr - 0.983, val - 0.987.\n",
      "lr 0.003, beta_1 0.9164, beta_2 0.9985, l2 0.02, filters 42, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 130, fold 1 trained 6 epochs in 4.054s.  Loss: tr - 34.027, val - 40.789.  Accuracy: tr - 0.564, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8964, beta_2 0.9984, l2 0.02, filters 57, kernel_sz 5,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 131, fold 1 trained 10 epochs in 6.081s.  Loss: tr - 0.105, val - 0.09.  Accuracy: tr - 0.964, val - 0.969.\n",
      "lr 0.0086, beta_1 0.9143, beta_2 0.9987, l2 0.0176, filters 45, kernel_sz 5,  dense_sz 129, activs relu, padding 0\n",
      "Configuration 132, fold 1 trained 6 epochs in 2.229s.  Loss: tr - 35.375, val - 41.832.  Accuracy: tr - 0.551, val - 0.582.\n",
      "lr 0.0044, beta_1 0.8904, beta_2 0.9999, l2 0.0156, filters 43, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 133, fold 1 trained 10 epochs in 4.773s.  Loss: tr - 0.66, val - 0.656.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.0004, beta_1 0.8767, beta_2 0.9999, l2 0.0096, filters 50, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 134, fold 1 trained 10 epochs in 5.564s.  Loss: tr - 0.053, val - 0.047.  Accuracy: tr - 0.982, val - 0.983.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 1 trained 10 epochs in 5.122s.  Loss: tr - 0.061, val - 0.051.  Accuracy: tr - 0.979, val - 0.982.\n",
      "lr 0.0001, beta_1 0.8959, beta_2 0.9999, l2 0.0178, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 136, fold 1 trained 10 epochs in 4.796s.  Loss: tr - 0.113, val - 0.097.  Accuracy: tr - 0.963, val - 0.966.\n",
      "lr 0.0001, beta_1 0.8709, beta_2 0.9996, l2 0.02, filters 29, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 137, fold 1 trained 10 epochs in 4.793s.  Loss: tr - 0.075, val - 0.064.  Accuracy: tr - 0.977, val - 0.983.\n",
      "lr 0.0001, beta_1 0.8972, beta_2 0.9982, l2 0.0119, filters 32, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 138, fold 1 trained 10 epochs in 3.83s.  Loss: tr - 0.444, val - 0.403.  Accuracy: tr - 0.86, val - 0.882.\n",
      "lr 0.0001, beta_1 0.8657, beta_2 0.9999, l2 0.02, filters 54, kernel_sz 4,  dense_sz 131, activs relu, padding 1\n",
      "Configuration 139, fold 1 trained 10 epochs in 8.126s.  Loss: tr - 0.081, val - 0.067.  Accuracy: tr - 0.977, val - 0.98.\n",
      "lr 0.0001, beta_1 0.8766, beta_2 0.999, l2 0.0169, filters 50, kernel_sz 5,  dense_sz 138, activs relu, padding 0\n",
      "Configuration 140, fold 1 trained 10 epochs in 4.128s.  Loss: tr - 0.386, val - 0.357.  Accuracy: tr - 0.85, val - 0.853.\n",
      "lr 0.0033, beta_1 0.8937, beta_2 0.9999, l2 0.02, filters 57, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 141, fold 1 trained 10 epochs in 8.885s.  Loss: tr - 0.663, val - 0.654.  Accuracy: tr - 0.584, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8919, beta_2 0.9985, l2 0.0169, filters 43, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 142, fold 1 trained 10 epochs in 4.781s.  Loss: tr - 0.188, val - 0.161.  Accuracy: tr - 0.936, val - 0.945.\n",
      "lr 0.0034, beta_1 0.8826, beta_2 0.9981, l2 0.0198, filters 46, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 143, fold 1 trained 6 epochs in 3.04s.  Loss: tr - 31.988, val - 40.789.  Accuracy: tr - 0.575, val - 0.592.\n",
      "Significant difference in validation losses across group, after 2 folds (0.0031836058332736176)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 2 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 102, evaluated once before -- skipping.\n",
      "lr 0.0053, beta_1 0.8832, beta_2 0.9988, l2 0.0182, filters 34, kernel_sz 5,  dense_sz 124, activs relu, padding 1\n",
      "Configuration 116, fold 2 trained 6 epochs in 2.413s.  Loss: tr - 6.865, val - 0.694.  Accuracy: tr - 0.529, val - 0.528.\n",
      "lr 0.0021, beta_1 0.8841, beta_2 0.9999, l2 0.02, filters 49, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 117, fold 2 trained 10 epochs in 5.465s.  Loss: tr - 0.659, val - 0.654.  Accuracy: tr - 0.591, val - 0.591.\n",
      "lr 0.0033, beta_1 0.8752, beta_2 0.9984, l2 0.0163, filters 57, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 118, fold 2 trained 10 epochs in 6.06s.  Loss: tr - 0.681, val - 0.655.  Accuracy: tr - 0.557, val - 0.591.\n",
      "lr 0.0005, beta_1 0.95, beta_2 0.9992, l2 0.0151, filters 55, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 119, fold 2 trained 10 epochs in 5.836s.  Loss: tr - 0.098, val - 0.085.  Accuracy: tr - 0.962, val - 0.968.\n",
      "lr 0.0001, beta_1 0.907, beta_2 0.9999, l2 0.02, filters 38, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 120, fold 2 trained 10 epochs in 4.704s.  Loss: tr - 0.248, val - 0.194.  Accuracy: tr - 0.926, val - 0.946.\n",
      "lr 0.0001, beta_1 0.8999, beta_2 0.9999, l2 0.02, filters 56, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 121, fold 2 trained 10 epochs in 5.77s.  Loss: tr - 0.137, val - 0.112.  Accuracy: tr - 0.955, val - 0.959.\n",
      "lr 0.0049, beta_1 0.934, beta_2 0.9997, l2 0.02, filters 44, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 122, fold 2 trained 6 epochs in 4.147s.  Loss: tr - 34.108, val - 40.979.  Accuracy: tr - 0.562, val - 0.59.\n",
      "lr 0.0003, beta_1 0.9139, beta_2 0.9999, l2 0.02, filters 58, kernel_sz 5,  dense_sz 126, activs relu, padding 1\n",
      "Configuration 123, fold 2 trained 10 epochs in 6.207s.  Loss: tr - 0.094, val - 0.073.  Accuracy: tr - 0.966, val - 0.977.\n",
      "lr 0.0013, beta_1 0.8947, beta_2 0.9999, l2 0.0123, filters 33, kernel_sz 5,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 124, fold 2 trained 10 epochs in 5.67s.  Loss: tr - 0.318, val - 0.209.  Accuracy: tr - 0.865, val - 0.919.\n",
      "lr 0.0007, beta_1 0.8903, beta_2 0.9986, l2 0.02, filters 40, kernel_sz 5,  dense_sz 120, activs relu, padding 1\n",
      "Configuration 125, fold 2 trained 10 epochs in 4.418s.  Loss: tr - 0.073, val - 0.066.  Accuracy: tr - 0.972, val - 0.976.\n",
      "lr 0.0053, beta_1 0.85, beta_2 0.998, l2 0.0149, filters 53, kernel_sz 5,  dense_sz 94, activs relu, padding 1\n",
      "Configuration 126, fold 2 trained 6 epochs in 3.408s.  Loss: tr - 37.197, val - 44.139.  Accuracy: tr - 0.547, val - 0.559.\n",
      "lr 0.0001, beta_1 0.8811, beta_2 0.9999, l2 0.02, filters 37, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 127, fold 2 trained 10 epochs in 4.635s.  Loss: tr - 0.207, val - 0.174.  Accuracy: tr - 0.94, val - 0.948.\n",
      "lr 0.0003, beta_1 0.9275, beta_2 0.9985, l2 0.02, filters 43, kernel_sz 5,  dense_sz 132, activs relu, padding 1\n",
      "Configuration 128, fold 2 trained 10 epochs in 4.789s.  Loss: tr - 0.062, val - 0.056.  Accuracy: tr - 0.977, val - 0.985.\n",
      "lr 0.0001, beta_1 0.8762, beta_2 0.9982, l2 0.0193, filters 59, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 129, fold 2 trained 10 epochs in 6.244s.  Loss: tr - 0.096, val - 0.077.  Accuracy: tr - 0.966, val - 0.977.\n",
      "lr 0.003, beta_1 0.9164, beta_2 0.9985, l2 0.02, filters 42, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 130, fold 2 trained 10 epochs in 6.727s.  Loss: tr - 0.658, val - 0.653.  Accuracy: tr - 0.588, val - 0.591.\n",
      "lr 0.0001, beta_1 0.8964, beta_2 0.9984, l2 0.02, filters 57, kernel_sz 5,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 131, fold 2 trained 10 epochs in 6.077s.  Loss: tr - 0.083, val - 0.069.  Accuracy: tr - 0.972, val - 0.977.\n",
      "lr 0.0086, beta_1 0.9143, beta_2 0.9987, l2 0.0176, filters 45, kernel_sz 5,  dense_sz 129, activs relu, padding 0\n",
      "Configuration 132, fold 2 trained 6 epochs in 2.232s.  Loss: tr - 38.978, val - 47.197.  Accuracy: tr - 0.522, val - 0.528.\n",
      "lr 0.0044, beta_1 0.8904, beta_2 0.9999, l2 0.0156, filters 43, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 133, fold 2 trained 10 epochs in 4.782s.  Loss: tr - 0.661, val - 0.655.  Accuracy: tr - 0.585, val - 0.593.\n",
      "lr 0.0004, beta_1 0.8767, beta_2 0.9999, l2 0.0096, filters 50, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 134, fold 2 trained 10 epochs in 5.852s.  Loss: tr - 0.038, val - 0.04.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 2 trained 10 epochs in 4.838s.  Loss: tr - 0.07, val - 0.063.  Accuracy: tr - 0.975, val - 0.979.\n",
      "lr 0.0001, beta_1 0.8959, beta_2 0.9999, l2 0.0178, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 136, fold 2 trained 10 epochs in 4.793s.  Loss: tr - 0.229, val - 0.189.  Accuracy: tr - 0.933, val - 0.943.\n",
      "lr 0.0001, beta_1 0.8709, beta_2 0.9996, l2 0.02, filters 29, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 137, fold 2 trained 10 epochs in 4.792s.  Loss: tr - 0.144, val - 0.114.  Accuracy: tr - 0.954, val - 0.959.\n",
      "lr 0.0001, beta_1 0.8972, beta_2 0.9982, l2 0.0119, filters 32, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 138, fold 2 trained 10 epochs in 3.82s.  Loss: tr - 0.139, val - 0.109.  Accuracy: tr - 0.959, val - 0.966.\n",
      "lr 0.0001, beta_1 0.8657, beta_2 0.9999, l2 0.02, filters 54, kernel_sz 4,  dense_sz 131, activs relu, padding 1\n",
      "Configuration 139, fold 2 trained 10 epochs in 8.085s.  Loss: tr - 0.13, val - 0.105.  Accuracy: tr - 0.962, val - 0.966.\n",
      "lr 0.0001, beta_1 0.8766, beta_2 0.999, l2 0.0169, filters 50, kernel_sz 5,  dense_sz 138, activs relu, padding 0\n",
      "Configuration 140, fold 2 trained 10 epochs in 4.413s.  Loss: tr - 0.3, val - 0.27.  Accuracy: tr - 0.89, val - 0.91.\n",
      "lr 0.0033, beta_1 0.8937, beta_2 0.9999, l2 0.02, filters 57, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 141, fold 2 trained 10 epochs in 8.616s.  Loss: tr - 0.681, val - 0.656.  Accuracy: tr - 0.578, val - 0.591.\n",
      "lr 0.0001, beta_1 0.8919, beta_2 0.9985, l2 0.0169, filters 43, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 142, fold 2 trained 10 epochs in 4.786s.  Loss: tr - 0.168, val - 0.14.  Accuracy: tr - 0.943, val - 0.952.\n",
      "lr 0.0034, beta_1 0.8826, beta_2 0.9981, l2 0.0198, filters 46, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 143, fold 2 trained 10 epochs in 5.051s.  Loss: tr - 0.655, val - 0.651.  Accuracy: tr - 0.602, val - 0.595.\n",
      "Significant difference in validation losses across group, after 3 folds (2.4274521880852855e-06)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 3 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 102, evaluated once before -- skipping.\n",
      "lr 0.0053, beta_1 0.8832, beta_2 0.9988, l2 0.0182, filters 34, kernel_sz 5,  dense_sz 124, activs relu, padding 1\n",
      "Configuration 116, fold 3 trained 10 epochs in 4.013s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.594, val - 0.599.\n",
      "lr 0.0021, beta_1 0.8841, beta_2 0.9999, l2 0.02, filters 49, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 117, fold 3 trained 10 epochs in 5.449s.  Loss: tr - 0.657, val - 0.65.  Accuracy: tr - 0.593, val - 0.607.\n",
      "lr 0.0033, beta_1 0.8752, beta_2 0.9984, l2 0.0163, filters 57, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 118, fold 3 trained 10 epochs in 6.371s.  Loss: tr - 0.67, val - 0.651.  Accuracy: tr - 0.59, val - 0.599.\n",
      "lr 0.0005, beta_1 0.95, beta_2 0.9992, l2 0.0151, filters 55, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 119, fold 3 trained 10 epochs in 5.856s.  Loss: tr - 0.107, val - 0.103.  Accuracy: tr - 0.959, val - 0.962.\n",
      "lr 0.0001, beta_1 0.907, beta_2 0.9999, l2 0.02, filters 38, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 120, fold 3 trained 10 epochs in 4.424s.  Loss: tr - 0.157, val - 0.141.  Accuracy: tr - 0.949, val - 0.953.\n",
      "lr 0.0001, beta_1 0.8999, beta_2 0.9999, l2 0.02, filters 56, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 121, fold 3 trained 10 epochs in 5.756s.  Loss: tr - 0.182, val - 0.164.  Accuracy: tr - 0.935, val - 0.939.\n",
      "lr 0.0049, beta_1 0.934, beta_2 0.9997, l2 0.02, filters 44, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 122, fold 3 trained 6 epochs in 4.148s.  Loss: tr - 39.888, val - 47.12.  Accuracy: tr - 0.512, val - 0.529.\n",
      "lr 0.0003, beta_1 0.9139, beta_2 0.9999, l2 0.02, filters 58, kernel_sz 5,  dense_sz 126, activs relu, padding 1\n",
      "Configuration 123, fold 3 trained 10 epochs in 6.19s.  Loss: tr - 0.07, val - 0.066.  Accuracy: tr - 0.975, val - 0.978.\n",
      "lr 0.0013, beta_1 0.8947, beta_2 0.9999, l2 0.0123, filters 33, kernel_sz 5,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 124, fold 3 trained 10 epochs in 5.968s.  Loss: tr - 0.054, val - 0.045.  Accuracy: tr - 0.981, val - 0.986.\n",
      "lr 0.0007, beta_1 0.8903, beta_2 0.9986, l2 0.02, filters 40, kernel_sz 5,  dense_sz 120, activs relu, padding 1\n",
      "Configuration 125, fold 3 trained 10 epochs in 4.406s.  Loss: tr - 0.061, val - 0.068.  Accuracy: tr - 0.977, val - 0.971.\n",
      "lr 0.0053, beta_1 0.85, beta_2 0.998, l2 0.0149, filters 53, kernel_sz 5,  dense_sz 94, activs relu, padding 1\n",
      "Configuration 126, fold 3 trained 6 epochs in 3.413s.  Loss: tr - 34.294, val - 39.271.  Accuracy: tr - 0.574, val - 0.607.\n",
      "lr 0.0001, beta_1 0.8811, beta_2 0.9999, l2 0.02, filters 37, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 127, fold 3 trained 10 epochs in 4.31s.  Loss: tr - 0.173, val - 0.165.  Accuracy: tr - 0.946, val - 0.938.\n",
      "lr 0.0003, beta_1 0.9275, beta_2 0.9985, l2 0.02, filters 43, kernel_sz 5,  dense_sz 132, activs relu, padding 1\n",
      "Configuration 128, fold 3 trained 10 epochs in 4.788s.  Loss: tr - 0.101, val - 0.095.  Accuracy: tr - 0.963, val - 0.966.\n",
      "lr 0.0001, beta_1 0.8762, beta_2 0.9982, l2 0.0193, filters 59, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 129, fold 3 trained 10 epochs in 6.216s.  Loss: tr - 0.127, val - 0.114.  Accuracy: tr - 0.959, val - 0.962.\n",
      "lr 0.003, beta_1 0.9164, beta_2 0.9985, l2 0.02, filters 42, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 130, fold 3 trained 10 epochs in 6.749s.  Loss: tr - 0.585, val - 0.468.  Accuracy: tr - 0.722, val - 0.808.\n",
      "lr 0.0001, beta_1 0.8964, beta_2 0.9984, l2 0.02, filters 57, kernel_sz 5,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 131, fold 3 trained 10 epochs in 6.363s.  Loss: tr - 0.116, val - 0.11.  Accuracy: tr - 0.962, val - 0.957.\n",
      "lr 0.0086, beta_1 0.9143, beta_2 0.9987, l2 0.0176, filters 45, kernel_sz 5,  dense_sz 129, activs relu, padding 0\n",
      "Configuration 132, fold 3 trained 6 epochs in 2.235s.  Loss: tr - 33.946, val - 40.138.  Accuracy: tr - 0.586, val - 0.599.\n",
      "lr 0.0044, beta_1 0.8904, beta_2 0.9999, l2 0.0156, filters 43, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 133, fold 3 trained 10 epochs in 4.771s.  Loss: tr - 0.703, val - 0.657.  Accuracy: tr - 0.586, val - 0.599.\n",
      "lr 0.0004, beta_1 0.8767, beta_2 0.9999, l2 0.0096, filters 50, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 134, fold 3 trained 10 epochs in 5.558s.  Loss: tr - 0.064, val - 0.06.  Accuracy: tr - 0.978, val - 0.979.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 3 trained 10 epochs in 4.822s.  Loss: tr - 0.079, val - 0.075.  Accuracy: tr - 0.973, val - 0.973.\n",
      "lr 0.0001, beta_1 0.8959, beta_2 0.9999, l2 0.0178, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 136, fold 3 trained 10 epochs in 4.81s.  Loss: tr - 0.178, val - 0.159.  Accuracy: tr - 0.941, val - 0.943.\n",
      "lr 0.0001, beta_1 0.8709, beta_2 0.9996, l2 0.02, filters 29, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 137, fold 3 trained 10 epochs in 5.079s.  Loss: tr - 0.121, val - 0.122.  Accuracy: tr - 0.96, val - 0.954.\n",
      "lr 0.0001, beta_1 0.8972, beta_2 0.9982, l2 0.0119, filters 32, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 138, fold 3 trained 10 epochs in 3.827s.  Loss: tr - 0.127, val - 0.115.  Accuracy: tr - 0.96, val - 0.961.\n",
      "lr 0.0001, beta_1 0.8657, beta_2 0.9999, l2 0.02, filters 54, kernel_sz 4,  dense_sz 131, activs relu, padding 1\n",
      "Configuration 139, fold 3 trained 10 epochs in 8.074s.  Loss: tr - 0.108, val - 0.096.  Accuracy: tr - 0.97, val - 0.97.\n",
      "lr 0.0001, beta_1 0.8766, beta_2 0.999, l2 0.0169, filters 50, kernel_sz 5,  dense_sz 138, activs relu, padding 0\n",
      "Configuration 140, fold 3 trained 10 epochs in 4.118s.  Loss: tr - 0.4, val - 0.391.  Accuracy: tr - 0.832, val - 0.837.\n",
      "lr 0.0033, beta_1 0.8937, beta_2 0.9999, l2 0.02, filters 57, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 141, fold 3 trained 10 epochs in 8.602s.  Loss: tr - 0.663, val - 0.652.  Accuracy: tr - 0.582, val - 0.599.\n",
      "lr 0.0001, beta_1 0.8919, beta_2 0.9985, l2 0.0169, filters 43, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 142, fold 3 trained 10 epochs in 4.787s.  Loss: tr - 0.167, val - 0.153.  Accuracy: tr - 0.945, val - 0.948.\n",
      "lr 0.0034, beta_1 0.8826, beta_2 0.9981, l2 0.0198, filters 46, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 143, fold 3 trained 10 epochs in 5.354s.  Loss: tr - 0.657, val - 0.651.  Accuracy: tr - 0.597, val - 0.607.\n",
      "Significant difference in validation losses across group, after 4 folds (1.840014800929572e-10)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 4 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 102, evaluated once before -- skipping.\n",
      "lr 0.0053, beta_1 0.8832, beta_2 0.9988, l2 0.0182, filters 34, kernel_sz 5,  dense_sz 124, activs relu, padding 1\n",
      "Configuration 116, fold 4 trained 6 epochs in 2.399s.  Loss: tr - 39.522, val - 50.586.  Accuracy: tr - 0.514, val - 0.494.\n",
      "lr 0.0021, beta_1 0.8841, beta_2 0.9999, l2 0.02, filters 49, kernel_sz 5,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 117, fold 4 trained 10 epochs in 5.453s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.592, val - 0.593.\n",
      "lr 0.0033, beta_1 0.8752, beta_2 0.9984, l2 0.0163, filters 57, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 118, fold 4 trained 6 epochs in 3.64s.  Loss: tr - 40.903, val - 49.975.  Accuracy: tr - 0.5, val - 0.5.\n",
      "lr 0.0005, beta_1 0.95, beta_2 0.9992, l2 0.0151, filters 55, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 119, fold 4 trained 10 epochs in 5.854s.  Loss: tr - 0.077, val - 0.062.  Accuracy: tr - 0.972, val - 0.978.\n",
      "lr 0.0001, beta_1 0.907, beta_2 0.9999, l2 0.02, filters 38, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 120, fold 4 trained 10 epochs in 4.431s.  Loss: tr - 0.098, val - 0.082.  Accuracy: tr - 0.965, val - 0.974.\n",
      "lr 0.0001, beta_1 0.8999, beta_2 0.9999, l2 0.02, filters 56, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 121, fold 4 trained 10 epochs in 5.76s.  Loss: tr - 0.108, val - 0.091.  Accuracy: tr - 0.962, val - 0.969.\n",
      "lr 0.0049, beta_1 0.934, beta_2 0.9997, l2 0.02, filters 44, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 122, fold 4 trained 10 epochs in 7.185s.  Loss: tr - 0.658, val - 0.655.  Accuracy: tr - 0.594, val - 0.592.\n",
      "lr 0.0003, beta_1 0.9139, beta_2 0.9999, l2 0.02, filters 58, kernel_sz 5,  dense_sz 126, activs relu, padding 1\n",
      "Configuration 123, fold 4 trained 10 epochs in 6.202s.  Loss: tr - 0.064, val - 0.055.  Accuracy: tr - 0.978, val - 0.978.\n",
      "lr 0.0013, beta_1 0.8947, beta_2 0.9999, l2 0.0123, filters 33, kernel_sz 5,  dense_sz 138, activs relu, padding 1\n",
      "Configuration 124, fold 4 trained 10 epochs in 5.684s.  Loss: tr - 0.166, val - 0.118.  Accuracy: tr - 0.937, val - 0.959.\n",
      "lr 0.0007, beta_1 0.8903, beta_2 0.9986, l2 0.02, filters 40, kernel_sz 5,  dense_sz 120, activs relu, padding 1\n",
      "Configuration 125, fold 4 trained 10 epochs in 4.421s.  Loss: tr - 0.085, val - 0.069.  Accuracy: tr - 0.97, val - 0.976.\n",
      "lr 0.0053, beta_1 0.85, beta_2 0.998, l2 0.0149, filters 53, kernel_sz 5,  dense_sz 94, activs relu, padding 1\n",
      "Configuration 126, fold 4 trained 6 epochs in 3.41s.  Loss: tr - 34.178, val - 40.8.  Accuracy: tr - 0.571, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8811, beta_2 0.9999, l2 0.02, filters 37, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 127, fold 4 trained 10 epochs in 4.324s.  Loss: tr - 0.126, val - 0.107.  Accuracy: tr - 0.959, val - 0.964.\n",
      "lr 0.0003, beta_1 0.9275, beta_2 0.9985, l2 0.02, filters 43, kernel_sz 5,  dense_sz 132, activs relu, padding 1\n",
      "Configuration 128, fold 4 trained 10 epochs in 5.071s.  Loss: tr - 0.159, val - 0.126.  Accuracy: tr - 0.939, val - 0.951.\n",
      "lr 0.0001, beta_1 0.8762, beta_2 0.9982, l2 0.0193, filters 59, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 129, fold 4 trained 10 epochs in 6.226s.  Loss: tr - 0.054, val - 0.049.  Accuracy: tr - 0.984, val - 0.984.\n",
      "lr 0.003, beta_1 0.9164, beta_2 0.9985, l2 0.02, filters 42, kernel_sz 4,  dense_sz 98, activs relu, padding 1\n",
      "Configuration 130, fold 4 trained 10 epochs in 6.748s.  Loss: tr - 0.639, val - 0.597.  Accuracy: tr - 0.669, val - 0.724.\n",
      "lr 0.0001, beta_1 0.8964, beta_2 0.9984, l2 0.02, filters 57, kernel_sz 5,  dense_sz 125, activs relu, padding 1\n",
      "Configuration 131, fold 4 trained 10 epochs in 6.091s.  Loss: tr - 0.211, val - 0.175.  Accuracy: tr - 0.93, val - 0.944.\n",
      "lr 0.0086, beta_1 0.9143, beta_2 0.9987, l2 0.0176, filters 45, kernel_sz 5,  dense_sz 129, activs relu, padding 0\n",
      "Configuration 132, fold 4 trained 6 epochs in 2.236s.  Loss: tr - 37.572, val - 44.368.  Accuracy: tr - 0.543, val - 0.556.\n",
      "lr 0.0044, beta_1 0.8904, beta_2 0.9999, l2 0.0156, filters 43, kernel_sz 5,  dense_sz 134, activs relu, padding 1\n",
      "Configuration 133, fold 4 trained 6 epochs in 2.87s.  Loss: tr - 33.44, val - 40.698.  Accuracy: tr - 0.566, val - 0.593.\n",
      "lr 0.0004, beta_1 0.8767, beta_2 0.9999, l2 0.0096, filters 50, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 134, fold 4 trained 10 epochs in 5.548s.  Loss: tr - 0.087, val - 0.073.  Accuracy: tr - 0.971, val - 0.976.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 4 trained 10 epochs in 5.109s.  Loss: tr - 0.04, val - 0.034.  Accuracy: tr - 0.987, val - 0.99.\n",
      "lr 0.0001, beta_1 0.8959, beta_2 0.9999, l2 0.0178, filters 43, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 136, fold 4 trained 10 epochs in 4.796s.  Loss: tr - 0.08, val - 0.067.  Accuracy: tr - 0.974, val - 0.976.\n",
      "lr 0.0001, beta_1 0.8709, beta_2 0.9996, l2 0.02, filters 29, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 137, fold 4 trained 10 epochs in 4.825s.  Loss: tr - 0.338, val - 0.269.  Accuracy: tr - 0.896, val - 0.925.\n",
      "lr 0.0001, beta_1 0.8972, beta_2 0.9982, l2 0.0119, filters 32, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 138, fold 4 trained 10 epochs in 3.832s.  Loss: tr - 0.113, val - 0.092.  Accuracy: tr - 0.961, val - 0.97.\n",
      "lr 0.0001, beta_1 0.8657, beta_2 0.9999, l2 0.02, filters 54, kernel_sz 4,  dense_sz 131, activs relu, padding 1\n",
      "Configuration 139, fold 4 trained 10 epochs in 8.08s.  Loss: tr - 0.075, val - 0.065.  Accuracy: tr - 0.979, val - 0.983.\n",
      "lr 0.0001, beta_1 0.8766, beta_2 0.999, l2 0.0169, filters 50, kernel_sz 5,  dense_sz 138, activs relu, padding 0\n",
      "Configuration 140, fold 4 trained 10 epochs in 4.119s.  Loss: tr - 0.472, val - 0.443.  Accuracy: tr - 0.791, val - 0.801.\n",
      "lr 0.0033, beta_1 0.8937, beta_2 0.9999, l2 0.02, filters 57, kernel_sz 4,  dense_sz 150, activs tanh, padding 1\n",
      "Configuration 141, fold 4 trained 10 epochs in 8.9s.  Loss: tr - 0.66, val - 0.655.  Accuracy: tr - 0.579, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8919, beta_2 0.9985, l2 0.0169, filters 43, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 142, fold 4 trained 10 epochs in 4.814s.  Loss: tr - 0.142, val - 0.113.  Accuracy: tr - 0.961, val - 0.969.\n",
      "lr 0.0034, beta_1 0.8826, beta_2 0.9981, l2 0.0198, filters 46, kernel_sz 5,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 143, fold 4 trained 10 epochs in 5.069s.  Loss: tr - 0.657, val - 0.655.  Accuracy: tr - 0.591, val - 0.592.\n",
      "Significant difference in validation losses across group, after 5 folds (2.7759889513879772e-14)\n",
      "Configuration 123 (mean vloss 0.06035095751285553) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 125 (mean vloss 0.0689733348786831) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 129 (mean vloss 0.07446552067995071) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 119 (mean vloss 0.07498799040913581) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 128 (mean vloss 0.07936176061630248) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 124 (mean vloss 0.08423388488590718) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 139 (mean vloss 0.08895325511693955) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 121 (mean vloss 0.10438673049211503) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 131 (mean vloss 0.10622536540031433) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 142 (mean vloss 0.1238166056573391) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 137 (mean vloss 0.13318621516227722) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 127 (mean vloss 0.1353524297475815) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 136 (mean vloss 0.13968058824539184) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 138 (mean vloss 0.17192208468914033) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 120 (mean vloss 0.1731294572353363) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 140 (mean vloss 0.3745811700820923) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 117 (mean vloss 0.6537636876106262) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 141 (mean vloss 0.6546488523483276) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 130 (mean vloss 8.620205956697465) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 143 (mean vloss 8.680993390083312) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 116 (mean vloss 10.648193597793579) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 133 (mean vloss 17.963220608234405) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 122 (mean vloss 18.00902392864227) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 118 (mean vloss 28.97350730895996) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 126 (mean vloss 41.79874038696289) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 132 (mean vloss 44.8799072265625) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Paired tests completed\n",
      "Iteration completed in 721.435s. Remaining configurations 4.  Mean validation loss [0.006 0.005 0.013 0.007 0.01  0.009 0.009 0.013 0.007 0.008 0.012 0.012\n",
      " 0.007 0.014 0.012 0.009 0.01  0.012 0.031 0.012 0.01  0.015 0.018 0.006\n",
      " 0.013 0.016 0.019 0.009] and acc [0.248 0.248 0.247 0.248 0.247 0.248 0.248 0.245 0.247 0.247 0.246 0.247\n",
      " 0.247 0.245 0.246 0.247 0.246 0.246 0.239 0.246 0.248 0.245 0.244 0.248\n",
      " 0.245 0.245 0.243 0.247]\n",
      "---------------------------------------------------------------------------\n",
      "####################################################################################################\n",
      "Iteration 6 of Iterated F-race.\n",
      "Training fold 0 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 135, evaluated once before -- skipping.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 0 trained 10 epochs in 4.83s.  Loss: tr - 0.072, val - 0.059.  Accuracy: tr - 0.975, val - 0.979.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 0 trained 10 epochs in 4.815s.  Loss: tr - 0.033, val - 0.024.  Accuracy: tr - 0.989, val - 0.992.\n",
      "lr 0.0001, beta_1 0.8701, beta_2 0.9999, l2 0.0176, filters 37, kernel_sz 4,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 146, fold 0 trained 10 epochs in 6.375s.  Loss: tr - 0.108, val - 0.088.  Accuracy: tr - 0.968, val - 0.978.\n",
      "lr 0.0014, beta_1 0.8788, beta_2 0.9999, l2 0.0147, filters 39, kernel_sz 5,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 147, fold 0 trained 10 epochs in 4.748s.  Loss: tr - 0.413, val - 0.309.  Accuracy: tr - 0.814, val - 0.875.\n",
      "lr 0.0013, beta_1 0.9216, beta_2 0.9998, l2 0.0142, filters 44, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 148, fold 0 trained 10 epochs in 4.847s.  Loss: tr - 0.291, val - 0.193.  Accuracy: tr - 0.877, val - 0.925.\n",
      "lr 0.0019, beta_1 0.8634, beta_2 0.9992, l2 0.0164, filters 50, kernel_sz 5,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 149, fold 0 trained 10 epochs in 5.537s.  Loss: tr - 0.099, val - 0.06.  Accuracy: tr - 0.966, val - 0.979.\n",
      "lr 0.0019, beta_1 0.8889, beta_2 0.9999, l2 0.02, filters 42, kernel_sz 5,  dense_sz 85, activs relu, padding 1\n",
      "Configuration 150, fold 0 trained 10 epochs in 4.741s.  Loss: tr - 0.628, val - 0.554.  Accuracy: tr - 0.667, val - 0.764.\n",
      "lr 0.0001, beta_1 0.9004, beta_2 0.9973, l2 0.0183, filters 32, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 151, fold 0 trained 10 epochs in 3.815s.  Loss: tr - 0.155, val - 0.132.  Accuracy: tr - 0.952, val - 0.96.\n",
      "lr 0.0001, beta_1 0.8941, beta_2 0.9996, l2 0.0157, filters 47, kernel_sz 5,  dense_sz 59, activs relu, padding 1\n",
      "Configuration 152, fold 0 trained 10 epochs in 5.101s.  Loss: tr - 0.156, val - 0.137.  Accuracy: tr - 0.948, val - 0.954.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 0 trained 10 epochs in 4.973s.  Loss: tr - 0.096, val - 0.069.  Accuracy: tr - 0.965, val - 0.976.\n",
      "lr 0.0001, beta_1 0.8913, beta_2 0.9999, l2 0.0153, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 154, fold 0 trained 10 epochs in 4.921s.  Loss: tr - 0.263, val - 0.222.  Accuracy: tr - 0.913, val - 0.928.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 0 trained 10 epochs in 7.183s.  Loss: tr - 0.091, val - 0.073.  Accuracy: tr - 0.971, val - 0.975.\n",
      "lr 0.0001, beta_1 0.9012, beta_2 0.9999, l2 0.0088, filters 45, kernel_sz 4,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 156, fold 0 trained 10 epochs in 7.187s.  Loss: tr - 0.118, val - 0.087.  Accuracy: tr - 0.971, val - 0.98.\n",
      "lr 0.0001, beta_1 0.9105, beta_2 0.9976, l2 0.0184, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 157, fold 0 trained 10 epochs in 4.903s.  Loss: tr - 0.195, val - 0.168.  Accuracy: tr - 0.94, val - 0.946.\n",
      "lr 0.0031, beta_1 0.8889, beta_2 0.9983, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 64, activs relu, padding 1\n",
      "Configuration 158, fold 0 trained 10 epochs in 3.979s.  Loss: tr - 0.652, val - 0.646.  Accuracy: tr - 0.611, val - 0.633.\n",
      "lr 0.0001, beta_1 0.8784, beta_2 0.9978, l2 0.0141, filters 43, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 159, fold 0 trained 10 epochs in 7.294s.  Loss: tr - 0.175, val - 0.141.  Accuracy: tr - 0.945, val - 0.958.\n",
      "lr 0.0019, beta_1 0.8967, beta_2 0.9975, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 160, fold 0 trained 10 epochs in 3.987s.  Loss: tr - 0.116, val - 0.07.  Accuracy: tr - 0.955, val - 0.98.\n",
      "lr 0.0001, beta_1 0.9154, beta_2 0.9999, l2 0.0144, filters 52, kernel_sz 4,  dense_sz 101, activs relu, padding 1\n",
      "Configuration 161, fold 0 trained 10 epochs in 7.967s.  Loss: tr - 0.113, val - 0.092.  Accuracy: tr - 0.963, val - 0.972.\n",
      "lr 0.0023, beta_1 0.8844, beta_2 0.9998, l2 0.0136, filters 43, kernel_sz 5,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 162, fold 0 trained 10 epochs in 4.776s.  Loss: tr - 0.268, val - 0.199.  Accuracy: tr - 0.895, val - 0.935.\n",
      "lr 0.0001, beta_1 0.8993, beta_2 0.9993, l2 0.0154, filters 37, kernel_sz 4,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 163, fold 0 trained 10 epochs in 6.37s.  Loss: tr - 0.224, val - 0.17.  Accuracy: tr - 0.946, val - 0.956.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 0 trained 10 epochs in 6.998s.  Loss: tr - 0.034, val - 0.029.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0017, beta_1 0.8969, beta_2 0.9973, l2 0.0163, filters 41, kernel_sz 5,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 165, fold 0 trained 10 epochs in 4.958s.  Loss: tr - 0.267, val - 0.187.  Accuracy: tr - 0.894, val - 0.928.\n",
      "lr 0.0032, beta_1 0.8735, beta_2 0.999, l2 0.0187, filters 47, kernel_sz 3,  dense_sz 70, activs relu, padding 1\n",
      "Configuration 166, fold 0 trained 10 epochs in 5.591s.  Loss: tr - 0.625, val - 0.566.  Accuracy: tr - 0.676, val - 0.709.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 0 trained 10 epochs in 5.996s.  Loss: tr - 0.045, val - 0.032.  Accuracy: tr - 0.985, val - 0.989.\n",
      "lr 0.0024, beta_1 0.8996, beta_2 0.9992, l2 0.0182, filters 39, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 168, fold 0 trained 10 epochs in 4.444s.  Loss: tr - 0.654, val - 0.651.  Accuracy: tr - 0.607, val - 0.606.\n",
      "lr 0.0001, beta_1 0.8714, beta_2 0.9999, l2 0.0114, filters 43, kernel_sz 5,  dense_sz 84, activs relu, padding 1\n",
      "Configuration 169, fold 0 trained 10 epochs in 4.788s.  Loss: tr - 0.222, val - 0.19.  Accuracy: tr - 0.918, val - 0.933.\n",
      "lr 0.0013, beta_1 0.8721, beta_2 0.9999, l2 0.0121, filters 48, kernel_sz 5,  dense_sz 51, activs relu, padding 1\n",
      "Configuration 170, fold 0 trained 10 epochs in 5.01s.  Loss: tr - 0.115, val - 0.079.  Accuracy: tr - 0.957, val - 0.969.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 0 trained 10 epochs in 5.179s.  Loss: tr - 0.048, val - 0.037.  Accuracy: tr - 0.984, val - 0.989.\n",
      "Not enough evidence to support a difference across group, after 1 folds (0.46506624123787865)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 1 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 135, evaluated once before -- skipping.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 1 trained 10 epochs in 4.824s.  Loss: tr - 0.063, val - 0.057.  Accuracy: tr - 0.978, val - 0.98.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 1 trained 10 epochs in 4.805s.  Loss: tr - 0.167, val - 0.142.  Accuracy: tr - 0.931, val - 0.937.\n",
      "lr 0.0001, beta_1 0.8701, beta_2 0.9999, l2 0.0176, filters 37, kernel_sz 4,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 146, fold 1 trained 10 epochs in 6.36s.  Loss: tr - 0.318, val - 0.281.  Accuracy: tr - 0.894, val - 0.915.\n",
      "lr 0.0014, beta_1 0.8788, beta_2 0.9999, l2 0.0147, filters 39, kernel_sz 5,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 147, fold 1 trained 10 epochs in 4.442s.  Loss: tr - 0.576, val - 0.498.  Accuracy: tr - 0.727, val - 0.763.\n",
      "lr 0.0013, beta_1 0.9216, beta_2 0.9998, l2 0.0142, filters 44, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 148, fold 1 trained 10 epochs in 4.834s.  Loss: tr - 0.062, val - 0.05.  Accuracy: tr - 0.977, val - 0.984.\n",
      "lr 0.0019, beta_1 0.8634, beta_2 0.9992, l2 0.0164, filters 50, kernel_sz 5,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 149, fold 1 trained 10 epochs in 5.529s.  Loss: tr - 0.638, val - 0.602.  Accuracy: tr - 0.662, val - 0.691.\n",
      "lr 0.0019, beta_1 0.8889, beta_2 0.9999, l2 0.02, filters 42, kernel_sz 5,  dense_sz 85, activs relu, padding 1\n",
      "Configuration 150, fold 1 trained 10 epochs in 5.024s.  Loss: tr - 0.648, val - 0.628.  Accuracy: tr - 0.621, val - 0.692.\n",
      "lr 0.0001, beta_1 0.9004, beta_2 0.9973, l2 0.0183, filters 32, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 151, fold 1 trained 10 epochs in 3.824s.  Loss: tr - 0.268, val - 0.214.  Accuracy: tr - 0.914, val - 0.934.\n",
      "lr 0.0001, beta_1 0.8941, beta_2 0.9996, l2 0.0157, filters 47, kernel_sz 5,  dense_sz 59, activs relu, padding 1\n",
      "Configuration 152, fold 1 trained 10 epochs in 5.103s.  Loss: tr - 0.187, val - 0.165.  Accuracy: tr - 0.937, val - 0.945.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 1 trained 10 epochs in 4.67s.  Loss: tr - 0.03, val - 0.022.  Accuracy: tr - 0.991, val - 0.994.\n",
      "lr 0.0001, beta_1 0.8913, beta_2 0.9999, l2 0.0153, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 154, fold 1 trained 10 epochs in 4.92s.  Loss: tr - 0.249, val - 0.196.  Accuracy: tr - 0.923, val - 0.942.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 1 trained 10 epochs in 7.182s.  Loss: tr - 0.056, val - 0.042.  Accuracy: tr - 0.982, val - 0.985.\n",
      "lr 0.0001, beta_1 0.9012, beta_2 0.9999, l2 0.0088, filters 45, kernel_sz 4,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 156, fold 1 trained 10 epochs in 7.492s.  Loss: tr - 0.295, val - 0.239.  Accuracy: tr - 0.914, val - 0.928.\n",
      "lr 0.0001, beta_1 0.9105, beta_2 0.9976, l2 0.0184, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 157, fold 1 trained 10 epochs in 4.906s.  Loss: tr - 0.19, val - 0.167.  Accuracy: tr - 0.939, val - 0.943.\n",
      "lr 0.0031, beta_1 0.8889, beta_2 0.9983, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 64, activs relu, padding 1\n",
      "Configuration 158, fold 1 trained 10 epochs in 3.988s.  Loss: tr - 0.669, val - 0.666.  Accuracy: tr - 0.593, val - 0.592.\n",
      "lr 0.0001, beta_1 0.8784, beta_2 0.9978, l2 0.0141, filters 43, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 159, fold 1 trained 10 epochs in 7.026s.  Loss: tr - 0.068, val - 0.057.  Accuracy: tr - 0.979, val - 0.986.\n",
      "lr 0.0019, beta_1 0.8967, beta_2 0.9975, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 160, fold 1 trained 10 epochs in 3.994s.  Loss: tr - 0.658, val - 0.653.  Accuracy: tr - 0.595, val - 0.592.\n",
      "lr 0.0001, beta_1 0.9154, beta_2 0.9999, l2 0.0144, filters 52, kernel_sz 4,  dense_sz 101, activs relu, padding 1\n",
      "Configuration 161, fold 1 trained 10 epochs in 7.977s.  Loss: tr - 0.129, val - 0.114.  Accuracy: tr - 0.96, val - 0.96.\n",
      "lr 0.0023, beta_1 0.8844, beta_2 0.9998, l2 0.0136, filters 43, kernel_sz 5,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 162, fold 1 trained 10 epochs in 5.084s.  Loss: tr - 0.069, val - 0.045.  Accuracy: tr - 0.976, val - 0.987.\n",
      "lr 0.0001, beta_1 0.8993, beta_2 0.9993, l2 0.0154, filters 37, kernel_sz 4,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 163, fold 1 trained 10 epochs in 6.385s.  Loss: tr - 0.113, val - 0.09.  Accuracy: tr - 0.97, val - 0.978.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 1 trained 10 epochs in 7.001s.  Loss: tr - 0.039, val - 0.037.  Accuracy: tr - 0.988, val - 0.989.\n",
      "lr 0.0017, beta_1 0.8969, beta_2 0.9973, l2 0.0163, filters 41, kernel_sz 5,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 165, fold 1 trained 10 epochs in 4.67s.  Loss: tr - 0.205, val - 0.174.  Accuracy: tr - 0.925, val - 0.94.\n",
      "lr 0.0032, beta_1 0.8735, beta_2 0.999, l2 0.0187, filters 47, kernel_sz 3,  dense_sz 70, activs relu, padding 1\n",
      "Configuration 166, fold 1 trained 10 epochs in 5.6s.  Loss: tr - 0.052, val - 0.039.  Accuracy: tr - 0.984, val - 0.988.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 1 trained 10 epochs in 5.998s.  Loss: tr - 0.052, val - 0.047.  Accuracy: tr - 0.983, val - 0.983.\n",
      "lr 0.0024, beta_1 0.8996, beta_2 0.9992, l2 0.0182, filters 39, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 168, fold 1 trained 10 epochs in 4.736s.  Loss: tr - 0.655, val - 0.649.  Accuracy: tr - 0.594, val - 0.634.\n",
      "lr 0.0001, beta_1 0.8714, beta_2 0.9999, l2 0.0114, filters 43, kernel_sz 5,  dense_sz 84, activs relu, padding 1\n",
      "Configuration 169, fold 1 trained 10 epochs in 4.802s.  Loss: tr - 0.191, val - 0.158.  Accuracy: tr - 0.94, val - 0.95.\n",
      "lr 0.0013, beta_1 0.8721, beta_2 0.9999, l2 0.0121, filters 48, kernel_sz 5,  dense_sz 51, activs relu, padding 1\n",
      "Configuration 170, fold 1 trained 10 epochs in 5.007s.  Loss: tr - 0.478, val - 0.35.  Accuracy: tr - 0.765, val - 0.87.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 1 trained 10 epochs in 4.887s.  Loss: tr - 0.046, val - 0.034.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Not enough evidence to support a difference across group, after 2 folds (0.060495768301381314)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 2 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 135, evaluated once before -- skipping.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 2 trained 10 epochs in 4.827s.  Loss: tr - 0.034, val - 0.036.  Accuracy: tr - 0.989, val - 0.991.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 2 trained 10 epochs in 4.8s.  Loss: tr - 0.035, val - 0.037.  Accuracy: tr - 0.989, val - 0.991.\n",
      "lr 0.0001, beta_1 0.8701, beta_2 0.9999, l2 0.0176, filters 37, kernel_sz 4,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 146, fold 2 trained 10 epochs in 6.656s.  Loss: tr - 0.152, val - 0.124.  Accuracy: tr - 0.956, val - 0.965.\n",
      "lr 0.0014, beta_1 0.8788, beta_2 0.9999, l2 0.0147, filters 39, kernel_sz 5,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 147, fold 2 trained 10 epochs in 4.46s.  Loss: tr - 0.142, val - 0.104.  Accuracy: tr - 0.945, val - 0.961.\n",
      "lr 0.0013, beta_1 0.9216, beta_2 0.9998, l2 0.0142, filters 44, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 148, fold 2 trained 10 epochs in 4.818s.  Loss: tr - 0.226, val - 0.167.  Accuracy: tr - 0.909, val - 0.937.\n",
      "lr 0.0019, beta_1 0.8634, beta_2 0.9992, l2 0.0164, filters 50, kernel_sz 5,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 149, fold 2 trained 10 epochs in 5.532s.  Loss: tr - 0.647, val - 0.626.  Accuracy: tr - 0.653, val - 0.707.\n",
      "lr 0.0019, beta_1 0.8889, beta_2 0.9999, l2 0.02, filters 42, kernel_sz 5,  dense_sz 85, activs relu, padding 1\n",
      "Configuration 150, fold 2 trained 10 epochs in 4.732s.  Loss: tr - 0.526, val - 0.392.  Accuracy: tr - 0.77, val - 0.828.\n",
      "lr 0.0001, beta_1 0.9004, beta_2 0.9973, l2 0.0183, filters 32, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 151, fold 2 trained 10 epochs in 3.82s.  Loss: tr - 0.239, val - 0.193.  Accuracy: tr - 0.927, val - 0.942.\n",
      "lr 0.0001, beta_1 0.8941, beta_2 0.9996, l2 0.0157, filters 47, kernel_sz 5,  dense_sz 59, activs relu, padding 1\n",
      "Configuration 152, fold 2 trained 10 epochs in 5.387s.  Loss: tr - 0.267, val - 0.22.  Accuracy: tr - 0.901, val - 0.924.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 2 trained 10 epochs in 4.688s.  Loss: tr - 0.048, val - 0.044.  Accuracy: tr - 0.984, val - 0.988.\n",
      "lr 0.0001, beta_1 0.8913, beta_2 0.9999, l2 0.0153, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 154, fold 2 trained 10 epochs in 4.921s.  Loss: tr - 0.082, val - 0.067.  Accuracy: tr - 0.973, val - 0.98.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 2 trained 10 epochs in 7.172s.  Loss: tr - 0.05, val - 0.045.  Accuracy: tr - 0.983, val - 0.987.\n",
      "lr 0.0001, beta_1 0.9012, beta_2 0.9999, l2 0.0088, filters 45, kernel_sz 4,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 156, fold 2 trained 10 epochs in 7.18s.  Loss: tr - 0.101, val - 0.089.  Accuracy: tr - 0.967, val - 0.967.\n",
      "lr 0.0001, beta_1 0.9105, beta_2 0.9976, l2 0.0184, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 157, fold 2 trained 10 epochs in 4.907s.  Loss: tr - 0.194, val - 0.163.  Accuracy: tr - 0.933, val - 0.943.\n",
      "lr 0.0031, beta_1 0.8889, beta_2 0.9983, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 64, activs relu, padding 1\n",
      "Configuration 158, fold 2 trained 10 epochs in 4.267s.  Loss: tr - 0.422, val - 0.361.  Accuracy: tr - 0.812, val - 0.829.\n",
      "lr 0.0001, beta_1 0.8784, beta_2 0.9978, l2 0.0141, filters 43, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 159, fold 2 trained 10 epochs in 6.984s.  Loss: tr - 0.119, val - 0.099.  Accuracy: tr - 0.963, val - 0.969.\n",
      "lr 0.0019, beta_1 0.8967, beta_2 0.9975, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 160, fold 2 trained 10 epochs in 3.961s.  Loss: tr - 0.262, val - 0.192.  Accuracy: tr - 0.89, val - 0.926.\n",
      "lr 0.0001, beta_1 0.9154, beta_2 0.9999, l2 0.0144, filters 52, kernel_sz 4,  dense_sz 101, activs relu, padding 1\n",
      "Configuration 161, fold 2 trained 10 epochs in 7.909s.  Loss: tr - 0.4, val - 0.329.  Accuracy: tr - 0.911, val - 0.926.\n",
      "lr 0.0023, beta_1 0.8844, beta_2 0.9998, l2 0.0136, filters 43, kernel_sz 5,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 162, fold 2 trained 10 epochs in 4.765s.  Loss: tr - 0.491, val - 0.359.  Accuracy: tr - 0.759, val - 0.868.\n",
      "lr 0.0001, beta_1 0.8993, beta_2 0.9993, l2 0.0154, filters 37, kernel_sz 4,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 163, fold 2 trained 10 epochs in 6.366s.  Loss: tr - 0.102, val - 0.085.  Accuracy: tr - 0.97, val - 0.977.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 2 trained 10 epochs in 7.28s.  Loss: tr - 0.034, val - 0.037.  Accuracy: tr - 0.988, val - 0.991.\n",
      "lr 0.0017, beta_1 0.8969, beta_2 0.9973, l2 0.0163, filters 41, kernel_sz 5,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 165, fold 2 trained 10 epochs in 4.668s.  Loss: tr - 0.447, val - 0.353.  Accuracy: tr - 0.8, val - 0.854.\n",
      "lr 0.0032, beta_1 0.8735, beta_2 0.999, l2 0.0187, filters 47, kernel_sz 3,  dense_sz 70, activs relu, padding 1\n",
      "Configuration 166, fold 2 trained 10 epochs in 5.594s.  Loss: tr - 0.599, val - 0.506.  Accuracy: tr - 0.704, val - 0.84.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 2 trained 10 epochs in 5.995s.  Loss: tr - 0.035, val - 0.04.  Accuracy: tr - 0.988, val - 0.99.\n",
      "lr 0.0024, beta_1 0.8996, beta_2 0.9992, l2 0.0182, filters 39, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 168, fold 2 trained 10 epochs in 4.442s.  Loss: tr - 0.489, val - 0.431.  Accuracy: tr - 0.756, val - 0.812.\n",
      "lr 0.0001, beta_1 0.8714, beta_2 0.9999, l2 0.0114, filters 43, kernel_sz 5,  dense_sz 84, activs relu, padding 1\n",
      "Configuration 169, fold 2 trained 10 epochs in 4.79s.  Loss: tr - 0.174, val - 0.142.  Accuracy: tr - 0.945, val - 0.964.\n",
      "lr 0.0013, beta_1 0.8721, beta_2 0.9999, l2 0.0121, filters 48, kernel_sz 5,  dense_sz 51, activs relu, padding 1\n",
      "Configuration 170, fold 2 trained 10 epochs in 5.279s.  Loss: tr - 0.053, val - 0.051.  Accuracy: tr - 0.982, val - 0.984.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 2 trained 10 epochs in 4.898s.  Loss: tr - 0.052, val - 0.053.  Accuracy: tr - 0.983, val - 0.985.\n",
      "Significant difference in validation losses across group, after 3 folds (0.0004958581794342233)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 3 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 135, evaluated once before -- skipping.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 3 trained 10 epochs in 4.799s.  Loss: tr - 0.078, val - 0.074.  Accuracy: tr - 0.973, val - 0.975.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 3 trained 10 epochs in 4.807s.  Loss: tr - 0.045, val - 0.045.  Accuracy: tr - 0.985, val - 0.984.\n",
      "lr 0.0001, beta_1 0.8701, beta_2 0.9999, l2 0.0176, filters 37, kernel_sz 4,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 146, fold 3 trained 10 epochs in 6.36s.  Loss: tr - 0.248, val - 0.217.  Accuracy: tr - 0.936, val - 0.939.\n",
      "lr 0.0014, beta_1 0.8788, beta_2 0.9999, l2 0.0147, filters 39, kernel_sz 5,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 147, fold 3 trained 10 epochs in 4.464s.  Loss: tr - 0.255, val - 0.199.  Accuracy: tr - 0.906, val - 0.923.\n",
      "lr 0.0013, beta_1 0.9216, beta_2 0.9998, l2 0.0142, filters 44, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 148, fold 3 trained 10 epochs in 5.101s.  Loss: tr - 0.64, val - 0.615.  Accuracy: tr - 0.619, val - 0.659.\n",
      "lr 0.0019, beta_1 0.8634, beta_2 0.9992, l2 0.0164, filters 50, kernel_sz 5,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 149, fold 3 trained 10 epochs in 5.533s.  Loss: tr - 0.396, val - 0.342.  Accuracy: tr - 0.826, val - 0.858.\n",
      "lr 0.0019, beta_1 0.8889, beta_2 0.9999, l2 0.02, filters 42, kernel_sz 5,  dense_sz 85, activs relu, padding 1\n",
      "Configuration 150, fold 3 trained 10 epochs in 4.73s.  Loss: tr - 0.604, val - 0.508.  Accuracy: tr - 0.698, val - 0.762.\n",
      "lr 0.0001, beta_1 0.9004, beta_2 0.9973, l2 0.0183, filters 32, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 151, fold 3 trained 10 epochs in 3.818s.  Loss: tr - 0.275, val - 0.245.  Accuracy: tr - 0.919, val - 0.925.\n",
      "lr 0.0001, beta_1 0.8941, beta_2 0.9996, l2 0.0157, filters 47, kernel_sz 5,  dense_sz 59, activs relu, padding 1\n",
      "Configuration 152, fold 3 trained 10 epochs in 5.097s.  Loss: tr - 0.192, val - 0.178.  Accuracy: tr - 0.93, val - 0.935.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 3 trained 10 epochs in 4.689s.  Loss: tr - 0.071, val - 0.06.  Accuracy: tr - 0.975, val - 0.979.\n",
      "lr 0.0001, beta_1 0.8913, beta_2 0.9999, l2 0.0153, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 154, fold 3 trained 10 epochs in 5.189s.  Loss: tr - 0.231, val - 0.206.  Accuracy: tr - 0.923, val - 0.928.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 3 trained 10 epochs in 7.152s.  Loss: tr - 0.186, val - 0.175.  Accuracy: tr - 0.928, val - 0.93.\n",
      "lr 0.0001, beta_1 0.9012, beta_2 0.9999, l2 0.0088, filters 45, kernel_sz 4,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 156, fold 3 trained 10 epochs in 7.164s.  Loss: tr - 0.347, val - 0.306.  Accuracy: tr - 0.909, val - 0.912.\n",
      "lr 0.0001, beta_1 0.9105, beta_2 0.9976, l2 0.0184, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 157, fold 3 trained 10 epochs in 4.895s.  Loss: tr - 0.238, val - 0.221.  Accuracy: tr - 0.929, val - 0.925.\n",
      "lr 0.0031, beta_1 0.8889, beta_2 0.9983, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 64, activs relu, padding 1\n",
      "Configuration 158, fold 3 trained 10 epochs in 3.976s.  Loss: tr - 0.654, val - 0.647.  Accuracy: tr - 0.598, val - 0.604.\n",
      "lr 0.0001, beta_1 0.8784, beta_2 0.9978, l2 0.0141, filters 43, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 159, fold 3 trained 10 epochs in 6.992s.  Loss: tr - 0.122, val - 0.105.  Accuracy: tr - 0.965, val - 0.97.\n",
      "lr 0.0019, beta_1 0.8967, beta_2 0.9975, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 160, fold 3 trained 10 epochs in 4.27s.  Loss: tr - 0.45, val - 0.381.  Accuracy: tr - 0.78, val - 0.827.\n",
      "lr 0.0001, beta_1 0.9154, beta_2 0.9999, l2 0.0144, filters 52, kernel_sz 4,  dense_sz 101, activs relu, padding 1\n",
      "Configuration 161, fold 3 trained 10 epochs in 7.926s.  Loss: tr - 0.192, val - 0.165.  Accuracy: tr - 0.947, val - 0.945.\n",
      "lr 0.0023, beta_1 0.8844, beta_2 0.9998, l2 0.0136, filters 43, kernel_sz 5,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 162, fold 3 trained 10 epochs in 4.768s.  Loss: tr - 0.19, val - 0.139.  Accuracy: tr - 0.928, val - 0.953.\n",
      "lr 0.0001, beta_1 0.8993, beta_2 0.9993, l2 0.0154, filters 37, kernel_sz 4,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 163, fold 3 trained 10 epochs in 6.365s.  Loss: tr - 0.114, val - 0.107.  Accuracy: tr - 0.965, val - 0.966.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 3 trained 10 epochs in 6.991s.  Loss: tr - 0.03, val - 0.029.  Accuracy: tr - 0.991, val - 0.99.\n",
      "lr 0.0017, beta_1 0.8969, beta_2 0.9973, l2 0.0163, filters 41, kernel_sz 5,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 165, fold 3 trained 10 epochs in 4.666s.  Loss: tr - 0.356, val - 0.272.  Accuracy: tr - 0.851, val - 0.894.\n",
      "lr 0.0032, beta_1 0.8735, beta_2 0.999, l2 0.0187, filters 47, kernel_sz 3,  dense_sz 70, activs relu, padding 1\n",
      "Configuration 166, fold 3 trained 10 epochs in 5.892s.  Loss: tr - 0.615, val - 0.501.  Accuracy: tr - 0.671, val - 0.762.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 3 trained 10 epochs in 6.01s.  Loss: tr - 0.069, val - 0.066.  Accuracy: tr - 0.975, val - 0.977.\n",
      "lr 0.0024, beta_1 0.8996, beta_2 0.9992, l2 0.0182, filters 39, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 168, fold 3 trained 10 epochs in 4.45s.  Loss: tr - 0.662, val - 0.652.  Accuracy: tr - 0.587, val - 0.599.\n",
      "lr 0.0001, beta_1 0.8714, beta_2 0.9999, l2 0.0114, filters 43, kernel_sz 5,  dense_sz 84, activs relu, padding 1\n",
      "Configuration 169, fold 3 trained 10 epochs in 4.802s.  Loss: tr - 0.091, val - 0.086.  Accuracy: tr - 0.977, val - 0.976.\n",
      "lr 0.0013, beta_1 0.8721, beta_2 0.9999, l2 0.0121, filters 48, kernel_sz 5,  dense_sz 51, activs relu, padding 1\n",
      "Configuration 170, fold 3 trained 10 epochs in 4.977s.  Loss: tr - 0.42, val - 0.364.  Accuracy: tr - 0.815, val - 0.853.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 3 trained 10 epochs in 4.89s.  Loss: tr - 0.045, val - 0.048.  Accuracy: tr - 0.985, val - 0.983.\n",
      "Significant difference in validation losses across group, after 4 folds (3.078230760840699e-07)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 4 on list of 30 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "Configuration 135, evaluated once before -- skipping.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 4 trained 10 epochs in 5.154s.  Loss: tr - 0.05, val - 0.045.  Accuracy: tr - 0.983, val - 0.986.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 4 trained 10 epochs in 4.813s.  Loss: tr - 0.095, val - 0.08.  Accuracy: tr - 0.965, val - 0.97.\n",
      "lr 0.0001, beta_1 0.8701, beta_2 0.9999, l2 0.0176, filters 37, kernel_sz 4,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 146, fold 4 trained 10 epochs in 6.361s.  Loss: tr - 0.174, val - 0.147.  Accuracy: tr - 0.947, val - 0.95.\n",
      "lr 0.0014, beta_1 0.8788, beta_2 0.9999, l2 0.0147, filters 39, kernel_sz 5,  dense_sz 54, activs relu, padding 1\n",
      "Configuration 147, fold 4 trained 10 epochs in 4.464s.  Loss: tr - 0.075, val - 0.056.  Accuracy: tr - 0.973, val - 0.981.\n",
      "lr 0.0013, beta_1 0.9216, beta_2 0.9998, l2 0.0142, filters 44, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 148, fold 4 trained 10 epochs in 4.809s.  Loss: tr - 0.36, val - 0.237.  Accuracy: tr - 0.839, val - 0.901.\n",
      "lr 0.0019, beta_1 0.8634, beta_2 0.9992, l2 0.0164, filters 50, kernel_sz 5,  dense_sz 77, activs relu, padding 1\n",
      "Configuration 149, fold 4 trained 10 epochs in 5.53s.  Loss: tr - 0.542, val - 0.442.  Accuracy: tr - 0.739, val - 0.774.\n",
      "lr 0.0019, beta_1 0.8889, beta_2 0.9999, l2 0.02, filters 42, kernel_sz 5,  dense_sz 85, activs relu, padding 1\n",
      "Configuration 150, fold 4 trained 10 epochs in 5.021s.  Loss: tr - 0.053, val - 0.046.  Accuracy: tr - 0.982, val - 0.985.\n",
      "lr 0.0001, beta_1 0.9004, beta_2 0.9973, l2 0.0183, filters 32, kernel_sz 5,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 151, fold 4 trained 10 epochs in 3.824s.  Loss: tr - 0.135, val - 0.115.  Accuracy: tr - 0.955, val - 0.96.\n",
      "lr 0.0001, beta_1 0.8941, beta_2 0.9996, l2 0.0157, filters 47, kernel_sz 5,  dense_sz 59, activs relu, padding 1\n",
      "Configuration 152, fold 4 trained 10 epochs in 5.096s.  Loss: tr - 0.251, val - 0.202.  Accuracy: tr - 0.913, val - 0.925.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 4 trained 10 epochs in 4.701s.  Loss: tr - 0.045, val - 0.035.  Accuracy: tr - 0.984, val - 0.989.\n",
      "lr 0.0001, beta_1 0.8913, beta_2 0.9999, l2 0.0153, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 154, fold 4 trained 10 epochs in 4.903s.  Loss: tr - 0.242, val - 0.2.  Accuracy: tr - 0.917, val - 0.928.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 4 trained 10 epochs in 7.159s.  Loss: tr - 0.043, val - 0.032.  Accuracy: tr - 0.985, val - 0.991.\n",
      "lr 0.0001, beta_1 0.9012, beta_2 0.9999, l2 0.0088, filters 45, kernel_sz 4,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 156, fold 4 trained 10 epochs in 7.169s.  Loss: tr - 0.338, val - 0.28.  Accuracy: tr - 0.915, val - 0.924.\n",
      "lr 0.0001, beta_1 0.9105, beta_2 0.9976, l2 0.0184, filters 30, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 157, fold 4 trained 10 epochs in 5.188s.  Loss: tr - 0.197, val - 0.159.  Accuracy: tr - 0.934, val - 0.951.\n",
      "lr 0.0031, beta_1 0.8889, beta_2 0.9983, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 64, activs relu, padding 1\n",
      "Configuration 158, fold 4 trained 10 epochs in 4.007s.  Loss: tr - 0.689, val - 0.662.  Accuracy: tr - 0.592, val - 0.593.\n",
      "lr 0.0001, beta_1 0.8784, beta_2 0.9978, l2 0.0141, filters 43, kernel_sz 4,  dense_sz 109, activs relu, padding 1\n",
      "Configuration 159, fold 4 trained 10 epochs in 6.992s.  Loss: tr - 0.102, val - 0.085.  Accuracy: tr - 0.965, val - 0.975.\n",
      "lr 0.0019, beta_1 0.8967, beta_2 0.9975, l2 0.0157, filters 36, kernel_sz 5,  dense_sz 96, activs relu, padding 1\n",
      "Configuration 160, fold 4 trained 10 epochs in 3.97s.  Loss: tr - 0.397, val - 0.292.  Accuracy: tr - 0.825, val - 0.876.\n",
      "lr 0.0001, beta_1 0.9154, beta_2 0.9999, l2 0.0144, filters 52, kernel_sz 4,  dense_sz 101, activs relu, padding 1\n",
      "Configuration 161, fold 4 trained 10 epochs in 7.918s.  Loss: tr - 0.107, val - 0.087.  Accuracy: tr - 0.961, val - 0.971.\n",
      "lr 0.0023, beta_1 0.8844, beta_2 0.9998, l2 0.0136, filters 43, kernel_sz 5,  dense_sz 90, activs relu, padding 1\n",
      "Configuration 162, fold 4 trained 10 epochs in 4.767s.  Loss: tr - 0.65, val - 0.637.  Accuracy: tr - 0.618, val - 0.659.\n",
      "lr 0.0001, beta_1 0.8993, beta_2 0.9993, l2 0.0154, filters 37, kernel_sz 4,  dense_sz 106, activs relu, padding 1\n",
      "Configuration 163, fold 4 trained 10 epochs in 6.685s.  Loss: tr - 0.103, val - 0.083.  Accuracy: tr - 0.968, val - 0.975.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 4 trained 10 epochs in 6.982s.  Loss: tr - 0.05, val - 0.041.  Accuracy: tr - 0.983, val - 0.987.\n",
      "lr 0.0017, beta_1 0.8969, beta_2 0.9973, l2 0.0163, filters 41, kernel_sz 5,  dense_sz 97, activs relu, padding 1\n",
      "Configuration 165, fold 4 trained 10 epochs in 4.668s.  Loss: tr - 0.471, val - 0.563.  Accuracy: tr - 0.807, val - 0.793.\n",
      "lr 0.0032, beta_1 0.8735, beta_2 0.999, l2 0.0187, filters 47, kernel_sz 3,  dense_sz 70, activs relu, padding 1\n",
      "Configuration 166, fold 4 trained 10 epochs in 5.588s.  Loss: tr - 0.436, val - 0.306.  Accuracy: tr - 0.813, val - 0.892.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 4 trained 10 epochs in 6.011s.  Loss: tr - 0.091, val - 0.066.  Accuracy: tr - 0.964, val - 0.975.\n",
      "lr 0.0024, beta_1 0.8996, beta_2 0.9992, l2 0.0182, filters 39, kernel_sz 5,  dense_sz 86, activs relu, padding 1\n",
      "Configuration 168, fold 4 trained 10 epochs in 4.447s.  Loss: tr - 0.46, val - 0.386.  Accuracy: tr - 0.761, val - 0.84.\n",
      "lr 0.0001, beta_1 0.8714, beta_2 0.9999, l2 0.0114, filters 43, kernel_sz 5,  dense_sz 84, activs relu, padding 1\n",
      "Configuration 169, fold 4 trained 10 epochs in 5.109s.  Loss: tr - 0.117, val - 0.103.  Accuracy: tr - 0.962, val - 0.966.\n",
      "lr 0.0013, beta_1 0.8721, beta_2 0.9999, l2 0.0121, filters 48, kernel_sz 5,  dense_sz 51, activs relu, padding 1\n",
      "Configuration 170, fold 4 trained 10 epochs in 4.981s.  Loss: tr - 0.301, val - 0.244.  Accuracy: tr - 0.864, val - 0.899.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 4 trained 10 epochs in 4.888s.  Loss: tr - 0.039, val - 0.033.  Accuracy: tr - 0.987, val - 0.99.\n",
      "Significant difference in validation losses across group, after 5 folds (3.6667391648467185e-10)\n",
      "Configuration 159 (mean vloss 0.09757867455482483) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 163 (mean vloss 0.10688837468624116) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 169 (mean vloss 0.13585451990365982) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 161 (mean vloss 0.15737104415893555) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 146 (mean vloss 0.17156697064638138) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 157 (mean vloss 0.1756984919309616) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 154 (mean vloss 0.1783214509487152) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 151 (mean vloss 0.17961453199386596) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 152 (mean vloss 0.18058953583240508) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 156 (mean vloss 0.20022617876529694) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 170 (mean vloss 0.21770967468619346) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 147 (mean vloss 0.23313051462173462) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 148 (mean vloss 0.252538525313139) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 162 (mean vloss 0.27592437267303466) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 165 (mean vloss 0.30970162749290464) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 160 (mean vloss 0.31779188215732573) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 166 (mean vloss 0.3835741639137268) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 149 (mean vloss 0.41442508324980737) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 150 (mean vloss 0.42573940455913545) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 168 (mean vloss 0.5536626219749451) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Configuration 158 (mean vloss 0.5963812172412872) dropped with pvalue 0.03125 against configuration 78 (mean vloss 0.03256055861711502)\n",
      "Paired tests completed\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 5 on list of 9 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 5 trained 10 epochs in 4.89s.  Loss: tr - 0.063, val - 0.064.  Accuracy: tr - 0.977, val - 0.979.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 5 trained 10 epochs in 4.779s.  Loss: tr - 0.048, val - 0.053.  Accuracy: tr - 0.983, val - 0.981.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 5 trained 10 epochs in 4.813s.  Loss: tr - 0.053, val - 0.052.  Accuracy: tr - 0.981, val - 0.983.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 5 trained 10 epochs in 4.983s.  Loss: tr - 0.055, val - 0.053.  Accuracy: tr - 0.98, val - 0.984.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 5 trained 10 epochs in 7.171s.  Loss: tr - 0.165, val - 0.168.  Accuracy: tr - 0.934, val - 0.93.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 5 trained 10 epochs in 6.958s.  Loss: tr - 0.139, val - 0.099.  Accuracy: tr - 0.944, val - 0.97.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 5 trained 10 epochs in 5.99s.  Loss: tr - 0.053, val - 0.05.  Accuracy: tr - 0.982, val - 0.984.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 5 trained 10 epochs in 4.883s.  Loss: tr - 0.058, val - 0.051.  Accuracy: tr - 0.979, val - 0.984.\n",
      "Not enough evidence to support a difference across group, after 6 folds (0.3954033696023559)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 6 on list of 9 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 6 trained 10 epochs in 4.916s.  Loss: tr - 0.055, val - 0.052.  Accuracy: tr - 0.982, val - 0.982.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 6 trained 10 epochs in 5.074s.  Loss: tr - 0.073, val - 0.063.  Accuracy: tr - 0.974, val - 0.976.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 6 trained 10 epochs in 4.806s.  Loss: tr - 0.046, val - 0.045.  Accuracy: tr - 0.985, val - 0.986.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 6 trained 10 epochs in 4.673s.  Loss: tr - 0.048, val - 0.044.  Accuracy: tr - 0.984, val - 0.987.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 6 trained 10 epochs in 7.17s.  Loss: tr - 0.085, val - 0.062.  Accuracy: tr - 0.971, val - 0.979.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 6 trained 10 epochs in 6.98s.  Loss: tr - 0.107, val - 0.094.  Accuracy: tr - 0.961, val - 0.966.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 6 trained 10 epochs in 6.023s.  Loss: tr - 0.053, val - 0.052.  Accuracy: tr - 0.982, val - 0.98.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 6 trained 10 epochs in 5.183s.  Loss: tr - 0.098, val - 0.084.  Accuracy: tr - 0.965, val - 0.967.\n",
      "Not enough evidence to support a difference across group, after 7 folds (0.24303017496230225)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 7 on list of 9 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 7 trained 10 epochs in 4.833s.  Loss: tr - 0.041, val - 0.04.  Accuracy: tr - 0.987, val - 0.985.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 7 trained 10 epochs in 4.778s.  Loss: tr - 0.074, val - 0.067.  Accuracy: tr - 0.974, val - 0.973.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 7 trained 10 epochs in 4.817s.  Loss: tr - 0.119, val - 0.099.  Accuracy: tr - 0.956, val - 0.964.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 7 trained 10 epochs in 4.703s.  Loss: tr - 0.101, val - 0.089.  Accuracy: tr - 0.964, val - 0.971.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 7 trained 10 epochs in 7.168s.  Loss: tr - 0.081, val - 0.055.  Accuracy: tr - 0.969, val - 0.979.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 7 trained 10 epochs in 7.266s.  Loss: tr - 0.067, val - 0.058.  Accuracy: tr - 0.978, val - 0.98.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 7 trained 10 epochs in 6.008s.  Loss: tr - 0.037, val - 0.035.  Accuracy: tr - 0.988, val - 0.988.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 7 trained 10 epochs in 4.898s.  Loss: tr - 0.062, val - 0.068.  Accuracy: tr - 0.977, val - 0.975.\n",
      "Not enough evidence to support a difference across group, after 8 folds (0.2892051512376863)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 8 on list of 9 configurations.\n",
      "Configuration 78, evaluated once before -- skipping.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 8 trained 10 epochs in 4.91s.  Loss: tr - 0.275, val - 0.21.  Accuracy: tr - 0.896, val - 0.91.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 8 trained 10 epochs in 4.784s.  Loss: tr - 0.051, val - 0.042.  Accuracy: tr - 0.982, val - 0.984.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 8 trained 10 epochs in 4.807s.  Loss: tr - 0.141, val - 0.105.  Accuracy: tr - 0.946, val - 0.962.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 8 trained 10 epochs in 4.966s.  Loss: tr - 0.106, val - 0.079.  Accuracy: tr - 0.962, val - 0.97.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 8 trained 10 epochs in 7.167s.  Loss: tr - 0.073, val - 0.048.  Accuracy: tr - 0.976, val - 0.984.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 8 trained 10 epochs in 6.973s.  Loss: tr - 0.045, val - 0.039.  Accuracy: tr - 0.986, val - 0.986.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 8 trained 10 epochs in 6.007s.  Loss: tr - 0.121, val - 0.092.  Accuracy: tr - 0.957, val - 0.968.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 8 trained 10 epochs in 4.893s.  Loss: tr - 0.046, val - 0.038.  Accuracy: tr - 0.985, val - 0.987.\n",
      "Not enough evidence to support a difference across group, after 9 folds (0.16070056117408402)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training fold 9 on list of 9 configurations.\n",
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Configuration 78, fold 9 trained 10 epochs in 8.088s.  Loss: tr - 0.028, val - 0.025.  Accuracy: tr - 0.992, val - 0.992.\n",
      "lr 0.0006, beta_1 0.8904, beta_2 0.9991, l2 0.016, filters 44, kernel_sz 5,  dense_sz 80, activs relu, padding 1\n",
      "Configuration 135, fold 9 trained 10 epochs in 5.114s.  Loss: tr - 0.054, val - 0.055.  Accuracy: tr - 0.98, val - 0.978.\n",
      "lr 0.0005, beta_1 0.8794, beta_2 0.9999, l2 0.0142, filters 43, kernel_sz 5,  dense_sz 78, activs relu, padding 1\n",
      "Configuration 144, fold 9 trained 10 epochs in 4.783s.  Loss: tr - 0.064, val - 0.066.  Accuracy: tr - 0.977, val - 0.974.\n",
      "lr 0.0008, beta_1 0.8763, beta_2 0.9991, l2 0.0178, filters 44, kernel_sz 5,  dense_sz 72, activs relu, padding 1\n",
      "Configuration 145, fold 9 trained 10 epochs in 4.824s.  Loss: tr - 0.04, val - 0.03.  Accuracy: tr - 0.988, val - 0.988.\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Configuration 153, fold 9 trained 10 epochs in 4.67s.  Loss: tr - 0.055, val - 0.052.  Accuracy: tr - 0.981, val - 0.979.\n",
      "lr 0.0005, beta_1 0.9119, beta_2 0.9999, l2 0.0176, filters 45, kernel_sz 4,  dense_sz 71, activs relu, padding 1\n",
      "Configuration 155, fold 9 trained 10 epochs in 7.164s.  Loss: tr - 0.105, val - 0.076.  Accuracy: tr - 0.964, val - 0.977.\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Configuration 164, fold 9 trained 10 epochs in 6.975s.  Loss: tr - 0.026, val - 0.022.  Accuracy: tr - 0.992, val - 0.994.\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Configuration 167, fold 9 trained 10 epochs in 6.293s.  Loss: tr - 0.055, val - 0.054.  Accuracy: tr - 0.98, val - 0.979.\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Configuration 171, fold 9 trained 10 epochs in 4.914s.  Loss: tr - 0.083, val - 0.083.  Accuracy: tr - 0.97, val - 0.97.\n",
      "Not enough evidence to support a difference across group, after 10 folds (0.0744329741649274)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iteration completed in 986.773s. Remaining configurations 9.  Mean validation loss 0.057 and acc 0.98\n",
      "---------------------------------------------------------------------------\n",
      "More than desired minimum configurations remaining: 9\n",
      "Iterated F-race takes 4694.53 seconds to tune roughly 180 configurations\n"
     ]
    }
   ],
   "source": [
    "space_f = {\n",
    "    'lr': (0.0001, 0.01),\n",
    "    'beta_1': (0.85, 0.95),\n",
    "    'beta_2': (0.99, 0.9999),\n",
    "    'l2': (0.0005, 0.02),\n",
    "    'filters': (16, 64),\n",
    "    'kernel_sz': (2,5),\n",
    "    'dense_sz': (25, 150),\n",
    "    'activs': (0, 1),\n",
    "    'padding': (0, 1)\n",
    "}\n",
    "INT_IDX = 4\n",
    "BIN_IDX = 7\n",
    "\n",
    "space_ranges = {\n",
    "    'lr': space_f['lr'][1] - space_f['lr'][0],\n",
    "    'beta_1': space_f['beta_1'][1] - space_f['beta_1'][0],\n",
    "    'beta_2': space_f['beta_2'][1] - space_f['beta_2'][0],\n",
    "    'l2': space_f['l2'][1] - space_f['l2'][0],\n",
    "    'filters': space_f['filters'][1] - space_f['filters'][0] + 1,\n",
    "    'kernel_sz': space_f['kernel_sz'][1] - space_f['kernel_sz'][0] + 1,\n",
    "    'dense_sz': space_f['dense_sz'][1] - space_f['dense_sz'][0] + 1,\n",
    "    'activs': space_f['activs'][1] - space_f['activs'][0] + 1,\n",
    "    'padding': space_f['padding'][1] - space_f['padding'][0] + 1\n",
    "}\n",
    "\n",
    "discrete_dists = {\n",
    "    # 'filters': np.repeat(1 / space_ranges['filters'], space_ranges['filters']),\n",
    "    # 'kernel_sz': np.repeat(1 / space_ranges['kernel_sz'], space_ranges['kernel_sz']),\n",
    "    # 'dense_sz': np.repeat(1 / space_ranges['dense_sz'], space_ranges['dense_sz']),\n",
    "    'activs': np.repeat(1 / space_ranges['activs'], space_ranges['activs']),\n",
    "    'padding': np.repeat(1 / space_ranges['padding'], space_ranges['padding'])\n",
    "}\n",
    "\n",
    "N = 30\n",
    "D = len(space_f)\n",
    "# sample N configurations\n",
    "model_configs = {}\n",
    "loss_history = {}\n",
    "acc_history = {}\n",
    "\n",
    "SAMPLER = Sobol(d=D)\n",
    "_ = SAMPLER.fast_forward(32)\n",
    "\n",
    "points = SAMPLER.random_base2(m=5)\n",
    "for i in range(N):\n",
    "    sample = points[i]\n",
    "    params = [p * (ub - lb) + lb for p, (lb, ub) in zip(sample, space_f.values())]\n",
    "    params = [p if idx < INT_IDX else round(p) for idx, p in enumerate(params)]\n",
    "    model_configs[i] = params\n",
    "IDX = N\n",
    "\n",
    "L = 6\n",
    "\n",
    "start = time.time()\n",
    "for l in range(1, L+1):\n",
    "    print(\"#\" * 100)\n",
    "    print(f\"Iteration {l} of Iterated F-race.\")\n",
    "    model_configs, loss_history, acc_history = train_models_frace(model_configs, loss_history, acc_history,\n",
    "                                                                  x_data=train_X, y_data=train_Y, batch_sz=1024,\n",
    "                                                                  epochs=10, pat=3, verbose=-1)\n",
    "    \n",
    "    N_e = len(model_configs)\n",
    "\n",
    "    scores = {idx: np.mean(loss_history[idx]) for idx in model_configs}\n",
    "    ranks = list(dict(sorted(scores.items(), key=lambda x: x[1])).keys())\n",
    "    if N_e > N_MIN:\n",
    "        print(f\"More than desired minimum configurations remaining: {N_e}\")\n",
    "        for i in range(N_MIN, N_e):\n",
    "            idx = ranks[i]\n",
    "            model_configs.pop(idx)\n",
    "            loss_history.pop(idx)\n",
    "            acc_history.pop(idx)\n",
    "            scores.pop(idx)\n",
    "        ranks = ranks[:N_MIN]\n",
    "    if l == L:\n",
    "        break\n",
    "        \n",
    "    # Select elite to sample (new one for every new sample? Or once for all new samples?)\n",
    "    weights = {idx: (N_e - ranks.index(idx)) / (N_e * (N_e + 1) / 2) for idx in model_configs}\n",
    "    cum_weights = np.cumsum(list(weights.values()))\n",
    "    r = np.random.rand()\n",
    "    \n",
    "    sample_idx = list(weights.keys())[np.searchsorted(cum_weights, r)]\n",
    "    sample_config = model_configs[sample_idx]\n",
    "    \n",
    "    # Keep only sampled elite config, and global best config\n",
    "    for key in [k for k in model_configs if k != sample_idx and k != ranks[0]]:\n",
    "        model_configs.pop(key)\n",
    "        loss_history.pop(key)\n",
    "        acc_history.pop(key)\n",
    "    \n",
    "    # Recompute sampling parameters\n",
    "    N_e = len(model_configs)\n",
    "    sig_mult = pow(1 / N, l / D)\n",
    "    \n",
    "    for pi, (key, param) in enumerate(zip(space_f, sample_config)):\n",
    "        if pi < BIN_IDX:\n",
    "            continue\n",
    "        discrete_dists[key] = np.array([pr * (1 - l / L) + int(idx + space_f[key][0] == param) * l / L\n",
    "                                        for idx, pr in enumerate(discrete_dists[key])])\n",
    "    \n",
    "    for i in range(N_e, N):\n",
    "        # Sample around parameters of elite candidate\n",
    "        new_conf = []\n",
    "        for pi, (key, param) in enumerate(zip(space_f, sample_config)):\n",
    "            bounds = space_f[key]\n",
    "            if pi < BIN_IDX:             \n",
    "                sample = np.random.normal(param, space_ranges[key] * sig_mult)\n",
    "                sample = bounds[0] if sample < bounds[0] else bounds[1] if sample > bounds[1] else sample\n",
    "                if pi >= INT_IDX:\n",
    "                    sample = round(sample)\n",
    "            else:\n",
    "                cum_weights = np.cumsum(discrete_dists[key])\n",
    "                r = np.random.rand()\n",
    "\n",
    "                sample_idx = np.searchsorted(cum_weights, r)\n",
    "                sample = list(range(bounds[0], bounds[1]+1))[sample_idx]\n",
    "            new_conf.append(sample)\n",
    "\n",
    "        # Assign IDX to new candidate, and increment IDX\n",
    "        model_configs[IDX] = new_conf\n",
    "        IDX += 1\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "ranks_records.append(ranks)\n",
    "scores_records.append(scores)\n",
    "configs_records.append(model_configs)\n",
    "print('Iterated F-race takes {:.2f} seconds to tune roughly {} configurations'.format(end - start, N * L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78, 164, 171, 167, 153]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks[:N_MIN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00083729   0.86430548   0.9999       0.01922641  54.\n",
      "   4.         150.           0.           1.        ]\n",
      "[0.02425255 0.02148129 0.05057108 0.0266208  0.03987708 0.0379369\n",
      " 0.03530578 0.05130914 0.02703423 0.02511398]\n",
      "Mean 0.03395028319209814\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "# lr, beta_1, beta_2, l2, filters, kernel_sz, dense_sz, activs, padding\n",
    "print(np.round(model_configs[78], 10))\n",
    "print(np.round(loss_history[78], 10))\n",
    "print(f\"Mean {np.mean(loss_history[78])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00050726  0.87423688  0.99807679  0.01167818 43.          4.\n",
      " 73.          0.          1.        ]\n",
      "[0.02864042 0.03718297 0.03719274 0.02931892 0.04071383 0.09852061\n",
      " 0.09436682 0.05758991 0.03873058 0.02171141]\n",
      "Mean 0.04839682150632143\n"
     ]
    }
   ],
   "source": [
    "print(np.round(model_configs[164], 10))\n",
    "print(np.round(loss_history[164], 10))\n",
    "print(f\"Mean {np.mean(loss_history[164])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00050747  0.89036671  0.99870419  0.01465391 30.          5.\n",
      " 50.          0.          1.        ]\n",
      "[0.03659145 0.03401849 0.05330357 0.04757323 0.03280251 0.05103321\n",
      " 0.08445738 0.06837597 0.03831699 0.08287126]\n",
      "Mean 0.05293440669775009\n"
     ]
    }
   ],
   "source": [
    "print(np.round(model_configs[171], 10))\n",
    "print(np.round(loss_history[171], 10))\n",
    "print(f\"Mean {np.mean(loss_history[171])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate / Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(scores, measure = \"Accuracy\"):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title('%s: mean=%.3f std=%.3f, n=%d' % (measure, np.mean(scores.values), np.std(scores.values), len(scores)))\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.violinplot(data=scores, cut=0)\n",
    "    # plt.gca().get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0008, beta_1 0.8979, beta_2 0.9948, l2 0.0199, filters 58, kernel_sz 3,  dense_sz 141, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.111s.  Loss: tr - 0.026, val - 0.021.  Accuracy: tr - 0.992, val - 0.991.\n",
      "Fold 1 trained 10 epochs in 6.774s.  Loss: tr - 0.049, val - 0.042.  Accuracy: tr - 0.984, val - 0.988.\n",
      "Fold 2 trained 10 epochs in 6.826s.  Loss: tr - 0.047, val - 0.046.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 3 trained 10 epochs in 6.78s.  Loss: tr - 0.031, val - 0.031.  Accuracy: tr - 0.991, val - 0.988.\n",
      "Fold 4 trained 10 epochs in 6.796s.  Loss: tr - 0.043, val - 0.036.  Accuracy: tr - 0.986, val - 0.988.\n",
      "Fold 5 trained 10 epochs in 6.795s.  Loss: tr - 0.025, val - 0.029.  Accuracy: tr - 0.993, val - 0.992.\n",
      "Fold 6 trained 10 epochs in 6.82s.  Loss: tr - 0.101, val - 0.079.  Accuracy: tr - 0.967, val - 0.974.\n",
      "Fold 7 trained 10 epochs in 7.125s.  Loss: tr - 0.031, val - 0.031.  Accuracy: tr - 0.99, val - 0.99.\n",
      "Fold 8 trained 10 epochs in 6.814s.  Loss: tr - 0.136, val - 0.105.  Accuracy: tr - 0.952, val - 0.964.\n",
      "Fold 9 trained 10 epochs in 6.848s.  Loss: tr - 0.028, val - 0.025.  Accuracy: tr - 0.992, val - 0.992.\n",
      "Cross-validation completed in 68.693s. Mean validation loss 0.045 and acc 0.985\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bayes_losses, bayes_accs, bayes_ratios = train_model(list(bayes_opt_params), return_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_accs = pd.DataFrame({'Bayes': bayes_accs})\n",
    "overall_losses = pd.DataFrame({'Bayes': bayes_losses})\n",
    "overall_ratios = pd.DataFrame({'Bayes': bayes_ratios})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0008, beta_1 0.8643, beta_2 0.9999, l2 0.0192, filters 54, kernel_sz 4,  dense_sz 150, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 8.025s.  Loss: tr - 0.082, val - 0.061.  Accuracy: tr - 0.973, val - 0.983.\n",
      "Fold 1 trained 10 epochs in 8.087s.  Loss: tr - 0.037, val - 0.031.  Accuracy: tr - 0.989, val - 0.992.\n",
      "Fold 2 trained 10 epochs in 8.012s.  Loss: tr - 0.053, val - 0.05.  Accuracy: tr - 0.982, val - 0.986.\n",
      "Fold 3 trained 10 epochs in 8.041s.  Loss: tr - 0.052, val - 0.048.  Accuracy: tr - 0.982, val - 0.985.\n",
      "Fold 4 trained 10 epochs in 8.316s.  Loss: tr - 0.035, val - 0.028.  Accuracy: tr - 0.989, val - 0.991.\n",
      "Fold 5 trained 10 epochs in 8.025s.  Loss: tr - 0.035, val - 0.038.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Fold 6 trained 10 epochs in 8.118s.  Loss: tr - 0.028, val - 0.031.  Accuracy: tr - 0.991, val - 0.992.\n",
      "Fold 7 trained 10 epochs in 8.146s.  Loss: tr - 0.044, val - 0.054.  Accuracy: tr - 0.985, val - 0.983.\n",
      "Fold 8 trained 10 epochs in 8.051s.  Loss: tr - 0.029, val - 0.026.  Accuracy: tr - 0.991, val - 0.992.\n",
      "Fold 9 trained 10 epochs in 8.1s.  Loss: tr - 0.024, val - 0.021.  Accuracy: tr - 0.993, val - 0.994.\n",
      "Cross-validation completed in 80.926s. Mean validation loss 0.039 and acc 0.989\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0005, beta_1 0.8742, beta_2 0.9981, l2 0.0117, filters 43, kernel_sz 4,  dense_sz 73, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 7.0s.  Loss: tr - 0.056, val - 0.042.  Accuracy: tr - 0.981, val - 0.987.\n",
      "Fold 1 trained 10 epochs in 7.281s.  Loss: tr - 0.026, val - 0.025.  Accuracy: tr - 0.992, val - 0.991.\n",
      "Fold 2 trained 10 epochs in 6.969s.  Loss: tr - 0.037, val - 0.039.  Accuracy: tr - 0.987, val - 0.989.\n",
      "Fold 3 trained 10 epochs in 6.978s.  Loss: tr - 0.041, val - 0.043.  Accuracy: tr - 0.987, val - 0.985.\n",
      "Fold 4 trained 10 epochs in 7.007s.  Loss: tr - 0.05, val - 0.051.  Accuracy: tr - 0.985, val - 0.984.\n",
      "Fold 5 trained 10 epochs in 7.02s.  Loss: tr - 0.034, val - 0.036.  Accuracy: tr - 0.989, val - 0.99.\n",
      "Fold 6 trained 10 epochs in 6.978s.  Loss: tr - 0.033, val - 0.043.  Accuracy: tr - 0.99, val - 0.988.\n",
      "Fold 7 trained 10 epochs in 6.998s.  Loss: tr - 0.062, val - 0.052.  Accuracy: tr - 0.979, val - 0.982.\n",
      "Fold 8 trained 10 epochs in 7.299s.  Loss: tr - 0.063, val - 0.056.  Accuracy: tr - 0.98, val - 0.981.\n",
      "Fold 9 trained 10 epochs in 7.089s.  Loss: tr - 0.039, val - 0.037.  Accuracy: tr - 0.988, val - 0.988.\n",
      "Cross-validation completed in 70.623s. Mean validation loss 0.042 and acc 0.987\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0005, beta_1 0.8904, beta_2 0.9987, l2 0.0147, filters 30, kernel_sz 5,  dense_sz 50, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 4.906s.  Loss: tr - 0.084, val - 0.062.  Accuracy: tr - 0.969, val - 0.978.\n",
      "Fold 1 trained 10 epochs in 4.956s.  Loss: tr - 0.047, val - 0.038.  Accuracy: tr - 0.984, val - 0.988.\n",
      "Fold 2 trained 10 epochs in 4.918s.  Loss: tr - 0.265, val - 0.205.  Accuracy: tr - 0.891, val - 0.919.\n",
      "Fold 3 trained 10 epochs in 4.913s.  Loss: tr - 0.066, val - 0.061.  Accuracy: tr - 0.977, val - 0.98.\n",
      "Fold 4 trained 10 epochs in 4.89s.  Loss: tr - 0.103, val - 0.084.  Accuracy: tr - 0.96, val - 0.971.\n",
      "Fold 5 trained 10 epochs in 4.93s.  Loss: tr - 0.098, val - 0.085.  Accuracy: tr - 0.966, val - 0.97.\n",
      "Fold 6 trained 10 epochs in 5.274s.  Loss: tr - 0.145, val - 0.114.  Accuracy: tr - 0.946, val - 0.958.\n",
      "Fold 7 trained 10 epochs in 4.926s.  Loss: tr - 0.062, val - 0.062.  Accuracy: tr - 0.978, val - 0.978.\n",
      "Fold 8 trained 10 epochs in 4.967s.  Loss: tr - 0.103, val - 0.081.  Accuracy: tr - 0.961, val - 0.969.\n",
      "Fold 9 trained 10 epochs in 4.935s.  Loss: tr - 0.383, val - 0.351.  Accuracy: tr - 0.825, val - 0.848.\n",
      "Cross-validation completed in 49.619s. Mean validation loss 0.114 and acc 0.956\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0003, beta_1 0.8544, beta_2 0.9984, l2 0.0174, filters 64, kernel_sz 5,  dense_sz 74, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 6.042s.  Loss: tr - 0.055, val - 0.044.  Accuracy: tr - 0.981, val - 0.985.\n",
      "Fold 1 trained 10 epochs in 6.025s.  Loss: tr - 0.059, val - 0.044.  Accuracy: tr - 0.98, val - 0.99.\n",
      "Fold 2 trained 10 epochs in 6.314s.  Loss: tr - 0.055, val - 0.056.  Accuracy: tr - 0.981, val - 0.985.\n",
      "Fold 3 trained 10 epochs in 6.056s.  Loss: tr - 0.035, val - 0.039.  Accuracy: tr - 0.989, val - 0.986.\n",
      "Fold 4 trained 10 epochs in 6.024s.  Loss: tr - 0.069, val - 0.059.  Accuracy: tr - 0.976, val - 0.979.\n",
      "Fold 5 trained 10 epochs in 6.027s.  Loss: tr - 0.098, val - 0.082.  Accuracy: tr - 0.965, val - 0.969.\n",
      "Fold 6 trained 10 epochs in 6.09s.  Loss: tr - 0.059, val - 0.061.  Accuracy: tr - 0.98, val - 0.977.\n",
      "Fold 7 trained 10 epochs in 6.077s.  Loss: tr - 0.035, val - 0.034.  Accuracy: tr - 0.989, val - 0.989.\n",
      "Fold 8 trained 10 epochs in 6.325s.  Loss: tr - 0.05, val - 0.043.  Accuracy: tr - 0.984, val - 0.986.\n",
      "Fold 9 trained 10 epochs in 6.072s.  Loss: tr - 0.065, val - 0.058.  Accuracy: tr - 0.978, val - 0.98.\n",
      "Cross-validation completed in 61.056s. Mean validation loss 0.052 and acc 0.983\n",
      "---------------------------------------------------------------------------\n",
      "lr 0.0008, beta_1 0.8793, beta_2 0.9971, l2 0.0126, filters 41, kernel_sz 5,  dense_sz 83, activs relu, padding 1\n",
      "Fold 0 trained 10 epochs in 4.708s.  Loss: tr - 0.084, val - 0.064.  Accuracy: tr - 0.971, val - 0.977.\n",
      "Fold 1 trained 10 epochs in 4.771s.  Loss: tr - 0.088, val - 0.068.  Accuracy: tr - 0.967, val - 0.976.\n",
      "Fold 2 trained 10 epochs in 4.776s.  Loss: tr - 0.044, val - 0.042.  Accuracy: tr - 0.985, val - 0.988.\n",
      "Fold 3 trained 10 epochs in 4.766s.  Loss: tr - 0.039, val - 0.035.  Accuracy: tr - 0.989, val - 0.989.\n",
      "Fold 4 trained 10 epochs in 4.702s.  Loss: tr - 0.051, val - 0.043.  Accuracy: tr - 0.983, val - 0.985.\n",
      "Fold 5 trained 10 epochs in 5.052s.  Loss: tr - 0.093, val - 0.076.  Accuracy: tr - 0.964, val - 0.975.\n",
      "Fold 6 trained 10 epochs in 4.769s.  Loss: tr - 0.07, val - 0.066.  Accuracy: tr - 0.975, val - 0.973.\n",
      "Fold 7 trained 10 epochs in 4.781s.  Loss: tr - 0.096, val - 0.077.  Accuracy: tr - 0.963, val - 0.971.\n",
      "Fold 8 trained 10 epochs in 4.723s.  Loss: tr - 0.047, val - 0.041.  Accuracy: tr - 0.985, val - 0.986.\n",
      "Fold 9 trained 10 epochs in 4.784s.  Loss: tr - 0.1, val - 0.085.  Accuracy: tr - 0.964, val - 0.967.\n",
      "Cross-validation completed in 47.838s. Mean validation loss 0.06 and acc 0.979\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in ranks:\n",
    "    args = model_configs[i]\n",
    "    losses, accs, ratios = train_model(args, return_all=True)\n",
    "    overall_losses[f\"F-race {i}\"] = losses\n",
    "    overall_accs[f\"F-race {i}\"] = accs\n",
    "    overall_ratios[f\"F-race {i}\"] = ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmgAAAFFCAYAAABYNRE8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAB2kUlEQVR4nO3de3xT9f0/8NfJvUna9E7v99Jyp1Ap95tAlWtBZepEmU6tq46vl80LQwZjblPcGAJjeMXLb26KFBBFECYTlQqICspVKJR76T3N/ZzP7480gdLSpm2Sc5K+n48Hm0lOznmfpDl55/25cYwxBkIIIYQQIhkysQMghBBCCCHNUYJGCCGEECIxlKARQgghhEgMJWiEEEIIIRJDCRohhBBCiMRQgkYIIYQQIjGUoBFCSIArKytDTk6O2GEQQryIEjRCrvGvf/0LvXr1wptvvil2KKSLGhsb8dRTT2HQoEEoKCjAn//8Z/A8f93tBUHAmjVrMGHCBPTv3x+33347fvzxx2bb7Nu3D7fffjvy8vIwfPhwPPXUU6irq2t1fyUlJcjJyUFZWVmn4vdX4vXZZ5/h5ptvRr9+/TBr1ix8//33bW7vyeu6bt06jB8/Hv3798ecOXNw6tQp92MffPABcnJyWvy7//77fXJ+nli1ahVuvfVW9O3bF3PmzGl1m7bOiRBvowSNkGts2LAB99xzD0pLS0WLgTEGu90u2vGDxeLFi3HgwAG88cYbWLZsGT788EP84x//uO72b7/9Nt544w0sWLAAmzZtQl5eHu677z7U19cDAIxGI4qLizFgwABs2LABq1atwo8//ojnnnuuxb5KS0thNpt9dm7e8tNPP+Hhhx9GUVER1q9fj0GDBuH++++/btIJtP+6fvXVV1i4cCEeeughvP/++4iKisIDDzwAh8MBAJg8eTJ27drl/vfZZ59Br9djwoQJPj/f6+F5HtOnT8fkyZNbfby9cyLE6xghxK28vJwNGTKEWSwWNnz4cHb8+PFmj1++fJk99thjLD8/nw0cOJDddddd7MKFC4wxxux2O/vrX//KRo0axfr168emTp3KvvzyS8YYY08++SR78sknm+3rrrvuYsuXL3ff7tmzJ3vvvffYXXfdxfr06cN27tzJ9u3bx+666y42ePBgVlBQwB599FFWVVXVbD+ff/45mzVrFuvbty8bMWIEe+GFFxhjjBUWFrJ33nmn2bavvvoqmz59ukevxV133cVeeOEF9uSTT7IBAwawSZMmsW+//Zb98MMPbObMmWzgwIHsscceYxaLxf0ck8nEfv/737OCggI2ePBg9uCDD7KzZ8+6H//000/ZrbfeygYOHMhGjBjBFi5cyBobG92PL1++nN11113sjTfeYMOHD2cFBQXu8+mo2tpa1qtXL/bVV1+573vvvffYsGHDGM/zrT7nZz/7GfvHP/7hvi0IAhs5ciR7++23GWOMfffdd6xnz57MaDS6t3nzzTfZlClTmu3nwoULbOzYsezs2bOsZ8+ebPfu3deN02KxsPnz57OhQ4eyfv36scLCQrZt2zZWUVHBevbs2ezfunXrGGOMHT58mM2cOZP17duX3X777ey9995jPXv27PiLxBh77rnn2M9//vNm5zx27Fj21ltvtbq9J69rSUkJ++1vf+t+vLGxkfXv35/t2LGj1X1u27aN9e3bl9XV1Xkctzf/Vlrb77U6ek6EdBVV0Ai5SmlpKSZOnAi1Wo3CwsIWVbSHH34YFy5cwJo1a/DBBx9g+vTp7qadl156CaWlpXj22Wfx4Ycf4tFHH4VM1rGP2IoVKzBnzhx8/PHH6Nu3L0wmE+644w6sW7cOL7/8Mi5evIhFixa5tz9+/DiKi4sxduxYlJaWYtWqVUhMTAQAzJw5Exs3bmy2/40bN2LGjBk4c+aMR01v7777LgYMGID169cjJycHTz31FJ5//nnMnz8fr732Gr788ku8//777u0XLlyI06dP4+WXX8Z//vMfRERE4KGHHoIgCAAAq9WKhx56CBs3bsSyZcuwZ88erFixotkxf/jhBxw+fBhvvvkmFi9ejNdffx07d+50Pz5lyhTk5eVd95/rnH/44QdwHIcbbrjB/dxhw4ahqqoKZ86cafV8rVYr1Gq1+zbHcVCpVPjuu+8AABkZGTAYDPjggw/A8zyqq6uxbds2jBgxotl+5s+fjwcffBAJCQltvr4A8Oabb+LgwYP45z//ic2bN+Ppp5+GTqdDfHw8XnrpJQBwV5omT54MnufxyCOPICkpCevXr8fdd9+NZcuWNdvnxo0b23yNpkyZ4t72+++/x9ChQ5ud89ChQ93nfC1PXtdr96nVatG/f//r7nPDhg0YN24cwsLC2n29ro3FG38rnujoORHSVQqxAyBEKhhj2LhxI/7whz8AcDbDPPHEE+5Ea/fu3fjhhx+wY8cOREdHAwDS09MBABaLBa+99hqWLVuGG2+8EQCQkpLS4Rh+9rOfYdKkSe7bI0eObPb4U089hdtvvx08z0Mul+Pll1/G2LFj8cgjj7i36d+/PwBgxowZ+Pvf/46KigokJyfj+PHjOHr0KKZOnQrGGNLT0xESEtJmPEOGDMEdd9wBALjvvvswe/ZsPP744xg8eDAAYNKkSdizZw9+/vOf48yZM/j444/x1VdfQa/XA3A2hd1www34/vvvMXDgwGbNR8nJyXjkkUfwwgsv4Le//a37fpVKhUWLFkGlUiEzMxMFBQXYs2cPxowZAwBYs2ZNm81KUVFRAICqqioYDAbI5XL3Y5GRke7HWnt/hg4din/9618YN24ckpOT8Z///AdnzpxBWloaAECv1+P111/HQw89hD/96U/geR7jxo3Db37zG/c+3n33XTgcDtx+++1tvrYuFy5cQJ8+fdzvW3Jysvsxg8EAAIiJiXHft3PnTly6dAnr1q1DaGgosrKycOjQIfzzn/90bzN+/HgMGDDgusdUKK5c+qurq92vi0tERASOHDnS6nM9eV1b22dkZCSqq6tb7K+urg6fffZZiyTTE976W/FER86JEG+gBI2QJnv37oXZbEZBQQEAYPDgwWCMoaysDMOGDcOxY8eQnp7uTs6udurUKdhstmZVhc7o3bt3s9sXL17Eiy++iH379qG6uhqMMTgcDly+fBk9evTAsWPHMH369Fb3FRcXh4KCAmzYsAEPP/wwNmzYgGHDhiE2NhYAsGXLlnbjyc7Odv+368spKyvLfV9UVBROnjwJwFnNs9vtGDVqVLN9WCwWVFRUYODAgfjpp5/wt7/9DQcPHkRdXR14nm/RuTwtLQ0qlcp9OyYmBlVVVe7brgphexhjLe7jOK7N5zz88MM4e/Ysbr75ZnAch8GDBzc7H5PJhAULFuCmm25CUVER6urq8Pzzz+PPf/4zfve73+HcuXNYsWIF3n33XY9iBJyJ9C9+8QscOnQII0eOxE033dTi7+BqJ0+eRHp6OkJDQ933XZuM6fV6d5LcntZep45u397r2paPPvoIOp0Oo0eP7vBzvfW3QogUUYJGSJPS0lJUVVWhX79+7vsEQXAnNoyx634Rtfclx3Gcu5nPpbVBABqNptntp59+Gna7HUuWLEFsbCzOnz+P++67z/3ctmICgKKiIqxatQolJSXuZteOuLrS4jrOtfe5zt1kMkGn0+GDDz5osR9XpeJXv/oVcnJysHTpUkRGRmL//v145plnmm2rVCqb3b76GICz2ercuXPXjXnRokWYPn06oqOj3Umgq9rj+vK+XuVEp9Nh+fLlsFgsaGhoQExMDG6//Xb3SMrNmzejsbGxWcwLFizAz3/+czz22GP48ccfcfny5WZVUACYO3cuZs6c2epggv79+2P79u347LPP8Pnnn+P222/HY489hrlz5173HK99z6/9+9u4cSMWLlx43ecnJCRg8+bN7tfi2ipQTU1Ni2qRiyeva2uVperqanfF+WobNmzAlClTWrzvnvDW34onOnJOhHgDJWiEwNn36JNPPsGLL76Inj17uu8/efIknnrqKSxcuBDZ2dk4ceIEqqqqWnzBp6amQqVSYc+ePe4mzqtFRkbi8OHD7ts2mw0nT55s0XfpWvv378df//pXDBs2DABw6NChZo9nZ2djz549uOeee1p9/qRJk7Bo0SK8+uqrqK2txcSJE9t+IbogJycHRqMRDocDmZmZLR6vrq5GeXk5XnrpJfdrvG3btg4fx9Nmq969e4Mxhr1797qrort370ZUVBSSkpLaPIZGo4FGo8G5c+dw4MABPPjggwAAs9ncol+hTCYDYwyMMQwdOrRFv6Zp06ZhyZIlLZqrrxYeHo6ioiIUFRXh5Zdfxrp16zB37lx3Mnx1MpSeno6TJ0/CaDS6q2QHDhxotr+ONHH2798fZWVlePjhh933lZWV4d577231uZ68rq59zpw5E4Dzdfv+++9b7PP06dOtJune4s0mTk/PiRBvoQSNEACffvopFAoFbr755mZ9a7Kzs7FkyRJs27YN06dPR69evfDrX/8aTzzxBCIjI/HNN9+goKAACQkJuOeee7B48WIwxpCTk4MTJ05Ao9GgoKAAN9xwA9auXYtPP/0U6enpeOWVVzyaRiM5ORnr169HRkYGTp061WKKiF/+8peYNWsWXnrpJUyZMgVmsxkHDx7Ez372MwBASEgIbrrpJixbtgxTpkxx9zm7ePEi7rnnHjz//PPuvk9dlZmZiUmTJmHevHl46qmnkJqaivPnz+Pjjz/Go48+CoPBAIPBgHfffRe/+MUvcODAAfzrX//q8HE8bbYKDw/H1KlTsWTJEvzxj3+EyWTCsmXLcOedd7qTrLfffhvbtm3D2rVrATibaY8dO4Y+ffqgoqICS5YswZAhQzB27FgAwPDhw/H888/jb3/7G2bNmoXa2lr88Y9/xJAhQ6DT6QCgWYLvkpSUhB49erQa5xtvvIEePXqgV69esFqt+OKLL9xVGdcgg88//xz9+/eHXq/HyJEjERUVhd/97nd45JFHcOzYMaxbt67ZPjvSxDl79mzMmDEDa9aswY033oh3330XjY2NmDZtmnubm266CY8//jgmTpzo0ev685//HPfffz/y8/MxYMAArFq1CnFxcS2S1A0bNiAjI8Nrf4PX6kgT57lz51BXV4fKyko0Njbi0KFDUCqV7iZ9T8+JEG+hUZyE4MoosquTM8DZZDJu3Dhs2LABgHOkZnR0NO677z7MnDkTH3zwgbsaMW/ePEydOhULFy7ElClTsHTpUvd+xowZg3vuuQe/+93vMGfOHOTk5KBPnz7txrVkyRKUl5dj6tSpWLZsWYsmyp49e2LlypX49NNPMX36dDz44IM4e/Zss21mzJgBu92OoqIi9312ux0nT570+jxdS5cuxciRI/H000/j5ptvxtNPPw1BEKBWqyGXy/HCCy9g165dmDJlCt59913MmzfPq8e/1sKFC9G7d2/cc889+PWvf43JkyfjoYcecj9eU1ODiooK922Hw4FVq1ZhypQpeOKJJzBs2DCsWLHC3aSYlZWFFStWYNeuXSgqKsKvfvUrpKSkNHuvOyokJASrVq3CjBkzcPfdd8NgMGDBggUAgPj4eBQXF+Opp57CsGHD8OGHH0Iul2PFihU4ffo0ZsyYgTfeeKNLr2NmZiZeeuklrF+/HjNmzMC+ffvw8ssvuwcoAM5KckNDg/t2e6/rsGHD8Pvf/x6rVq3CrFmzcPnyZfzzn/9s0STpGlXcmqeeeuq6E8b6wvLly1FUVIR///vf+OGHH1BUVIQHHnjA/bin50SIt3Csoz1ECSEB5aOPPsJf/vIX/Pe//+3wtB+EiGXOnDkYMmRIsxHKhHQndLUmJEjZbDacOHECr7zyCm677TZKzkjAMJlMqKiooP5dpFujKzYhQerDDz/E1KlTodPp6IuOBBStVovPPvvM3a+PkO6ImjgJIYQQQiSGKmiEEEIIIRJDCRohhBBCiMRQgkYIIYQQIjFBN1FtTU0jBIG61RFCCCFEumQyDhER1x8I41GCVltbi/nz5+PLL7+EwWDA//3f/zWb9PJqa9aswbvvvouamhoUFBTgD3/4A2JiYgAARqMRzz33HD777DM4HA5Mnz4dTz31lHuizzlz5uDbb791346NjcUnn3zSkfOFIDBK0AghhBAS0Dxq4ly8eDGUSiV27dqFpUuXYvHixThy5EiL7UpLS/Hee+9h7dq12L17NyIjI/H444+7H3/uuedQW1uLrVu3YvPmzfjmm2+wevXqZvt49tlnsX//fuzfv7/DyRkhhBBCSDBoN0EzmUzYunUr5s2bB51Oh/z8fEyYMKHFgsAAsGPHDsyePRvJyclQq9UoKSlBWVmZeymVHTt2oLi4GHq9HjExMZgzZ06LNeQIIYQQQrq7dps4y8vLIZfL3Yv3AkBubi7Kyspa3b61adWOHDmC5ORkMMZaPH7u3Dk0NDQgNDQUAPDiiy9i6dKlSE9Px6OPPoqCgoIOnVBUlGcLBBNCCCGESFW7CZrJZIJe3zzp0ev1MJlMLbYdM2YMVq9ejcLCQsTGxroXGbZYLACAsWPHYvXq1Xj++edhMpnw1ltvAQDMZjNCQ0PxxBNPIDMzEyqVCps3b0ZxcTE2bNiAlJQUj0+oqspIfdAIIYQQImkyGddmUandJk6tVguj0djsPqPRCK1W22LbWbNmoaioCPfeey8mTJiAzMxM6HQ6xMXFAQCeeeYZhIWF4eabb8Zdd92Fm266CUqlElFRUQCAAQMGQK/XQ6VSYebMmRg0aBB27tzZoRMmhBBCCAl07SZoaWlp4Hke5eXl7vsOHz6MrKysFttyHIeSkhJs374dX3zxBcaNGwdBEJCdnQ0AMBgM+Mtf/oJdu3Zh27ZtCA0NRd++fSGXy1s9NsdxrTaZEkIIIYQEM48qaBMnTsTy5cthMpmwb98+bN++HdOmTWuxbV1dHcrLy8EYQ0VFBRYsWIC5c+fCYDAAACoqKlBdXQ1BELB3716sWrUK8+bNAwDU19fj888/h9VqhcPhwMaNG7F3716MHDnSy6dMCCGEECJtHs2DtnDhQsyfPx/Dhw+HwWDAggULkJubCwDIy8vDyy+/jPz8fNTV1aG4uBjnz59HWFgYbrvtNjz88MPu/fz4449YsmQJ6uvrkZSUhKeffhrDhg0DADgcDixbtgwnTpyAXC5HRkYGVq5ciYyMDB+cNiGEEEKIdHEsyNoQaZAA6QiLxYJ9+76Gw+Hw6XF++ukYACAzM9unxwGA8PAI9O8/EBzH+fxYhBBCOqe9QQKUoJFuSxAE/H3Z8zhw8HuxQ/G6u+76BcaPnyh2GIQQQq6jvQQt6NbiJMRTH35YigMHv0dhhh59ojU+PdZ/DtUCAGb3CvfpcRiAD4834F//ehNpaenIyGg5mIcQQoj0UYJGuqUjRw5hw4Z16BejwdAErc+bA+Uy5/7D1K2PWPamWT3D8M9va/CPf/wdixb9GVrt9RfjJYQQIk0ercVJSDAxGo345z9fQmSIHFOzQ4Our1aIUoZbckJRU12FN998VexwCCGEdAIlaKTbeeedN1BfV4dbeoZBLQ/Oj0BymApjU3T4+uvdKCv7UuxwCCGEdFBwfjsRch379+9DWdmXGJ2sRUKoUuxwfGpEsg5JoSq8/fbrqK+vFzscQgghHUAJGuk2LBYL3n7rVfTQKTEyOfj7Zck5DtOzQ2ExmfCf/7wjdjiEEEI6gBI00m189NFG1NTWYkqmHgpZcPU7u55YnQLDEkPw5Zefu+diI4QQIn2UoJFuoaamBp9s+RB9YzRIMajEDsevRqXooFfL8Z9/v0Nr2xJCSICgBI10Cx99tBE8z2N8avA3bV5LLZdhdJIWx44fxY8/HhQ7HEIIIR6gBI0EvYaGevxv53b0j1UjMqR7Tv03KC4EYWoFPvpog9ihEEII8QAlaCTo7dq1E3aHA8MSu1/1zEUh43BDvAaHDv2Ic+fOih0OIYSQdlCCRoIaYwyff/5fJIep0EPXPatnLnk9QiDjOOzatVPsUAghhLSDErQA8cUX/8MXX/xP7DACztmzFbhw4QL6x6rFDkV0epUMmeEq7NnzFQ0WIIQQieveJQUfMZvNmP+736C+rtZr+xQEAQDw+utrvLZPAADH4bZbb0dh4RTv7lciDhz4DgCQE0kJGgDkRKnw4fEqXLhwDvHxiWKHQwgh5DooQfOBU6dOoramGgpDGmQKrVf2aa8rBwAoDGle2Z+Lo/4Uvv32m6BN0A4f/hHRWqVfFikPBOnhzilGjhw5TAkaIYRIGCVoPnDq1EkAgDp2IGQKjVf2yZsrm/bZ3yv7c2G8DeXlJyEIAmSy4GvxPnXqJNL14iZnjDE0WHlYeYY9503IjwsRbYH2SI0cIUq5+2+UEEKINAXfN7IEVFSchlwZ4rXkzJdkmnBYrRZUV1eJHYrXmc0m1NfXI0Yr7u+QvefNqLYIaLQzbD7egL3nzaLFwnEcokNkuHD+nGgxEEIIaR8laD5wuuIUoDKIHYZH5OpwAM6kMtjU1NQAAAxqcf/Mj1Rb27ztbwa1LCgTckIICSaUoHmZIAg4f/4cZOrASNBccZ49WyFyJN5nMjUCADQKcf/M7QJr87a/hShk7teGEEKINFGC5mWVlZfAOxyQB0iCxsmVkKu0OB+ETV48zwMA5N1kYXRPyTnO/doQQgiRJkrQvMyV6MhUYSJH0gHKUJw9e0bsKLxOLncODhBozq9meMbcrw0hwY7mkCSBikZxetmlSxcAADJVqMiReE6mCsOlS2fAGBNtdKEvhIQ4pzgx2ylBu5rFwRCi7b7LXhHx7dy5A1u3bAYv+L6SW1Pr7Iu6aeMHPj8W4ByIM3LUOEyePC2orqfE/yhB87KLFy9CplABcpXYoXhMptLDUmOG0diA0NAAqvy1Izw8HADQYKPmvKvV23iEx0SKHQbphgRBwPvv/wtbtmxGrFyBMJnvK7n1DgcAIKy2zufHAgATE7Bu3bs4f/4s5s69HwoFfc2SzqG/HC+rrLwITqn36i8nxhgEuxkQ7LDVHIcyPNOr+5cp9QCc/eeCKUHTanUI0WhQbaEE7Wo1VoY+MbFih0G6GZvNhpdfXoV9+75GH7UGI0N0kPmhwrShoRYAMFHvn1YNxhj2Wsz48svPUV1dhYcffhRaqliTTqA+aF5WefkyOC+tHuBirz0OZjeC8VZYL+yFvfa4V/fPKZ3xVlVd9up+xcZxHBITk3Gp0SF2KJJhtguotziQkJAkdiikGzEaG/DCC0uwb9/XGB6iwyg/JWdi4DgON4RoMV6rx9Ejh/DcHxfStDakUyhB8yLGGGqqq7yeoDkazrV5u6tkSuevu6qq4LuIpKZl4EIjD54GCgAAzhntAIDU1DRxAyHdxuXLlXjujwtRfuInTNKFYoBGvJU0/ClHrcEUXSguX7yAJX9YgDNngm8qI+JblKB5kcVihs1mhUzp3QQNzNH27a6SKcHJFKht6kwbTDIzs2HjBVw0UhUNAE7X28FxHDIyssQOhXQDZ8+ewR+XPIuaykuYqgtDpkotdkh+laRUoUgXBruxAX967vc4fvyo2CGRAEIJmhe5EhwuAJZ4uhrHcZApQ4IyQcvN7QUAOFFrEzkSaThZa0dqSiq0Wi//iCDkGidPnsCfn/s9HI1GzNCHIUGpFDskUUQpFCjShUFlt2PpC3/EDz8cEDskEiAoQfOi+vp6AIGXoAEAZGp3/MEkPDwCiYlJOF5DCZrZIeBMgw19+g4QOxQS5I4fP4oXnv8D5DYbinRhiJJ37/FoYXI5ivRhCGUMy5Y9j2+//UbskEgA8ChBq62tRUlJCfLy8jB27FiUlpZed9s1a9Zg/PjxyMvLQ3FxMSorK92PGY1GPPPMMxg+fDiGDBmCJUuWwOG40vTUkeNIUUNDAwCAkwdeGZ+Tq1Ff759h6P42YMAgnK63w+wQxA5FVMerbRAY0L//QLFDIUHs+PGjeHHpc1A7eMzQhyKMJkUGAGhlMkzXhSGKk2Hlir9Skkba5VGCtnjxYiiVSuzatQtLly7F4sWLceTIkRbblZaW4r333sPatWuxe/duREZG4vHHH3c//txzz6G2thZbt27F5s2b8c0332D16tUdPo5UGY2uBC1w5kBz4eQqGI1GscPwiby8wRAYw1GRFykX2+EqC0L1ocjMzBY7FBKkTp06ib+++GdoeAEz9KHQ+2Ges0CikckwVReKKJkcK1f+jZo7SZvaTdBMJhO2bt2KefPmQafTIT8/HxMmTMDGjRtbbLtjxw7Mnj0bycnJUKvVKCkpQVlZGSoqKtyPFxcXQ6/XIyYmBnPmzMG6des6fBypMptNAABOFngJGuQqWCxmsaPwifT0TISHh+PHy903QbPzDMdq7BicPwQyGfVsIN538eJ5vLj0T1A6HJiuD4WOkrNWqWUyTNGFIpzj8NJLL+LkyRNih0Qkqt2OAeXl5ZDL5UhPT3ffl5ubi7Kysla3Z61MZ3DkyBEkJyeDMdbi8XPnzqGhoQEVFRUdOs71REXpO7S9N8lkTU1ossDrb8HJlLDZrIiM1AblOo2jR4/GRx9ugsUhQKPofgnKsRorbLyACRPGISYmcJYhI4Ghvr4ef1/2PHiLGUW6MKqctUMjk2GKLgylxnos//vz+Ovf/obYWJo8mjTXbiZhMpmg1zdPevR6PUwmU4ttx4wZg9WrV6OwsBCxsbFYsWIFOI6DxWIBAIwdOxarV6/G888/D5PJhLfeegsAYDabO3SctlRVGSEI4sx5VVPTAJlcGZBz/HBNSeW5c9XQaAJwkEM7+vYdhI0bN+JwlRUDe4SIHY7fHbjkbN7s0SMVlZUNYodDggjP8/jri39C5aVLmK4PQ3gQ/sDzBZ1Mhsk6PdYb6/H7hYvwzPxFUKsDr/8y6TyZjGuzqNRuKUGr1bbom2Q0Glsdpj9r1iwUFRXh3nvvxYQJE5CZmQmdToe4uDgAwDPPPIOwsDDcfPPNuOuuu3DTTTdBqVQiKiqqQ8eRKpvNCi5QfzlyzrhttuBsBszMzEZUZCQOVlr8fmyrQ4BGo8H06dOh0Whg9fNgBatDwLEaG4YUDAvK6igR14YN63Do8I8YHaJDnKJ7TqXRWRFyBW7U6lFx5jTeeecNscMhEtNuBS0tLQ08z6O8vBxpaWkAgMOHDyMrq+VElxzHoaSkBCUlJQCAn376CStXrkR2trNTssFgwF/+8hf39v/617/Qt29fyOXyDh1HqhwO3p3oBBzOmatfPao2mHAchyEFI/DJlk1otAvQKf3XzGlxMEwqnIT7778fjDF8vnWz344NAIerrXAIDEOGDPPrcUnwO3r0MDZ/WIpclRq56uCrvPtDqlKFQZoQ7Nq1EwMG5GHw4CFih9Rply5dxL59X3t1n+XlJwEAaWnp7WzZMSkpaejTp59X9+lt7SZoWq0WEydOxPLly7FkyRIcOnQI27dvxzvvvNNi27q6OtTU1CA1NRVnzpzBggULMHfuXBgMBgBARUUFdDodwsPD8c0332DVqlV4/vnnO3wcqRIEHkDgNW8CcDfLCkLwTkVRUDAMH3+8CYcuW5Af77/KrEbBYevWrWCMYdu2bQhX+Pdv5IdKKyIiImj0JvEqu92O11/7J0LlCozUitf3Nxjka7SocDjw5tpX0atXn4BdXP2dd97AgQPf+WTfe/bs9ur+VCo1/va3lQgJkW4rnUe92RcuXIj58+dj+PDhMBgMWLBgAXJzcwEAeXl5ePnll5Gfn4+6ujoUFxfj/PnzCAsLw2233YaHH37YvZ8ff/wRS5YsQX19PZKSkvD0009j2LBhHh0nEDDGAjU/gyvwYE7QkpNT0SO2B364XOPXBE2tkMHSaMGmTZuctw3+awYyOwT8VGPDhEkTaPQm8apPP92Ci5cuYoo+DMoA7HcrJXKOw5gQHdY11GLDhg9wxx1zxA6pwy5frsSBA98jMaYfEnv099p+D53YBgDolTHRa/tsNFfhh5+2YPfuLzFu3ASv7dfbPErQwsPDsXLlylYf279/v/u/U1JSsGXLluvup7CwEIWFhZ06TsAI2DW5nYEH4gAHT3Ech/wbhuLjjzb4vZlTLEerrOAZQ35+4DabEOkxm034cFMpUpQqpCilO60QYwyNggAbY/jBakZvlUay17gYhQI5KjV27NiKwsLJiIyMEjukDtm1aycAhtionpB7cSYD1/vlzX2GamOhC4nA//7338BP0IhnnBWKgM3QAAR3ggYA+flDsHnzBhytsiIvLvhHcx6qsiLcEI709EyxQ5EEQRBQUXG6Q30tv//+WwCdW4FBrVYhMTE56D5Xn3/+GcwWM24INYgdSpt+sFlQ19Qq8D9TIxiAvmrpfu4Ha7Q4Ul+DTz/9BLNn3yl2OB4TBAG7du2EQZ8AjUr6zd0cxyEmIhvlp75GRcVpJCeniB1SqyhB8yK5XA6wwGwiZE1xKxTB/SeRkpKGiPBwHKk2B32CZhcYfqq1Y+ToUdS82eTrr3djzZoVnXrupk0fdOp5Tz65ADk5vTr1XClijGHnZ9vRQ6FErMRHbZ6y2VrclnKCFiaXI02pwq7PP8OsWbMD5np85MghVFdXISt5lNiheCw6PB2nzu/FF1/8D7fffpfY4bQqMN79AKFUqtyJTsBhPABAIfELbldxHIcBAwfjy8//C4fAoJAFV2XjaqfqbLDzAgYMyBM7FMmoqDgFTsZBP7SHx88xHawCAGj7dqzJiTkEGL++hIqK00GVoJ07dxbnL5zHqBDpd2R3XNOice1tKcpRqXGysQFHjx5G7959xQ7HI1999QUUciUiDdKsRLVGqdAgPDQRu3d/gdmz75Tkj1hK0LxIpVKBCYE5TQUTnAmaWi3d/iTe0rdvf3z22XacqbcjLTx4z/enGhsUcjlycnqLHYpkXLp0AXKdEqo4zweJmI/WAkCHngM4K00ypRwXL57v0POk7tChHwA4p4cg3pekVEEGDocOHQyIBM1ut2Pv3jJEhKV4tZ+YP8SEZ+Do6Z04fPhHSb7W0ksZA5hGowET+MCsogl2yGSyoK+gAUBubm9wHHCyztb+xgGsvM6OzMxsmp38Khcungen88+XCMdxkOkUuHTpol+O5y+nTp2EVi5HKE167BNKjkOkQoFTTfN/Sd3Bg9/DYjEjKty785T5Q3hYEuRypden8PAWStC8SKNp6tvA28UNpBOY4IBaExJ0nZlbo9XqkJSUgtP1gfc+ecriEHCh0Y6cXKqeuTDGUFlZCbmfEjQAkOkUuHjpgt+O5w+VlRdh4Oirw5fCOQ4XLwbG383evWVQKtQw6OPFDqXD5DIFwvWJ2Lv3a0lOMUWfMi9yrSXKhMCrzDDeBl2ATo7YGVlZOTjb4IDApN8npTPOGR1gDDQ57VUaG42wWa2Q6fxXJZZpFaiqqpLkxb+zjA1GaLrBDzkxqWUyNDY2ih1Gu3iex7fffoPw0CTIAjRpjzKkorHRiGPHjogdSguB+YpKlE7nTHAYH4gJmhU6vfSHR3tLWlo6bLyAajMvdig+cd7orA56e3mUQFZVdRkAIAvxXwVNrlWAdzjQ0FDvt2P6GmNC4M7HHSBkaJr4XOKOHz8Ks9mEiLBksUPptPDQRMg4Ob777huxQ2mBEjQvCg0NAwAwh/8X5O4y3gpDmLTnNPKmlJRUAMDFxsAc1NGeS40OhBsM7r9JciVBk2v92MTZdKzLly/77Zi+ptGEwBYAyUMgszIGjUb6a5t+//234DhZQDZvusjlSoTqYvHdd/vb39jPKEHzorCmBEfwcoLGeDs0Gg2mT5/uHIjgiz5uvMUdf3cQF+e8oFw2B2eCdtnMIz4hSewwJKW6uhqAfytormPV1FT77Zi+FhkVBWMATFcRyIyCEBArCRw8+D1CtTFQyAN7RG94aCLOnz8nuc8pJWheZDCEAwCYw+zV/TLBjkmTJuH+++/HxIkTwQTvJmiMCeAdFoSHh3t1v1KmVmsQbjCgxhKcTZw1FgE9esSJHYak1NRUg5Nx4NT+u+wFY4IWH5+IOocDdqqi+QRjDNWCgIREaf/AMhobUFFxKqCrZy6uc3BNISMVlKB5kUKhgD40DMxh8up+OZkSW7duxZo1a7Bt2zZwMu92cmYOC8BYQPxi86bIqGjUWYKn87aLjWcw2flu9362p66uFnKNwq8jlTmVDOA41NfX+u2YvpaRkQUG4JJD+qOgbU1Nha7Wh0Bomq0VeFgEHhkZWWKH0iZXp/owfeD/ENRqIqBQqHH06GGxQ2mGEjQvi4qMgmD3coImV8JisWDTpk2wWCzg5F5O0JrijYiI9Op+pS48PBJGh/Qv2B1ltDmrguHhESJHIi11dbWAH6tngHMuNEWIArW1tX49ri/l5ORCJpPhtF36CZqVsWatD9YASNBcr2uvXn1EjqRtx44dBcfJoA+JFjuULuM4DvqQGMmN5AysaX8DQExMDM5c8HIWzinavt1Fgt0IAIiOjvHqfqUuNDQU5iBM0ExN56TXh4ocibTU1deBU4swuapKhoaGBv8f10dCQrToldsHJ44cwlDGJD13oprjsHXrVjDGsG3bNuglHKvLCbsNiQmJiI31fDkyMZw4cRy6kEjIZMExYXGoNhoV57+FyWSCVtuxVUN8hSpoXhYdHQvBZvTqEGlFaEKbt7tKsDvn2+luCVpIiBYWe/D1QbM6nM22UrnISEV9fR1kfq6gAc5mzrogauIEgKHDRqCed+C8Q9qDbFQc16z1QSXxBK2W53HBYcfQYdJedFwQBJw6VQ5dSPB0o3CdS0XFKZEjuYISNC/r0SMOjAle7YemDM+COi4firBUqOPyoQz3bt8EwdYAgyG82y0JpFKp4BBY0E1Waxec59Pd3s+2MMbQ2NgITuX/X/ucWg6jMXgqaACQnz8EGrUGP9oCcEohCfvRaoZMJsPw4dJO0C5froTVaoFOEzzdYlwJ2unT5eIGchVK0LzMNXJOsHrvgsxxHFQRWQhJHAZVRJbXmxSYzeiedqI7USqdffn44MrPwDeNe5DLqQeDi81mA+9wQKby/yVPpgqMWeE7Qq3WYNTocfjJboNRCL4qtBhsTMAhuw2DBw9BRIS0+4+eO3cGAKDVhIsbiBcpFRooFWqcO3dW7FDcKEHzsvh4Z/OjYAuMmcMZY2D2Bnfc3YlM5vzzD7ICGpoKaO7zI4DJ5Kxoc0oRKmhKGSwWS1At9wQAEyfeBAD4zuLdaYW6qx+sFtgEATfdNFXsUNp14cJ5AECIOnjmzuQ4Dhq1wX1uUkBXcC8LCzNAE6KFYA2QBI23QnBYER+fKHYofhcIS6l0hqvAGqzn1xlmsytBE6EPmlIGMAarNbiaA6OjYzB02Ej8aLPCFGTJp7/ZGcN3Viv69O6H9PQMscNp16VLF6FUqKFQBFc3Co0qVFKL1FOC5mUcxyExIRGCrU7sUDwiWJ1xJiR0vwSNb2oLlEm733CHydwJGn1pupjNzioPp/D/m80pZM1iCCbTps2EAOAbi3enFupuDljMMAs8ZhTdInYoHqmqqoJKqRM7DK9TKXWoq6sFz0uj2Z4SNB9ITk4Bs9UFRAVDsNYCcMbc3fC8cwSaPMgSNHlTCc1ul/YIO39yVa9cyZI/uY4ZbBU0wNnndsTIMfjRZkW9RL7UAo1VEPCtzYL+/QYiK6un2OF4pLamGiqF70eJM8Zgs5tgttThQtURn3+nqpVaMMaccyZKACVoPpCcnALBYfP6igK+wFtqodeHdqt1OF3sdjsUMk7S8zh1hisHsdtt4gYiITabFYBICVrTLwCr1er3Y/vDjBm3QCaX42uqonXKNxYzbIKAW269XexQPFZXXwelIsTnx7lYfQQWWwPsvAUnz+7GxWrfTiTrOqeGBml0UaIEzQeSk1MBALylRuRI2sestUhNSxc7DFHYbFYo5MH3EVA0tXHabJSgudhsTbPei1EulbsqmtKfeb8zIiOjMHHSZByzWVEp8XnRpKaB53HAZsGw4aMCqhXDZGqEQuH7BdJr6s+0edvbXH3qjEajT4/jqeD7dpKA5OQUcBwHQeIJGhN48NY6pKWmiR2KKKxWK1TB1r4JuM8pWCs2neGqJnId7HDIGINgdoBvsMFyor5TTSxc04+AYE6YJ0+eDp1Wh6/MjQHRtUMqyiwmyORyzJo1W+xQPGa32+FwOCCX+z5BEwRHm7e9TS5znpNrUJHYKEHzAbVagx494iVfQROstQATkJraPStoVqsVymAbIQC4z8nVrEcAh6uy08GE3HqyAUKjA8wqoPHby7Ce7MT8hk1XWVefx2Ck1Woxo+hWnHXYcToAFlGXgksOO47ZrJhUOBmRkYEzI7/rh59cFnzzLMqblq2Syo8pStB8JDMzC8xaLelfk7y5GgCQlib9Yd2+YLVaIcK8pT5HFbSWXAlaR/sb2s43tnnbE66qXbAP2hg79kbExsRit8UUdKtzeBtjDF+ZTdDr9Jg8ebrY4XSIoykB57jgWIPzaq5zkkp3hCD8epKGtLQMCHaLpAcK8JZq6HR6REVFix2KKGw2K0SYFsvnlHKqoF1LcM1238H3m12zzMS1tz3SlBQKQT7jvkKhwG2z70S1w4HD9LfXplN2O8457JhRdCtCQgJrzVzXhMscgq/1gXN/VqUxRVEQfj1JQ0aGc71M3lwlciTXxyxVyMrKDrpRjJ6yWa3uDvXBREmDBFpwX3BF+Fvn3E2c0rjo+9KgQTcgMzMLe61m2KmK1iqBMZRZTYiNicWYMePFDqfDusP3hVTOkRI0H0lOToFCoQRvvix2KK1ivA28td6dSHZHdrsNIsy64HOublZSKdNLgftXv4jX3e4wcTDHcbjttjvRyPM4aA2+iXm94ZjNimqHA7fcejsUisDrxyVr6qcVjH/Pri5JUlkmTxpRBCGFQoG0tHQIEq2guRLHQJkY0RccvMM9qWsw4TgOco6TzGzY3V4Q/o21pWfPXPTrNwD7rRZYg/BLvCt4xrDXakFKciry8wvEDqdTlEpnUimw4Lu+uM5JqVSKHImTRwlabW0tSkpKkJeXh7Fjx6K0tPS6265Zswbjx49HXl4eiouLUVlZ6X7MZrNh0aJFGD58OPLz8zF37lycOHHC/ficOXPQr18/5OXlIS8vD4WFhZ0/MwnIzs6BYKkB8/HQ4M7gTZfBcTKkp2eKHYpomMCC9rtTJuOCvs9ToJHygCFvmzXrZ7AKAr6nhdSbOWyzoJ53YNYtP5NMM1pHqVTOucJ8PeWFGFzn5DpHsXmUoC1evBhKpRK7du3C0qVLsXjxYhw50nJG39LSUrz33ntYu3Ytdu/ejcjISDz++OPux9euXYu9e/eitLQUX331FXJzc/Hkk08228ezzz6L/fv3Y//+/fjkk0+6eHriys7uCcYE8JZqsUNpgTdfRlJyCjQajdihEB9wJgOB+QUQrAL1C7kzUlPTMCgvHwdsVlgl0uFabDxj2G+1ICM9E/36DRA7nE5TKBRQKJRw8L7vQuHgbdBoNJg+fTo0Gg0cvG/71br2HxLi+1USPNFugmYymbB161bMmzcPOp0O+fn5mDBhAjZu3Nhi2x07dmD27NlITk6GWq1GSUkJysrKUFFRAQA4c+YMRo0ahdjYWCiVShQVFeHYsWPePyuJcDUf8qbKdrb0L8Z4CJYq5PTMFTsUUclkMvijqHHtXGv+mHuNMUAehKskBKRuVDm72vQZs2AVBBwMwjVIO+OozYoGnsf0GbcEfLKu0+rg4H0/Upfn7Zg0aRLuv/9+TJw4EbyPk0LXOel0ep8ex1PtXsHLy8shl8uRnn5lMtPc3FwcP3681e1bK+O7qm233nor9u3bhwsXLsBms+H999/H6NGjm2374osvoqCgALfffjvKyso6dDJSo9eHIj4+UXIJmmCuARN49OzmCZpSqYJD8P2XZ06kus3b3iYwBp4xKBTS6EchBVzTUEoxcyWpdDz2l5SUNPTvNxAHbNZuP6JTYAzfWi1ISU4J6OqZiz401C8JmlyuxNatW7FmzRps27YNcrlvr2kOh/OcQkNDfXocT7U7hMRkMkGvb55N6vV6mEwt5/caM2YMVq9ejcLCQsTGxmLFihXgOA4Wi/MXVEpKCuLj4zFmzBjI5XIkJibirbfecj//iSeeQGZmJlQqFTZv3ozi4mJs2LABKSmer1EWFSWNzNdl4MD++GTrp2BMcH9JiM1hugQAGDZsMMLDpfGHKIbQMD3qa33/xZEfH4KvzjbCyjOMTdUjP8635XNb01xdUVEGxMR03/f3amFhTa+5CImC65AGg7bbvR933PkzPP30tzhis6CvWhrNRmI4ZbehlnfggTtuR2xsmNjhdFlsbDSO1VT4/DgKuQr1jTXYtGkTACBMZ/Dp8ewOZ5/JjIxESQwUaDdB02q1LRYONRqN0GpbTq43a9YsXLhwAffeey8sFgt+8YtfQKfTIS4uDgCwaNEiWK1W7N69GzqdDq+99hruv/9+bNiwATKZDAMGXPllMXPmTHz44YfYuXMn5syZ4/EJVVUZIfihKuKp1NQsCPzHECw1kIdIYzkP3lSJuLgE2O1yVFZ2YumaIKFUamDxQz96juMQqpYjFMAN8b6flNLicP79M6bo1u/v1UympqYRMS4NTdejxkZbt3s/YmNTkJaaju/PVKCPShPwTXud9Z3VgqjIKGRn9wuKvwGtNtSdzAQTm90ErVaH2loLAN83zctkXJtFpXZLOmlpaeB5HuXl5e77Dh8+jKyslvNncRyHkpISbN++HV988QXGjRsHQRCQnZ0NADh69ChuueUWREREQKVSYc6cOTh69CiqqlqfioLjuIAf+ZST0wsAwDdVrcTGmABmuYzc3F5ihyK60NBQNNqDrwOzsemcQkMD/5e6t7j744lxPWk6ZHdr4gSc1/BJhVNQxzu67RqdlQ4HzjvsmDDxJsjlwbE8UkREJKw2U9DNhWa1mxAloXVR271iaLVaTJw4EcuXL4fJZMK+ffuwfft2TJs2rcW2dXV1KC8vB2MMFRUVWLBgAebOnQuDwVmW7N+/P9avX4/6+no4HA688847iI6ORnR0NOrr6/H555/DarXC4XBg48aN2Lt3L0aOHOn9s/YjgyEcPXrEw9EojQRNsNRA4O3Ize0jdiiii4iIRKON90s/NH+qtzrLghERESJHIh1yubOxQIzvE9ePzGD5cu6o/PwhCAsN67aDBQ5azVAplRg1aqzYoXhNdHQMAAarXbpLGXaGzWFEdEys2GG4efSTbuHChbBarRg+fDgee+wxLFiwALm5zg7meXl52Lt3LwBnglZcXIyBAwfizjvvxNChQ/HII4+49/Pb3/4WWq0WhYWFKCgowI4dO7By5UpwHAeHw4Fly5Zh6NChGDp0KN5++22sXLkSGRmBv5B3r169wSyXJfFrw5Uouip73ZnzIgPU+qOd049qms7HdX7kquRIjAqa4Ioh8GaN9waFQoExY2/EabsN9X6ePFlxzVQz1972NSsTcNxux9BhI6HV6vx6bF+KaUpirLbAb651YYzBajVK6rrp0RUjPDwcK1eubPWx/fv3u/87JSUFW7ZsaXM/L7zwQquPRUZGYt26dZ6EE3Byc/vgs8+2S6IfGm+6iLj4BHdVszuLi0sAAFw2OxCtDZ4vz8smB0L1eskMFZcCd4ImRrW0m1fQAGD06HH4cNN6HLZZMCTEf4lKqkrVrGk1VaXy27EB57JODiYE5JqbbYmN7QEAsFgbYNDHixyNd9gdZvCCAz16xIkdilv36xQhAld/L0fjRVHjYIyHYL6M3r2oeRMAEhOTwHHAxcbgmhH7oolHYlKy2GFIimvNQ1GaOJuSwkBcd9FboqKi0adPPxyx2/zar7iPSgODTIYQjsNorQ59VP6dmPuIzYbEhCSkpQV+S9DVIiIioVAoYbbWiR2K15it9QBACVp3ExZmQHxCEniTuAkab64GExzoRQkaAECj0aBHbA+cbQiezssOgeFiowNpad13Ca/WuJsXxaigCVRBA4CRo8bCyPM468fBAhzHQSeTIUIuRx91iF9HkVbzDlxy2DFq9LigG70qk8nQI7aHO6kJBpamZDMuTjoVQUrQ/KRP7z4QzFVgIq6PyDdeBMBR/7OrZGXn4kwDDyHARwu7nGuwgxcYMjOzxQ5FUtwVNDHmQRNcMYg/r5KYBg4cDI1agyM2309wKgVHrVbIOA5Dhw4XOxSfSExKgtUWPBU0k6UWKpUakYE0ipN4R69efcAEB3hL61OK+ANvuoSk5GTo9d1rssy25OT0gsnOB00z58k651pyOTnde5WIa4naB40qaAAAlUqFG4YMxUmHPehXFmCM4ZjDhj59+yMsLDj7+yYkJMFsbQAvBEcLhMlai4SERElVOylB85OcnF7gOK6piuV/THBAMF9Gn979RDm+VPVuej2O1/h2EV5/OVZjQ1pqGiXh13D3/xJjIDX1QXMbOnQE7IKAU/bg+Lxdz3mHA0aex9ChI8QOxWcSE539XE2WWnED8RKLtRZJSZ6vWuQPlKD5iVarQ0pKmmgT1vJm5zQfubm9RTm+VEVERCA1JQ2HqwK/2aXBxuNsvR0D8/LFDkVyXNUrJkIFjQYJXJGT0wuGsDAcD/JmzuN2K5QKJfKC+LOYlBQ8CZrNbobNbnGfk1RQguZHvXv3beqH5v/mNL7xEmQyWbdfIL01+TcU4GyD3T1/WKD68bIVDMDgwUPEDkVyrlTQxGzipARNJpPhhiHDcdphh1UC80L6gsAYTtjtGJg3CBqNf0eN+lNMTCxUKjVM5mqxQ+kyk8V5DsnJVEHrtnr16gPGBPCmSr8fmzddRFpaBkJCuu+CxddTUODsxHvgUmCvLff9JQuSEpOQmJgkdiiS466gibhYenfvg+ZSUDAMPGM4aQvOZs6zDjvMAo8hQ4JzcICLTCZDcnIKTJYasUPpskaz8xySk1NFjqQ5StD8KDu7J2Ryud+n22C8HbylGr179/XrcQNFdHQMcnN6Yf8la8CO5rzY6MDZBjtGBtFyMt50ZZCACAenQQLNZGRkITIiEj8FaT+04zYrNGo1+vcfIHYoPpeSkgaTpTrg18xuNFchMiIKer20JvemBM2P1GoNMjOy/N4PjTdVAozR/GdtGDP2RtSYHfgpQAcL7DlvgkKhwLBhgb12ra+IOg8aVdCa4TgOBUNH4IzDDosQXM2cPGM46XD2A1Uq/btqgRhSU9Pg4O2wBPiSTyZrDVLT0sUOowVK0PysV68+4M01YLz/EgGH6SLkCgXNjdWGwYOHwBBmwO5zgbf4r8ku4LtLVhQUjEBoaJjY4UiSTOa81IkzDxpV0K41ZMiwpr5agfmD6Hoq7HZYBcHdbSLYpaY6k5pGs3jTR3WVg7fBbKlDamqa2KG0QAmanzlHUTK/9kMTTJeQlZkNlZ/XoQskCoUCN04oxE81Npw3Bta8PnvOm2DnBRQW3ix2KJJ1ZbF0EQ7elBS6kkQCpKSkokdsj6AbzXncboVOq0WfPt1jOqPExCTI5YqATtAamwY5pEmwgkbDivwsMzMLCoUSDtMlKEITfX48xtvAW2rQq9eNPj9WoBs3bgI+2rwB/z5Uhwi1d6sdF43Okbtrv/d+h9pzjQ706zdAcnP4SIk7ORIlQXM260lpAkyxuZo5N278AEaBh14W+NVFO2M4abdjxLAR3WZKFYVCgaSkZNRWBXKC5ow9NVV666V2j78iCVEqVcjMzMbxU+f9cjxHU383mv+sfTqdHrffcTe++GKn1/dt0DjXrFOEeb8JMl2uwK233uH1/QYTjhOviRMM4Kh61sKwYc4E7bjNioEardjhdNlJmxUOJgT15LStSU/PwK4zn4MxFpA/QhrNVQgPj4DBIL0VHyhBE0Fubi8cOfIjGG8DJ/dtsyNvugSFQon0dFo82xOjR4/D6NHjxA6DeJmYFTQGhsD72vK9Hj3ikZ6egaMVpzHAzwuZ+8JRmxWREZHIzs4ROxS/SkvLwGefbYfF1oAQdeD1gW20VCMnN0vsMFpFP+tE4Kpm+aMfmmCqRGZmNpTK7r1QM+neRP3ub2riJC2NHDkGVQ4HLvOBPUm0UeBR4bBj+IjR3a6vYVqas2mw0XRZ5Eg6zjVAQKoFjO71lyQRGRmZkCsU7uZHX3H1P8vN7eXT4xBC2kEJWquGDBkGhVyBQzaL2KF0yRGrc7DDyJFjRI7E/xISEqFQKGEMwIECrv5nUhwgAFCCJgqlUoWM9EwIZt/+4uCbftHQ8k6ku6MKljTpdHrk31CAY3Yb7AE62anAGA7ZbcjN7Y3Y2B5ih+N3CoUCKSmpATmS02iiBI20IienF3hLNZjguykdeHMlZHI5MjKk2b5OiL+IOtM5J/LxJW7s2BthEwQcC9ApN07b7WjgHRg3boLYoYgmPT0TjZZqsABbX7XRfBmRkdGSnT+SEjSR9OyZAzAG3uS7Xx28uRKpqelQq9U+OwYhgcCdIIlVSKME7bqys3OQlJiEgzZLQCayB21mGMIMyMvLFzsU0aSlpYPn7TBb68QOpUMaLdXIyJDe9BoulKCJJDMzGxzHgTf7ZqAAExwQzNXIoeZNQiA0LSkkRn7GcVzArvHqDxzHYcLEm1HlcOCcI7Amia7mHaiw2zFu/MRuM/dZa9wDBZomfQ0EdocFFmuDZAcIAJSgiSYkRIvEpGSfjeTkm8rN2dk9fbJ/QgKJK0ETpbM+BzBBCMjqkL8MGzYCofpQfGsNrMEC31nMUCoUGDu2+zZvAkB8fAJUKjWMATSS88oEtdLsfwZQgiaqntk5YNYan7Tbu5pOs7IoQSOEd03jIMYVrykpFIJsYXBvUipVmDDxJpy223DZ4RA7HI8YBR5H7TaMHDUOYT6YgDqQyGQy50ABS+AMFDBKfAQnQAmaqDIzsyHwdgg+aLcXzJcRE9NDsp0fCfEnV4ImxmhOTsY1i4G0bvz4idCo1fjGYhI7FI98azEDHIebb54qdiiSkJ6eiUazbwoOvtBoqkJ0dCy0Wp3YoVwXJWgiclW3eC8PT2aMQbBWUfMmIU3s9qa+TXIRmjibrrKOAKkMiUWn0+PGCTfhJ7sNVby0XyujwONHmxXDh49CdHSM2OFIQlpaOgTBAZMlMAYKmCQ+QACgBE1U0dEx0OtDwXt5PjRmb4RgtyAzM9ur+yUkULmSI1c1y59cx3QnieS6CgunQKPWYI9Z2lW0fWYzGMdh6tQisUORDFdTYSDMh2Z3WGCxGd2DG6SKEjQRcRyHzMxsMIt3R764KnI0/xkhTna7zfkfYlTQ5K4Ezeb/YwcYvV6Pm26eipN2Gy5IdERnHc/jsN2KMWPGd8uJaa+nR49450ABH0/A7g3GABggAFCCJrqMjEzw1now3nsXb95SDYVCicTEJK/tk5BAZrM5P1+iVNDksmYxkLZNmjQZYaGh+MpskuTI193mRiiUSkyfPkvsUCRFJpMhNTUNpgCYaqPRFBgJWveduEUi0tOdJVbeUgOFzju/xgRLNVJTUrv1vDyEXM3WNEs9p/D/b1KOKmgdotFoMHPWz7B27Sv4zmpGtLxr1zGr4Ezyznjh9TcKAk7YbZgx4xYYDOFd3l+wSUvLwE8//QTGBHCcdOs/jeYqxMTEQqvVih1Km+gbXGSuNnDeUu2VBI0xAYKlBunpg7q8L0KChbVpMWsxmjhdSaHFElhzfIlp1Kix2LH9E3x1psJr+9xkrPfKfiIjInHTTVO8sq9g4xooYLbWQauJEDuc6zJZq9Evp6/YYbSLEjSR6fWhiIyMRr2XysKCrQFMcEh6bhdC/M2VoIlRQYOCa4qBEjRPyWQyPDN/EU6dKhc7lBYSE5OgVmvEDkOSUlLSADhXFJBqguZcQcAYEN+RHiVotbW1mD9/Pr788ksYDAb83//9H4qKilrdds2aNXj33XdRU1ODgoIC/OEPf0BMjHMYss1mw5/+9Cd88sknsNls6Nu3L5599ln3UNeOHCeYpKWl47sfDntlX4KlBoD029YJ8SeLxQwA4BTiVdDMZrPfjx3I1GoNetJSdQElPj4BSqUKRlMVYiKkuYSSazkqVzIpZR79nFy8eDGUSiV27dqFpUuXYvHixThy5EiL7UpLS/Hee+9h7dq12L17NyIjI/H444+7H1+7di327t2L0tJSfPXVV8jNzcWTTz7Z4eMEm9TUNPDWBjC+66OWeEsN5AoF4uMTvBAZIcHBlRxxShH6oFETJ+kmAmFFgUBY4sml3auVyWTC1q1bMW/ePOh0OuTn52PChAnYuHFji2137NiB2bNnIzk5GWq1GiUlJSgrK0NFhbMfwZkzZzBq1CjExsZCqVSiqKgIx44d6/Bxgk1ycioAgLfWdnlfgqUGSYnJkMvlXd4XIcHCbDZBppCJs5KAkipopPtIS8uAySLdFQUazVWIjIyGXq8XO5R2tdvEWV5eDrlcjvT0K9lmbm4uysrKWt2+tWHRR44cQXJyMm699VYsWbIEFy5cQGRkJN5//32MHj26U8e5nqgo6b/o1xo4sDcAQLDUAtrOz0rNGANs9ejZcwBiYkK9FB0hgU8Q7JCpxPnRwik4gAM4zkGfSxL0+vbNxfbtn8BsrYdWEy52OC2YrDXIG9Q3ID6L7SZoJpOpRaap1+thMrWc6XnMmDFYvXo1CgsLERsbixUrVoDjOHdpPyUlBfHx8RgzZgzkcjkSExPx1ltvdfg4bamqMkIQpDd3TlsYU0MTou1yBY3xFvAOC6Kj41BZ2eCd4AgJAlVVtYAIzZuAc0JqmVKOyspq+lySoBcZGQ/ANVAgXNxgruHgbTBb6hEfnyyJz6JMxrVZVGr3iqXVamE0GpvdZzQaW50/ZNasWSgqKsK9996LCRMmIDMzEzqdDnFxcQCARYsWwWq1Yvfu3fj2229xyy234P7774cgCB06TrDhOA7JSckQbF1bw0xoWgMtKSnFG2EREjQaTUb3aEoxyFRymEyNoh2fEH9JSEiEQqGU5JJPV/qfpYkbiIfaTdDS0tLA8zzKy8vd9x0+fBhZWS2XEeI4DiUlJdi+fTu++OILjBs3DoIgIDvbuSbk0aNHccsttyAiIgIqlQpz5szB0aNHUVVV1aHjBKPExCQwW32XZs52JXi0ggAhzTUajeBUIk6cqZTBaKQEjQQ/uVyO5OQUSSZoxgBZQcDFowraxIkTsXz5cphMJuzbtw/bt2/HtGnTWmxbV1eH8vJyMMZQUVGBBQsWYO7cuTAYDACA/v37Y/369aivr4fD4cA777yD6OhoREdHd+g4wSgxMQmCwwbm6PxIL8FaB51Oj7AwgxcjIyTwGRuNkImYoHFKDkYvTZRKiNQ5BwpUS26prkZzFSIiIhEaGiZ2KB7x6Iq1cOFCWK1WDB8+HI899hgWLFiA3Fzn/DR5eXnYu3cvAGeCVlxcjIEDB+LOO+/E0KFD8cgjj7j389vf/hZarRaFhYUoKCjAjh07sHLlSvfIqraOE+wSEpxVr640cwq2eqqeEXINxlhTBU28kc2cWo4Go/h9Xgjxh7S0dDh4OyxWaf0oMVmq3csrBgKPJqoNDw/HypUrW31s//797v9OSUnBli1b2tzPCy+80KnjBDvXvGWCtR7QxXVqH8zWgISEAd4Mi5CAZ7Va4XA4oBIxQZOpZDBWGtvfkJAg4FrC0GiuQoim8y06Mpmizdsd4eBtMFvr3bEFAumuZtrNGAzhUKs1EGyd+5UtOKwQHFbExcV7OTJCApuxqXLFqUVs4lTLYbVY4HA4RIuBEH/x1kCBiLCkNm93hCuWQFjiyYUSNIngOA5xcfEQbJ0rCbOmxK5HD0rQCLlaQ4PzMyVTi1lBkzeLhZBgJpfLkZKSCqP5cpf20yMyBxpVKJRyDdITh6JHZE6n9+UaIEAVNNIpcXHxgKNzI71clTfXlCaEEKf6emdSxImYoLmOTQka6S7S0zO7vKIAx3FQKbUI0RgQF5XTpZVAjObLiIqKhl4v/QlqXShBk5DY2B7grY1gjO/wcwW7ERwnQ1RU51ciICQYSaKC1nRsV7JISLBLS0sHz9thtnZtfk9vMVmqkJERWNN2UYImIbGxPQAwMHvHVk8AAMFmREREJBSKzneiJCQYSSNBc15q6+ul8WVFiK9lZGQCAIymrjVzeoPdYYHFagyoEZwAJWiSEh3trH4Jto6P9mJ2Y1OCRwi5Wl1dnXOhdIW4gwQAauIk3UePHvHQqDUwSmDCWleSmJ6eKXIkHUMJmoTExMQCAAR7J/qhOUyIiaHmTUKuVV9fB5la3Moyp5SBk3HUxEm6DZlMhvSMTDRKoIJmNF0Gx3EBs4KACyVoEhIeHgGZTAbWwQSNCTx4u9ldgSOEXFFfXweIOMUG4OzsLNcoqImTdCsZGZlotNSAF8SdXqbBVImE+ERoNBpR4+goStAkRCaTITwiEkIH+6C5ErqoqGhfhEVIQKurrxV1DjQ3tQx1dZSgke4jIyMLjAloNFeLFgNjDCZLFTKzskWLobMkcNUiV4uOigZzdCxBE5q2pwSNkJbq6upEHSDgwqlkqKuvFTsMQvzGNWrSaKoULQaLrR52hzXgRnAClKBJTlRUNOAwd+g5rlGfERGRvgiJkIAlCAIaGxslkaDJNHJq4iTdisEQjsjIKFETNNcAAUrQSJdFNDVxMsY8fo7QlNBFRET4KixCApLRaAQTBHAa8RM0Ti1HQ0NDhz7bhAS6rKyeXV5RoCsaGi9BrdYgISFRtBg6ixI0iQkPjwBjAhhv9fg5zG6GVquDUqnyYWSEBB4pzIHmIlPLIfA8TKbOrRZCSCDKzMyC1dYIq02cv3uj+TIyMrIgkwVeuhN4EQe58HBnFYx1oJmTOcwwhFP1jJBruZoUpZKgAbSaAOleMjOdnfPFaObkeTsazTXICsABAgAlaJITHh4OAGAOi8fPYbwFkdS8SUgLrgqamOtwutBktaQ7SklJg0KhRIPpkt+P7WxaZcjK6un3Y3sDJWgSExZmANCxChp4CwyGcN8EREgAc1WrZCrxEzQZJWikG1IoFEhLSxdlyaeGRmdS6Fp2KtBQgiYxBoMzQRM87IPGGIPgsLgTO0LIFUZjAwDnFBdi41SuBK1B5EgI8a/s7BwYzVV+n7C2wVSJ+PgE6HR6vx7XW8S/apFm1GoNlCq1502cggNM4ClBI6QVDQ0NkKnk4GSc2KFcVUGjBI10L1lZPZsmrPXfupyMMRjNl5GdneO3Y3obJWgSpNeHepygMd65XWhoqC9DIiQgGY0NkhggAACcnINMIUNjIyVopHtxddJ3NTn6g9laC4fDGrD9zwBK0CQpLCwMjLd5tK1rOo7Q0DBfhkRIQDIaGwBl5y9zzC5Ao9Fg+vTp0Gg0YHahS/HIVHIYjcYu7YOQQBMaGobY2Di/JmgNjc5Ro9nZlKARLwrVhwKCh33QHM7t9PrAbGMnxJeMxoYu9T9jdgGTJk3C/fffj4kTJ3Y5QeNUMjQ2UoJGup+ePXNgNF/220TNDaZL0OtDERsb55fj+YJC7ABIS3q9HhA8raDZmp5DTZyEXKvR1AhO2/kEjVPKsHXrVjDGsG3bti4vus6UHFXQSLeUldUTu3bthMVajxCN7/tMG02VyO2dA44Tv/9pZ1EFTYL0er27MtaeKwkaVdAIuZbJZIKsCxU0TimDxWLBpk2bYLFYwHWhuRQAZEo5Gk2UoJHux9XUWO+H+dBsdjPM1vqAbt4EKEGTJK1WB4G3g7H2m1OYYAM4DhpNiB8iIyRwMMacSZVCOpc5TimD2dyBOQ4JCRJxcQnQaXV+6YfmmhQ3kAcIAJSgSZJWq3P+h2Bvd1vG26BRawJynTFCfMlisQCMdbnq5U2UoJHuiuM4ZGXnwGj2/ZJPDY2XoFAokJqa7vNj+ZJ0rlzETavVAgAY71mCFuJK6AghbhaLcwoaSSVoCg42qxWC0LXBBoQEouzsnjBb6mDvwFKGnWE0VSItLQNKpdKnx/E16Vy5iFtISFOC5kkFTbC7EzpCyBUWi7NSJakmzqZYrFbP+pgSEkxcTY6+bOYUBB5Gc1VAT1DrIp0rF3G7UkHzYCSnYIeOEjRCWrDZnEkQJ5fOKC5XLDabZ6O0CQkmaWkZkMvlaDD5rpnTOZWHEPD9zwBK0CTJ3eHfk3XLBAdCQmiAACHXclepJJSgwV1B820TDyFSpFKpkJycCqMPEzTXBLWZmVk+O4a/UIImQa6Ey5MmTjBK0Ahpjd3u/Pxwculc5lwVNFdshHQ3WVk9YTRXQfBgloLOMJoqER0dGxTrU3t05aqtrUVJSQny8vIwduxYlJaWXnfbNWvWYPz48cjLy0NxcTEqK69kyr/85S+Rl5fn/te3b19MmzbN/ficOXPQr18/9+OFhYWdP7MAptFoAHiYoAl2qNUaH0dESOBxJ2jSyc/ci7Y7HJSgke4pKysbguCAyVzj9X1fWSA98Js3AQ9XEli8eDGUSiV27dqFQ4cO4YEHHkCvXr2Qk9O8E15paSnee+89rF27FrGxsVi0aBEef/xxvPnmmwCAV155pdn2c+bMQUFBQbP7nn32Wdx2221dOaeA5064BL7dbQXeQXOgEdIKnm/qIiCTUBOnO0Fr/7NNSDDKyHA2PRpNldBro7y6b5u9ETa7CRkZmV7dr1ja/W1pMpmwdetWzJs3DzqdDvn5+ZgwYQI2btzYYtsdO3Zg9uzZSE5OhlqtRklJCcrKylBRUdFi2zNnzmDv3r2YMWOGd84kiKjVagDtV9AYY2CCw709IeQKnm9KgiSVoDn/z508EtLNREVFIzQ0zCcDBRpMlwEAGRnZXt+3GNqtoJWXl0MulyM9/cqEb7m5uSgrK2t1+9YWQj1y5AiSk5Ob3VdaWor8/PwW97/44otYunQp0tPT8eijj7aosLUnKio4ljxSKpVg7VXQmPMiHxkZhpgYWouTkKvpdCoAgJSW4nOtCxgWpqHPLOm2evXKxcHvj3h9v0ZTJRQKBfLyegf8HGiABwmayWRqsc6jXq+HyWRqse2YMWOwevVqFBYWIjY2FitWrADHce4JI6+2YcMGPPTQQ83ue+KJJ5CZmQmVSoXNmzejuLgYGzZsQEpKiscnVFVlhCC0TBIDjVKlhr2dUZyuBM5uByorG/wRFiEBo76+acZ+KWVoTWpqGukzS7qtxMQ0fP3113DwNijkKq/tt9FcheTkVNTWWgBIf6S0TMa1WVRqt4lTq9XCaGy+uK/RaGx1ctRZs2ahqKgI9957LyZMmIDMzEzodDrExcU1227v3r24fPlyi0EAAwYMgF6vh0qlwsyZMzFo0CDs3LmzvRCDkkqlAmPtNIM0JXDUxElIgGjKFVtraSCku0hPzwDgTKi8hTEBjZZqdx+3YNBugpaWlgae51FeXu6+7/Dhw8jKavkicByHkpISbN++HV988QXGjRsHQRCQnd28Pbi0tBQTJ06ETtf2EkUcx3XbC5lKpW53kABj/JVtCSHS13Q54yRY1SPEX9LSnF2mjCbvJWhmaz143u7edzDwqII2ceJELF++HCaTCfv27cP27dubTY/hUldXh/LycjDGUFFRgQULFmDu3LkwGK7MR2KxWLBlyxbMnDmz2XPr6+vx+eefw2q1wuFwYOPGjdi7dy9GjhzphdMMPGqVyp2AXZfgStACv62dEG/jXPNrSOlHHiVohECvD0VkZLRXK2iufaWlZXhtn2LzaJqNhQsXYv78+Rg+fDgMBgMWLFiA3NxcAEBeXh5efvll5Ofno66uDsXFxTh//jzCwsJw22234eGHH262r08//RShoaEYOnRos/sdDgeWLVuGEydOQC6XIyMjAytXrkRGRvC82B2hUqkA1nYbuiuBUyq914ZPSLCQN01QK6n8rCkYuVwuciSEiCstLR2HfvDeQIFGcxWUSiXi4uK9tk+xeZSghYeHY+XKla0+tn//fvd/p6SkYMuWLW3ua+rUqZg6dWqL+yMjI7Fu3TpPwukWlEpV+/OgCa4EjSpohFzLnQRJadBQ0+TplKCR7i4lJRXffLPHawMFGs01SEpKCarPloTm2CZXczZbtrMUBlXQCLku1+eCSShBc8VCn1nS3aWkpAEATJbaLu+LMQaTtQapqWld3peUUIImUUql0p2AXQ9rWstMqfSoEEpIt+KuLPPSSdDAOz+zCgV9Zkn3lpKSCgAwmau7vC+b3QSHw4qkJM+n5AoElKBJlFyuANpbTLYpgVMoqImTkGupVE0VNAklaK5YaGoc0t1FRERCowmByVrb5X2ZLM51PZOSktvZMrBQgiZRzgpaewka/Ron5Hpca9oyRzufozZwcq7N2x3lioUSNNLdcRyHpKRkmL3QxOlqJk1MTOryvqSEEjSJUijar6BdGcVJFTRCrqXRNCVo9s4naKp4XZu3O4o5nBU0jSakS/shJBgkJibBbK3t8nynZmstDGHh0OmCY6lHF0rQJEqhULr7mF0XVdAIua6QEOdqJ12poKnTQyHTKcCpZdANjIY6vWvrZzKHAKVKGVQjzQjprISERNgdVjj4ri3LZLbWISHIqmcAJWiSpVAoPFgs3fnFI5dTgkbItTQaDcBxYLYuNHFyHGQhCshDVdBkhHV5gllmE9yJIyHdXXx8AgDAZKnr9D4YY7BY65CQkOCtsCSDEjSJUijkHjRxUgWNkOuRyWQICQmB0IUmTm8TbHzQNcMQ0llxcc6kymKt7/Q+7A4zHLw9qCaodaEETaLkcgUYE9pum3dX0Ki5hJDW6HQ6MFs7lWg/YjYBoaFdayYlJFhERkZBoVDA3IUEzfXcHj0oQSN+ciXpajtBk8lktK4fIdcRFmYAs0qnggY7Q1homNhRECIJMpkMMdGxsNg6n6C5qm+xsT28FZZkUIImUe4ErY1mTsYYZFQ9I+S6DGEGoAt90LyNWXiEUoJGiFtsjzjY7MZOP99qM0ImkyEqKtqLUUkDJWgSJZO5ErS2hh8LkMsoQSPkegyGcAhWaTRxMoGBtzlgMISLHQohkhETEwuLzdjpqTYstgZERkYHZVcfStAkypMKGqiCRkibwsIM4C0OSazHKViciSIlaIRcERMTA563w8FbO/V8q92I2NhYL0clDZSgSZRC4Uy8WFsLpjf1QSOEtC4iIhLAleRITILFAQAID48QORJCpCM6OgaAs6myM2z2Rvc+gg19u0uUR02cjAVlWZcQb4mIcCZDgtkhciRXYnDFRAgBIiOdfces9sYOP1cQeNjsZkRGRnk7LEmgBE2i3JWxtgYJgCpohLTFdeGWRIJmcsbg+kIihACRkc4qt83W8QTNldQF4wABgBI0ybqSeLVTQaNBAoRclysZciVHYhLMDqhUKuh0XVvPk5BgoteHQqFQwmo3dfi5tqbnBGu3AUrQJOpKBa2tzs2MKmiEtEGr1UITEgJeAgka3+hAZFQUzVtIyFU4jkO4IRw2R+cTNFdf02BD3+4S5epbxtqpoFGCRkjbYqJjIDTaxQ4DzMwjNib4JtMkpKsiIiNh70QFze5wJWhUQSN+5HEFjQYJENKm2NgeYGZxJ6tljEFodCAmJjinAyCkK8LDI+DgLR1+ns1uhlKphEYT4oOoxEcJmkR52geNKmiEtC0mpgd4o63TE2F6A7MJEOx8UC5HQ0hXGQwG2OzmDj/P7jAjLCw8aLsN0Le7RHFc+xU0Rn3QCGlXbGwPMIGJOpKTNzqbWGOoiZOQFsLCDHDwNghCx+YrtDnMCA8P901QEkDf7hJFFTRCvCMuLh7AlSRJDK5ju2IhhFwRFmYA4KyIdQTPWxEWFrxr29K3u0R5lKCBQU4JGiFt6tHDmRQJDeIlaILRDk4mC9oZzwnpitBQZ5Jld3SsH5rdYXE/NxjRt7tEudrU2+w3w1jQtr0T4i3h4eFQqdXiVtAabIiJiYFCoRAtBkKkKjQ0FEDHEjTGGCVoRByeVtCoiZOQtnEch/i4BPCiVtB4JMQninZ8QqTMlWR1ZMF0nncO/HEld8GIvt0lyl0Zo4lqCemyhIRECA3iDBJgAgNvtCGeEjRCWqXX6wF0rIJmb0rm9HpK0IifeTpIgJo4CWlfQkISeLMdgt3/86EJjXYwgSEhgRI0QloTEqKFTCbrUAXN4XAlaHpfhSU6StAkytPEiypohLQvMdGZHPH1Nr8f29F0zMTEJL8fm5BAIJPJEBKidSddnnAlczodVdCIn3naxEkVNELal5DgTI7ESND4ejvAcdTESUgb9Dp9xypoPFXQiEg8TbwoQSOkfdHRMVCpVCIlaDZER8dArVb7/diEBAp9aCgcvOefzysVNJ2vQhKdRwlabW0tSkpKkJeXh7Fjx6K0tPS6265Zswbjx49HXl4eiouLUVlZ6X7sl7/8JfLy8tz/+vbti2nTpnXqOMGORnES4j0ymQwJiUmiJGhCvR0pySl+Py4hgUSn04MXOpCgOWzgOA5abfAmaB5NyrN48WIolUrs2rULhw4dwgMPPIBevXohJyen2XalpaV47733sHbtWsTGxmLRokV4/PHH8eabbwIAXnnllWbbz5kzBwUFBR0+TnfgyVJPTVv6PBZCgkFKcioqyk6D+XFwDeMFOIw2JCVRgkZIW3Q6XYcraBpNSFAXKdo9M5PJhK1bt2LevHnQ6XTIz8/HhAkTsHHjxhbb7tixA7Nnz0ZycjLUajVKSkpQVlaGioqKFtueOXMGe/fuxYwZMzp8nO7Ao+8PxiCTUYJGiCeSklLAWx1glo6t99cVfJ0NYEAyVdAIaZMzQetYH7Rgrp4BHlTQysvLIZfLkZ6e7r4vNzcXZWVlrW7f2sz3R44cQXJycrP7SktLkZ+f776/o8e5nqio4OgwWF/vPA/WRhMnxwFqtRIxMcE7ioUQb+nXLxcA4KizQRXinxn9HXXOikD//r3oc0pIG2JiIuFw2MCYcKUFqQ0O3oaoqLCg/ly1e5UymUwtRkno9XqYTKYW244ZMwarV69GYWEhYmNjsWLFCnAcB4ul5eRzGzZswEMPPdSp47SlqsoIQWivWVD6amubFo1tswsag90uoLKywS8xERLIQkOjAQB8nRWI0/rlmHydDSq1GnK5jj6nhLTJmY44eBuUCk27W/O8DWp1REB/rmQyrs2iUrsJmlarhdFobHaf0WiEVtvyAjdr1ixcuHAB9957LywWC37xi19Ap9MhLi6u2XZ79+7F5cuXUVhY2KnjdAdX+shcP0NjYJ41hRJCoNXqEBkVBWNdxxZk7gpHnQ2pSSlB3U+GEG/Q6ZyJiscJmmBzPydYtXvVSEtLA8/zKC8vd993+PBhZGVltdiW4ziUlJRg+/bt+OKLLzBu3DgIgoDs7Oxm25WWlmLixInNhsd25DjdgScJGnfV/xJC2peakg6hzj9rcjLGINTbkZqa5pfjERLIXP3JPO2H5uBtQT3FBuBBgqbVajFx4kQsX74cJpMJ+/btw/bt25tNj+FSV1eH8vJyMMZQUVGBBQsWYO7cuTAYDO5tLBYLtmzZgpkzZ3b6ON2BJ6PMmIfbEUKcUlPT4GiwgTl8v+ST0OiAYOeRnJzq82MREuhcXZxOnd+Ho6c+c/8zWWphstQ2u+/oqc9gs1soQQOAhQsXwmq1Yvjw4XjsscewYMEC5OY6O9zm5eVh7969AJwJWnFxMQYOHIg777wTQ4cOxSOPPNJsX59++ilCQ0MxdOjQDh2HtILW4iSkQ1JSnMmSq/O+LznqnJUAqqAR0r6EhERkZGRBFyqHMsTu/qfTh0CnD2l2nzLEjoSERPTu3U/ssH3Ko6FM4eHhWLlyZauP7d+/3/3fKSkp2LJlS5v7mjp1KqZOndrh43Q37ukz2pgHjQNV0AjpiJSUNACAo9YKZVT7/Vy6gq+xQSaTITExuf2NCenmtFodfve7xWKHISnUc1Wirgwzbm+QACVohHgqIiISOp0OfK0fKmi1VsQnJECpVPr8WISQ4EMJmkR5mndRgkaI5ziOQ1paBgQfN3EyxiDU2ZGelunT4xBCghclaJLV/ihOUAWNkA5LTU2Ho94GxvtuvkTBwoO3OtxNqoQQ0lGUoEmUO/FqOz8jhHRQWlo6mMB8unA6X2N1H4sQQjqDEjSJ8mQeNIDRBJiEdNDVAwV8xVFrBTiOFkknhHQafbtLFDVdEuIbMTGx0ISE+DhBsyGuRxw0Gt+OFCWEBC9K0AIcJXKEdAzHcUhLTYdQ67sVBYQ6O9LSMny2f0JI8KMETaJcTZesjXnQmtYS8Es8hASTtLR0OOqtYIL3O3IKFgd4s536nxFCuoQStEDGrprQlhDisZSUdDDeNwMFHE1zrKWmUoJGCOk8StAkytNBAoSQjnNVt3zRD821T9eyUoQQ0hmUoEmUp33LqA8aIR0XG9sDarXaJysK8LVWxMb2QEiI1uv7JoR0H5SgSZQnSz3RNBuEdI5MJkNKapq7OdKbhFrqf0YI6Tr6dpcoT1o4nQMIqIJGSGekpWaAr7O1MxCnYwQrD4fJTv3PCCFdRgmaZHnSB83zNTsJIc2lpKSC8QL4Bu9Nt+Hqf0YJGiGkqyhBkyhPR2dSHzRCOseVRPFeHCjg6tNGa3ASQrqKEjTJ8qiNkxI0QjopPj4BCoXCq/3QHHVWRERGQq/Xe22fhJDuiRI0ifKsgsauGkxACOkIuVyO5ORUr1bQhDo70mkFAUKIF9C3u0S5K2PtdGCmAhohnZeamga+zu6VgQLMLsDRYKPmTUKIV1CCJlGeVMYYo2k2COmKlJQ0CHYegsnR5X056qj/GSHEexRiB0Ba56qg0VqchPhOamoaAMBRY4VM2/blsL0q25URnGneCI0Q0s1RgiZRV/qgtT1IgCpohHReYmIyZDIZjF9fanfb6vUn291Gp9cjPDzCG6ERQro5StAkypOVBBioDxohXaFSqVBc/GucO3fGK/vLyMiikdWEEK+gBE2iPLrIMxrFSUhX5ecPATBE7DAIIaQZ+naXqPZGcTr7w1ATJyGEEBKM6Ntdoq4kXu1Ns0HNKYQQQkiwoQRNwjiOa2MeNOf9VEEjhBBCgg99u0uYszp2nQStKXGjChohhBASfChBkzBOJsP1mzipgkYIIYQEK/p2lzAZJ7v+5JiMEjRCCCEkWNG3u4S12cQJauIkhBBCgpVHCVptbS1KSkqQl5eHsWPHorS09LrbrlmzBuPHj0deXh6Ki4tRWVnZ7PGDBw/izjvvRF5eHoYPH461a9e6H5szZw769euHvLw85OXlobCwsHNnFSRkMhkNEiCEEEK6IY8mql28eDGUSiV27dqFQ4cO4YEHHkCvXr2Qk5PTbLvS0lK89957WLt2LWJjY7Fo0SI8/vjjePPNNwEA1dXV+OUvf4lnnnkGN910E2w2Gy5evNhsH88++yxuu+02L51eYJPJZOCvU0Fj1MRJCCGEBK12v91NJhO2bt2KefPmQafTIT8/HxMmTMDGjRtbbLtjxw7Mnj0bycnJUKvVKCkpQVlZGSoqKgAAr7/+OkaNGoXp06dDpVJBr9cjMzPT+2cVJLi2KmjuBE3ux4gIIYQQ4g/tVtDKy8shl8uRnp7uvi83NxdlZWWtbt9ap/YjR44gOTkZ3333HXr27InbbrsNFRUVGDBgABYuXIiEhAT3ti+++CKWLl2K9PR0PProoygoKOjQCUVF6Tu0vZQpFQrY2mniNBi0iIkJ9V9QhBBCCPG5dhM0k8kEvb550qPX62EymVpsO2bMGKxevRqFhYWIjY3FihUrwHEcLBYLAODixYv48ccf8dprryEnJwcvvPACHnvsMbz77rsAgCeeeAKZmZlQqVTYvHkziouLsWHDBqSkpHh8QlVVRghC27PvBw4O7LrzoAkAgMZGGyorG/wYEyGEEEK6Sibj2iwqtdvEqdVqYTQam91nNBqh1WpbbDtr1iwUFRXh3nvvxYQJE5CZmQmdToe4uDgAgFqtxsSJE9G/f3+o1Wo8/PDD2L9/PxoanAnGgAEDoNfroVKpMHPmTAwaNAg7d+7s0AkHE5lc5k7EWnImbnI5NXESQgghwabdBC0tLQ08z6O8vNx93+HDh5GVldViW47jUFJSgu3bt+OLL77AuHHjIAgCsrOzAaDFoAKX6831xXHc9ecB6wbkMjmuN80Goz5ohBBCSNDyqII2ceJELF++HCaTCfv27cP27dsxbdq0FtvW1dWhvLwcjDFUVFRgwYIFmDt3LgwGAwBnhe3TTz/FoUOHYLfbsWrVKgwePBhhYWGor6/H559/DqvVCofDgY0bN2Lv3r0YOXKk9886QMjl8jYGCTgrawoFJWiEEEJIsPFomo2FCxdi/vz5GD58OAwGAxYsWIDc3FwAQF5eHl5++WXk5+ejrq4OxcXFOH/+PMLCwnDbbbfh4Ycfdu9n2LBheOyxx/Dggw/CbDZj0KBBePHFFwEADocDy5Ytw4kTJyCXy5GRkYGVK1ciIyPDB6cdGJwJWttNnBxH02wQQgghwYZjQdaGGEyDBBb+/hmcq7ZBmzy6xWO8uQqm8m349a+fwMCBg0SIjhBCCCGd1eVBAkQ8irYqaE15NTVxEkIIIcGHEjQJUygU103QWNP9crlHrdSEEEIICSCUoElY233QhCvbEEIIISSoUIImYc7kq+1RnJSgEUIIIcGH2sckTKFQgPFWOBrOtniMN1e5tyGEEEJIcKFvdwkLDQ2DYDPCfObz626j0wXP2qOEEEIIcaJpNiTMZrPh/PmW1TOXkBAtYmN7+DEiQgghhHhDe9NsUAVNwlQqFVJT08UOgxBCCCF+RoMECCGEEEIkhhI0QgghhBCJoQSNEEIIIURiKEEjhBBCCJEYStAIIYQQQiSGEjRCCCGEEImhBI0QQgghRGIoQSOEEEIIkZigm6hWJuPEDoEQQgghpE3t5StBt9QTIYQQQkigoyZOQgghhBCJoQSNEEIIIURiKEEjhBBCCJEYStAIIYQQQiSGEjRCCCGEEImhBI0QQgghRGIoQSOEEEIIkRhK0AghhBBCJIYSNEIIIYQQiaEEjRBCCCFEYoJuLU6pGj9+PC5fvgy5XA6ZTIbevXvj2WefRXZ2ttihdVtXvycuGzduRHJysohRNbdx40YsXLjQfVsQBFgsFqxbtw59+/YFACxfvhzvv/8+GhsbkZmZiWeeeQYDBw4UKWLfCYT3y2az4YknnsDBgwdx9uxZvPnmmygoKHA//sEHH2D+/PnQaDTu+1avXu3e5o033sBbb72FmpoaaLVaTJ48Gb/5zW+gVCr9fi7eFgzvHwCcPn0aS5YswZ49e6BSqXDLLbfgt7/9bYv9TJ8+HSaTCf/73//8eQo+0R3euylTpuDcuXPuba1WK0aPHo3Vq1f79TyaYcQvxo0bx7744gvGGGM2m409//zzbObMmSJH1b1d/Z50lN1u93I0nlm3bh278cYbmSAIjDHGNm/ezEaMGMFOnjzJeJ5nr7/+OhsxYoT78WASCO+X1Wplr7/+OtuzZw8bMWIE2717d7PH161bx26//fbrPv/UqVOsrq6OMcZYdXU1u/vuu9mrr77q05j9JRjeP6vVysaNG8dee+011tjYyCwWCzt06FCL/bz00kvszjvvZKNGjfJL3L7Wnd47xhgTBIGNHz+erV+/3g+RXx81cYpAqVRi2rRp+OmnnwAAO3fuRFFREQYNGoSxY8fiH//4h3vbBx54AG+88Uaz599yyy3YsGEDAODEiRO49957MWTIEBQWFuLDDz90b7dz505MnjwZeXl5GDFiBNasWeP7kwtSTz31FBYuXIgHH3wQeXl5+Pzzz9t83wBg//79uOOOO5Cfn4+RI0firbfeAuCsgq1ZswYTJ05EQUEBfv3rX6OmpsajONavX4+ioiJwHAcAOHPmDAYPHoy0tDTIZDLccsstqKys9Hh/wUqs90ulUmHu3LnIz8+HTNbxy2tKSgrCwsKa3Xf69OkO7yfQSfX9++CDDxAXF4df/OIX0Gq1UKvVyM3NbbbNyZMn8dFHH+GBBx7w0qsRWAL5vXPZs2cPqqurMWnSpC6+Gl0kanrYjVz9C8RqtbLnn3+e3X333Ywxxr7++mt2+PBhxvM8O3ToEBsyZAj773//yxhj7KOPPmJFRUXu/fz0009s4MCBrLGxkZlMJjZ69Gj273//m9ntdvbDDz+wIUOGuH8VjBgxgu3Zs4cxxlhtbS07cOCAH89Y+jryq/DJJ59keXl57Ouvv2aCIDCLxdLm+3bu3DmWl5fH1q9fz2w2G6urq2Pff/89Y4yxN954g91yyy3s7NmzzGq1sgULFrBHHnmk3RjOnDnDcnNz2enTp5vdN2PGDHb8+HFmt9vZK6+8wmbNmtXxFyMABNr7NWrUqFYraAMGDGBDhgxhkyZNYi+99FKLCsPGjRtZXl4e69mzJxs6dCg7fPiwR+csdcHw/j311FPs8ccfZ/feey8bMmQIu+uuu1q8P3fffTf773//y3bv3t0tK2iB/N5dve2TTz7p0fn6EvVB86OSkhLI5XKYzWbodDr885//BADccMMN7m1yc3MxZcoUlJWVYezYsbjxxhuxcOFCHDt2DNnZ2diwYQMmTZoErVaLjz76CPHx8Zg9ezYAoHfv3pg8eTK2bNmC3NxcKBQKHDlyBDk5OTAYDDAYDKKct5S53hMAGDhwIF555ZXrbjt+/Hj3e6VWq9t83zZt2oShQ4eiqKgIgLNq2q9fPwDAv//9b8yfPx8JCQkAgP/7v//DiBEjYLPZoFKprnv80tJS5OfnN+v3ER0djcGDB2PKlCmQyWQIDw/Ha6+91rkXIwAE0vvVmhtuuAGbNm1CYmIijh07hkcffRRKpRIPPvige5tp06Zh2rRpKC8vx/r16xEZGdmhY0hZoL9/Fy9eRFlZGVatWoVhw4bhzTffxK9+9St8/PHHUKlUKC0thV6vx9ixY1FWVtahfUtdsL93LmazGZ988kmLKp8YKEHzo5UrV2L48OHgeR7bt2/H/fffj48++ghnz57FX//6Vxw7dgx2ux02mw1Tp04F4CzbTp48GRs2bMDjjz+OTZs24Y9//CMA4OzZszh48CDy8/Pdx+B5HlOmTAHg7Dy+atUqvPjii+jZsyd+85vfYPDgwf4/cQlzvSdXW716tTt5njZtGhYvXgwASExMbLbd/v37r/u+nTt3Dqmpqa0e8+zZs3jkkUealeEVCgUqKytbHONqGzZsaPZFDgArVqzAgQMH8NlnnyE6OhoffvghfvnLX2LLli3Q6/UevgqBI5Der9ZcnVzn5OSgpKQEr776aov3FQDS0tLQs2dPLFq0CCtWrOjQcaQq0N8/tVqNwYMHY8yYMQCA++67D//4xz9w4sQJxMfHY/ny5e7muWATzO/d1U2dW7duRXh4OIYMGdKh/fsCJWgikMvlmDRpEp599ll88803eOGFFzBnzhy88sorUKvVWLx4MUwmk3v7WbNm4ZFHHsGoUaPA87x7ZEp8fDxuuOEGvP76660ep3///li9ejXsdjveffddzJs3D7t27fLLOQay4uJiFBcXt7vdE088cd33LT4+Ht99912rz4uLi8Nzzz3XoWR53759uHTpEgoLC5vdf/ToUUyePBlxcXEAgKKiIvzpT3/C8ePHg3IkZ2uk+H55iuM4MMau+7jD4Qj6PmiB9P7l5OTgm2++afWxw4cP49KlS+4WDbvdjoaGBowYMQL/7//9v+smIYEsWN67q5WWlmLGjBnufr5iokECImCM4dNPP0V9fT0yMzNhMplgMBigVqvx7bffYvPmzc2279+/P3Q6Hf7whz9g+vTp7l8TY8eORXl5OUpLS2Gz2WC323HgwAEcP34cNpsNGzduRENDA5RKJbRarST+4IJJW+/btGnT8NVXX2HTpk2w2+2or6/HwYMHAQB33HEHli1bhjNnzgAAqqur8emnn7Z5rNLSUkyaNKlFVax///7YsmULKisrIQgCNm3aBIvFEpRfBl3lr/fLZrPBarUCcH5JW61WCIIAwDlw5/LlywCAn376CatWrcKNN97ofu57772HqqoqAMDx48fx8ssvY9iwYV58FQKXFN6/6dOn47vvvsOXX34Jnuexdu1aREREICMjA3l5efjvf/+L0tJSlJaWYsmSJYiKikJpaSmSkpJ89bIEBKm/dy4XLlxAWVkZZs6c6fXXoDMoQfOj4uJi5OXlYdCgQVi2bBn+/Oc/Izs7GwsXLsTf//535OXlYfXq1bj55ptbPLeoqAjHjh1zt9MDgF6vx6uvvoqPP/4Yo0ePxogRI/D888/DYrEAcDaJ3XjjjRg0aBDeeecd/PWvf/XXqXYLbb1vCQkJePnll/H2229j6NChmDJlivtX4t13343x48fjvvvuw6BBg3Dbbbfh22+/ve5xrFYrPv7441YvGvfffz/69OmDmTNnIj8/H6+88gr+/ve/IyIiwuvnG+j89X7ddNNN6N+/Py5evIj77rsP/fv3x549ewAAu3fvxvTp0zFw4EA88MADmDhxYrPmzX379mHq1Knux0eNGoXHHnvMNy9IgJHC+5eRkYGlS5di4cKFuOGGG/Dpp59i1apVUKlUUKlUiImJcf8zGAyQyWSIiYlpNn9YdyT1985lw4YNGDhwIFJSUnzzQnQQx9qqrxPJ+PDDD/H6669j3bp1YodCCCGEEB+jCloAMJlMeOedd/Czn/1M7FAIIYQQ4geUoEnc//73PwwdOhQ6nU4y7eKEEEII8S1q4iSEEEIIkRiqoBFCCCGESAwlaIQQQgghEkMJGiGEEEKIxFCCRgghhBAiMZSgEUIIIYRIDCVohBBCCCES8/8BHq+74aXdSUYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_performance(overall_accs.iloc[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_performance(overall_losses.iloc[:,:5], measure=\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_performance(overall_ratios.iloc[:,:5], measure=\"T:V Loss Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bayes         0.044544\n",
       "F-race 78     0.038802\n",
       "F-race 153    0.059723\n",
       "F-race 164    0.042427\n",
       "F-race 167    0.051958\n",
       "F-race 171    0.114332\n",
       "dtype: float64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_losses.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code-cleaned Version F-Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
